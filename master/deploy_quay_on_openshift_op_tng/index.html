<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Deploy Red Hat Quay on OpenShift Container Platform with the Red Hat Quay Operator</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.4"/><meta name="description" content="Deploy Red Hat Quay on an OpenShift Cluster with the Red Hat Quay Operator"/><link rel="next" href="#idm45898220856352" title="Preface"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm45898217789760"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Quay</span> <span class="productnumber">3.9</span></div><div><h1 class="title">Deploy Red Hat Quay on OpenShift Container Platform with the Red Hat Quay Operator</h1></div><div><h2 class="subtitle">Deploy Red Hat Quay on OpenShift with Quay Operator</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm45898215151520">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Deploy Red Hat Quay on an OpenShift Cluster with the Red Hat Quay Operator
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="preface"><a href="#idm45898220856352">Preface</a></span></li><li><span class="chapter"><a href="#operator-concepts">1. Introduction to the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#operator-quayregistry-api">1.1. QuayRegistry API</a></span></li><li><span class="section"><a href="#operator-components-intro">1.2. Red Hat Quay Operator components</a></span></li><li><span class="section"><a href="#operator-components-managed">1.3. Using managed components</a></span></li><li><span class="section"><a href="#operator-components-unmanaged">1.4. Using unmanaged components for dependencies</a></span></li><li><span class="section"><a href="#operator-config-bundle-secret">1.5. Config bundle secret</a></span></li><li><span class="section"><a href="#operator-prereq">1.6. Prerequisites for Red Hat Quay on OpenShift Container Platform</a></span><ul><li><span class="section"><a href="#openshift-cluster">1.6.1. OpenShift cluster</a></span></li><li><span class="section"><a href="#resource-requirements">1.6.2. Resource Requirements</a></span></li><li><span class="section"><a href="#object-storage">1.6.3. Object Storage</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#operator-install">2. Installing the Red Hat Quay Operator from the OperatorHub</a></span></li><li><span class="chapter"><a href="#operator-preconfigure">3. Configuring Red Hat Quay before deployment</a></span><ul><li><span class="section"><a href="#config-preconfigure-automation">3.1. Pre-configuring Red Hat Quay for automation</a></span><ul><li><span class="section"><a href="#allowing-the-api-to-create-first-user">3.1.1. Allowing the API to create the first user</a></span></li><li><span class="section"><a href="#enabling-general-api-access">3.1.2. Enabling general API access</a></span></li><li><span class="section"><a href="#adding-super-user">3.1.3. Adding a superuser</a></span></li><li><span class="section"><a href="#restricting-user-creation">3.1.4. Restricting user creation</a></span></li><li><span class="section"><a href="#enabling-new-functionality-38">3.1.5. Enabling new functionality in Red Hat Quay 3.8</a></span></li><li><span class="section"><a href="#enabling-new-functionality-37">3.1.6. Enabling new functionality in Red Hat Quay 3.7</a></span></li><li><span class="section"><a href="#suggested-configuration-for-automation">3.1.7. Suggested configuration for automation</a></span></li></ul></li><li><span class="section"><a href="#operator-storage-preconfig">3.2. Configuring object storage</a></span><ul><li><span class="section"><a href="#operator-unmanaged-storage">3.2.1. Unmanaged storage</a></span></li><li><span class="section"><a href="#operator-managed-storage">3.2.2. Managed storage</a></span></li></ul></li><li><span class="section"><a href="#configuring-the-database-poc">3.3. Configuring the database</a></span><ul><li><span class="section"><a href="#operator-unmanaged-postgres">3.3.1. Using an existing PostgreSQL database</a></span></li><li><span class="section"><a href="#config-fields-db">3.3.2. Database configuration</a></span></li><li><span class="section"><a href="#operator-managed-postgres">3.3.3. Using the managed PostgreSQL database</a></span></li></ul></li><li><span class="section"><a href="#operator-preconfig-tls-routes">3.4. Configuring SSL/TLS and Routes</a></span><ul><li><span class="section"><a href="#creating-config-bundle-secret-tls-cert-key-pair">3.4.1. Creating the config bundle secret with the SSL/TLS cert and key pair</a></span></li></ul></li><li><span class="section"><a href="#operator-components-unmanaged-other">3.5. Configuring external Redis</a></span><ul><li><span class="section"><a href="#operator-unmanaged-redis">3.5.1. Using external Redis</a></span></li><li><span class="section"><a href="#operator-unmanaged-hpa">3.5.2. Horizontal Pod Autoscaler</a></span></li><li><span class="section"><a href="#operator-unmanaged-route">3.5.3. Disabling Route Component</a></span></li><li><span class="section"><a href="#operator-unmanaged-monitoring">3.5.4. Unmanaged monitoring</a></span></li><li><span class="section"><a href="#operator-unmanaged-mirroring">3.5.5. Unmanaged mirroring</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#operator-deploy">4. Deploying Red Hat Quay using the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#operator-deploy-cli">4.1. Deploying Red Hat Quay from the command line</a></span><ul><li><span class="section"><a href="#operator-deploy-view-pods-cli">4.1.1. Viewing created components using the command line</a></span></li><li><span class="section"><a href="#operator-deploy-hpa">4.1.2. Horizontal Pod Autoscaling (HPA)</a></span></li><li><span class="section"><a href="#deploy-quay-api">4.1.3. Using the API to deploy Red Hat Quay</a></span></li><li><span class="section"><a href="#operator-monitor-deploy-cli">4.1.4. Monitoring and debugging the deployment process</a></span></li></ul></li><li><span class="section"><a href="#operator-deploy-ui">4.2. Deploying Red Hat Quay from the OpenShift Container Platform console</a></span><ul><li><span class="section"><a href="#operator-first-user">4.2.1. Using the Red Hat Quay UI to create the first user</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#operator-config-cli">5. Configuring Red Hat Quay on OpenShift Container Platform</a></span><ul><li><span class="section"><a href="#editing-config-bundle-secret-in-ocp-console">5.1. Editing the config bundle secret in the OpenShift Container Platform console</a></span></li><li><span class="section"><a href="#operator-config-cli-access">5.2. Determining QuayRegistry endpoints and secrets</a></span><ul><li><span class="section"><a href="#determining-username-password-config-editor-tool">5.2.1. Locating the username and password for the config editor tool</a></span></li></ul></li><li><span class="section"><a href="#operator-config-cli-download">5.3. Downloading the existing configuration</a></span><ul><li><span class="section"><a href="#using-config-editor-endpoint">5.3.1. Using the config editor endpoint to download the existing configuration</a></span></li><li><span class="section"><a href="#using-config-bundle-secret">5.3.2. Using the config bundle secret to download the existing configuration</a></span></li></ul></li><li><span class="section"><a href="#operator-custom-ssl-certs-config-bundle">5.4. Using the config bundle to configure custom SSL/TLS certs</a></span></li></ul></li><li><span class="chapter"><a href="#operator-config-ui">6. Using the config tool to reconfigure Red Hat Quay on OpenShift Container Platform</a></span><ul><li><span class="section"><a href="#operator-config-ui-access">6.1. Accessing the config editor</a></span><ul><li><span class="section"><a href="#retrieving-the-config-editor-credentials">6.1.1. Retrieving the config editor credentials</a></span></li><li><span class="section"><a href="#logging-into-config-editor">6.1.2. Logging into the config editor</a></span></li><li><span class="section"><a href="#operator-config-ui-change">6.1.3. Changing configuration</a></span></li></ul></li><li><span class="section"><a href="#operator-config-ui-monitoring">6.2. Monitoring reconfiguration in the Red Hat Quay UI</a></span><ul><li><span class="section"><a href="#reconfiguring-quayregistry-resource">6.2.1. QuayRegistry resource</a></span></li></ul></li><li><span class="section"><a href="#operator-config-ui-updated">6.3. Accessing updated information after reconfiguration</a></span><ul><li><span class="section"><a href="#accessing-updated-config-ui">6.3.1. Accessing the updated config.yaml file in the UI</a></span></li></ul></li><li><span class="section"><a href="#config-ui-custom-ssl-certs">6.4. Custom SSL/TLS certificates UI</a></span></li><li><span class="section"><a href="#operator-external-access">6.5. External Access to the Registry</a></span></li></ul></li><li><span class="chapter"><a href="#quay_operator_features">7. Quay Operator features</a></span><ul><li><span class="section"><a href="#operator-console-monitoring-alerting">7.1. Console monitoring and alerting</a></span><ul><li><span class="section"><a href="#operator-dashboard">7.1.1. Dashboard</a></span></li><li><span class="section"><a href="#operator-metrics">7.1.2. Metrics</a></span></li><li><span class="section"><a href="#operator-alerting">7.1.3. Alerting</a></span></li></ul></li><li><span class="section"><a href="#clair-vulnerability-scanner">7.2. Clair for Red Hat Quay</a></span><ul><li><span class="section"><a href="#clair-vulnerability-scanner-hosts">7.2.1. Clair vulnerability databases</a></span></li><li><span class="section"><a href="#clair-quay-operator-overview">7.2.2. Clair on OpenShift Container Platform</a></span></li><li><span class="section"><a href="#clair-testing">7.2.3. Testing Clair</a></span></li></ul></li><li><span class="section"><a href="#fips-overview">7.3. Federal Information Processing Standard (FIPS) readiness and compliance</a></span></li></ul></li><li><span class="chapter"><a href="#advanced_concepts">8. Advanced Concepts</a></span><ul><li><span class="section"><a href="#operator-deploy-infrastructure">8.1. Deploying Quay on infrastructure nodes</a></span><ul><li><span class="section"><a href="#label-taint-nodes-for-infrastructure-use">8.1.1. Labeling and tainting nodes for infrastructure use</a></span></li><li><span class="section"><a href="#creating-project-with-node-selector">8.1.2. Creating a project with node selector and toleration</a></span></li><li><span class="section"><a href="#installing-quay-operator-in-namespace">8.1.3. Installing the Red Hat Quay Operator in the namespace</a></span></li><li><span class="section"><a href="#creating-the-registry">8.1.4. Creating the Red Hat Quay registry</a></span></li></ul></li><li><span class="section"><a href="#monitoring-single-namespace">8.2. Enabling monitoring when the Red Hat Quay Operator is installed in a single namespace</a></span><ul><li><span class="section"><a href="#creating-cluster-monitoring-config-map">8.2.1. Creating a cluster monitoring config map</a></span></li><li><span class="section"><a href="#creating-user-defined-workload-monitoring-config-map">8.2.2. Creating a user-defined workload monitoring ConfigMap object</a></span></li><li><span class="section"><a href="#enabling-monitoring-user-defined-projects">8.2.3. Enable monitoring for user-defined projects</a></span></li><li><span class="section"><a href="#creating-service-object-expose-quay-metrics">8.2.4. Creating a Service object to expose Red Hat Quay metrics</a></span></li><li><span class="section"><a href="#creating-servicemonitor-object">8.2.5. Creating a ServiceMonitor object</a></span></li><li><span class="section"><a href="#view-metrics-in-ocp">8.2.6. Viewing metrics in OpenShift Container Platform</a></span></li></ul></li><li><span class="section"><a href="#operator-resize-storage">8.3. Resizing Managed Storage</a></span><ul><li><span class="section"><a href="#resizing-noobaa-pvc">8.3.1. Resizing the NooBaa PVC</a></span></li><li><span class="section"><a href="#adding-another-storage-pool">8.3.2. Adding an additional storage pool</a></span></li></ul></li><li><span class="section"><a href="#operator-customize-images">8.4. Customizing Default Operator Images</a></span><ul><li><span class="section"><a href="#custom-environment-variables">8.4.1. Environment Variables</a></span></li><li><span class="section"><a href="#applying-overrides-to-running-operator">8.4.2. Applying overrides to a running Operator</a></span></li></ul></li><li><span class="section"><a href="#operator-cloudfront">8.5. AWS S3 CloudFront</a></span></li><li><span class="section"><a href="#clair-advanced-configuration-overview">8.6. Advanced Clair configuration</a></span><ul><li><span class="section"><a href="#unmanaged-clair-configuration">8.6.1. Unmanaged Clair configuration</a></span></li><li><span class="section"><a href="#custom-clair-configuration-managed-database">8.6.2. Running a custom Clair configuration with a managed Clair database</a></span></li><li><span class="section"><a href="#clair-disconnected-environments">8.6.3. Clair in disconnected environments</a></span></li><li><span class="section"><a href="#clair-crda-configuration">8.6.4. Enabling Clair CRDA</a></span></li><li><span class="section"><a href="#mapping-repositories-to-cpe-information">8.6.5. Mapping repositories to Common Product Enumeration information</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#red-hat-quay-builders-enhancement">9. Red Hat Quay build enhancements</a></span><ul><li><span class="section"><a href="#red-hat-quay-builds-architecture">9.1. Red Hat Quay enhanced build architecture</a></span></li><li><span class="section"><a href="#red-hat-quay-build-limitations">9.2. Red Hat Quay build limitations</a></span></li><li><span class="section"><a href="#builders-virtual-environment">9.3. Creating a Red Hat Quay builders environment with OpenShift Container Platform</a></span><ul><li><span class="section"><a href="#openshift-tls-component">9.3.1. OpenShift Container Platform TLS component</a></span></li><li><span class="section"><a href="#red-hat-quay-quota-builders-establishment">9.3.2. Using OpenShift Container Platform for Red Hat Quay builders</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#georepl-intro">10. Geo-replication</a></span><ul><li><span class="section"><a href="#arch-georpl-features">10.1. Geo-replication features</a></span></li><li><span class="section"><a href="#arch-georepl-prereqs">10.2. Geo-replication requirements and constraints</a></span></li><li><span class="section"><a href="#georepl-arch-operator">10.3. Geo-replication using the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#georepl-deploy-operator">10.3.1. Setting up geo-replication on OpenShift Container Platform</a></span></li><li><span class="section"><a href="#georepl-mixed-storage">10.3.2. Mixed storage for geo-replication</a></span></li></ul></li><li><span class="section"><a href="#upgrading-geo-repl-quay-operator">10.4. Upgrading a geo-replication deployment of the Red Hat Quay Operator</a></span></li></ul></li><li><span class="chapter"><a href="#backing-up-and-restoring-intro">11. Backing up and restoring Red Hat Quay managed by the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#backing-up-red-hat-quay-operator">11.1. Backing up Red Hat Quay</a></span><ul><li><span class="section"><a href="#quay-configuration-backup">11.1.1. Red Hat Quay configuration backup</a></span></li><li><span class="section"><a href="#scaling-down-quay-deployment">11.1.2. Scaling down your Red Hat Quay deployment</a></span></li><li><span class="section"><a href="#backing-up-managed-database">11.1.3. Backing up the Red Hat Quay managed database</a></span></li><li><span class="section"><a href="#scaling-up-quay-deployment">11.1.4. Scale the Red Hat Quay deployment back up</a></span></li></ul></li><li><span class="section"><a href="#restoring-up-red-hat-quay">11.2. Restoring Red Hat Quay</a></span><ul><li><span class="section"><a href="#restoring-quay-and-configuration-from-backup">11.2.1. Restoring Red Hat Quay and its configuration from a backup</a></span></li><li><span class="section"><a href="#scale-down-quay-deployment">11.2.2. Scaling down your Red Hat Quay deployment</a></span></li><li><span class="section"><a href="#restoring-quay-database">11.2.3. Restoring your Red Hat Quay database</a></span></li><li><span class="section"><a href="#restoring-quay-object-storage-data">11.2.4. Restore your Red Hat Quay object storage data</a></span></li><li><span class="section"><a href="#scaling-up-quay">11.2.5. Scaling up your Red Hat Quay deployment</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#operator-ipv6-dual-stack">12. Deploying IPv6 on the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#proc-manage-enabling-ipv6">12.1. Enabling the IPv6 protocol family</a></span></li><li><span class="section"><a href="#operator-ipv6-limitations-38">12.2. IPv6 limitations</a></span></li></ul></li><li><span class="chapter"><a href="#operator-upgrade">13. Upgrading the Red Hat Quay Operator Overview</a></span><ul><li><span class="section"><a href="#operator-lifecycle-manager">13.1. Operator Lifecycle Manager</a></span></li><li><span class="section"><a href="#upgrading-quay-operator">13.2. Upgrading the Quay Operator</a></span><ul><li><span class="section"><a href="#upgrading-red-hat-quay">13.2.1. Upgrading Quay</a></span></li><li><span class="section"><a href="#upgrade-33-36">13.2.2. Upgrading directly from 3.3.z or 3.4.z to 3.6</a></span></li><li><span class="section"><a href="#swift-config-upgrading-from-33-to-36">13.2.3. Swift configuration when upgrading from 3.3.z to 3.6</a></span></li><li><span class="section"><a href="#changin-update-channel-for-operator">13.2.4. Changing the update channel for the Red Hat Quay Operator</a></span></li><li><span class="section"><a href="#manually-approving-pending-operator-upgrade">13.2.5. Manually approving a pending Operator upgrade</a></span></li></ul></li><li><span class="section"><a href="#upgrading-quayregistry">13.3. Upgrading a QuayRegistry</a></span></li><li><span class="section"><a href="#upgrading-quayecosystem">13.4. Upgrading a QuayEcosystem</a></span><ul><li><span class="section"><a href="#reverting-quayecosystem-upgrade">13.4.1. Reverting QuayEcosystem Upgrade</a></span></li><li><span class="section"><a href="#supported-quayecossytem-configurations-for-upgrades">13.4.2. Supported QuayEcosystem Configurations for Upgrades</a></span></li></ul></li></ul></li></ul></div><section class="preface" id="idm45898220856352"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div><p>
			Red Hat Quay is an enterprise-quality container registry. Use Red Hat Quay to build and store container images, then make them available to deploy across your enterprise.
		</p><p>
			The Red Hat Quay Operator provides a simple method to deploy and manage Red Hat Quay on an OpenShift cluster.
		</p><p id="operator-differences">
			With the release of Red Hat Quay 3.4.0, the Red Hat Quay Operator was re-written to offer an enhanced experience and to add more support for Day 2 operations. As a result, the Red Hat Quay Operator is now simpler to use and is more opinionated. The key difference from versions prior to Red Hat Quay 3.4.0 include the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The <code class="literal">QuayEcosystem</code> custom resource has been replaced with the <code class="literal">QuayRegistry</code> custom resource.
				</li><li class="listitem"><p class="simpara">
					The default installation options produces a fully supported Red Hat Quay environment, with all managed dependencies (database, caches, object storage, and so on) supported for production use.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Some components might not be highly available.
					</p></div></div></li><li class="listitem">
					A new validation library for Red Hat Quay’s configuration, which is shared by the Red Hat Quay application and config tool for consistency.
				</li><li class="listitem"><p class="simpara">
					Object storage can now be managed by the Red Hat Quay Operator using the <code class="literal">ObjectBucketClaim</code> Kubernetes API
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Red Hat OpenShift Data Foundation can be used to provide a supported implementation of this API on OpenShift Container Platform.
					</p></div></div></li><li class="listitem">
					Customization of the container images used by deployed pods for testing and development scenarios.
				</li></ul></div></section><section class="chapter" id="operator-concepts"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Introduction to the Red Hat Quay Operator</h1></div></div></div><p>
			Use the content in this chapter to execute the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Install the Red Hat Quay Operator
				</li><li class="listitem">
					Configure managed, or unmanaged, object storage
				</li><li class="listitem">
					Configure unmanaged components, such as the database, Redis, routes, TLS, and so on.
				</li><li class="listitem">
					Deploy the Red Hat Quay registry on OpenShift Container Platform using the Red Hat Quay Operator
				</li><li class="listitem">
					Use advanced features supported by the Red Hat Quay Operator
				</li><li class="listitem">
					Upgrade the registry by upgrading the Red Hat Quay Operator
				</li></ul></div><section class="section" id="operator-quayregistry-api"><div class="titlepage"><div><div><h2 class="title">1.1. QuayRegistry API</h2></div></div></div><p>
				The Red Hat Quay Operator provides the <code class="literal">QuayRegistry</code> custom resource API to declaratively manage <code class="literal">Quay</code> container registries on the cluster. Use either the OpenShift Container Platform UI or a command-line tool to interact with this API.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Creating a <code class="literal">QuayRegistry</code> results in the Red Hat Quay Operator deploying and configuring all necessary resources needed to run Red Hat Quay on the cluster.
					</li><li class="listitem">
						Editing a <code class="literal">QuayRegistry</code> results in the Red Hat Quay Operator reconciling the changes and creating, updating, and deleting objects to match the desired configuration.
					</li><li class="listitem">
						Deleting a <code class="literal">QuayRegistry</code> results in garbage collection of all previously created resources. After deletion, the <code class="literal">Quay</code> container registry is no longer be available.
					</li></ul></div><p>
				The <code class="literal">QuayRegistry</code> API is user friendly, and the fields are outlined in the following sections.
			</p></section><section class="section" id="operator-components-intro"><div class="titlepage"><div><div><h2 class="title">1.2. Red Hat Quay Operator components</h2></div></div></div><p>
				Red Hat Quay has a significant number of dependencies. These include a database, object storage, Redis, and others. The Red Hat Quay Operator manages an opinionated deployment of Red Hat Quay and its dependencies on Kubernetes. These dependencies are treated as <span class="emphasis"><em>components</em></span> and are configured through the <code class="literal">QuayRegistry</code> API.
			</p><p>
				In the <code class="literal">QuayRegistry</code> custom resource, the <code class="literal">spec.components</code> field configures components. Each component contains two fields: <code class="literal">kind</code> (the name of the component), and <code class="literal">managed</code> (a boolean that addresses whether the component lifecycle is handled by the Red Hat Quay Operator). By default, all components are managed and auto-filled upon reconciliation for visibility:
			</p><pre class="programlisting language-yaml">spec:
  components:
    - kind: quay
      managed: true
    - kind: postgres
      managed: true
    - kind: clair
      managed: true
    - kind: redis
      managed: true
    - kind: horizontalpodautoscaler
      managed: true
    - kind: objectstorage
      managed: true
    - kind: route
      managed: true
    - kind: mirror
      managed: true
    - kind: monitoring
      managed: true
    - kind: tls
      managed: true
    - kind: clairpostgres
      managed: true</pre></section><section class="section" id="operator-components-managed"><div class="titlepage"><div><div><h2 class="title">1.3. Using managed components</h2></div></div></div><p>
				Unless your <code class="literal">QuayRegistry</code> custom resource specifies otherwise, the Red Hat Quay Operator uses defaults for the following managed components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>quay:</strong></span> Holds overrides for the Red Hat Quay deployment. For example, environment variables and number of replicas. This component is new as of Red Hat Quay 3.7 and cannot be set to unmanaged.
					</li><li class="listitem">
						<span class="strong strong"><strong>postgres:</strong></span> For storing the registry metadata, uses a version of Postgres 10 from the <a class="link" href="https://www.softwarecollections.org/en/">Software Collections</a>
					</li><li class="listitem">
						<span class="strong strong"><strong>clair:</strong></span> Provides image vulnerability scanning.
					</li><li class="listitem">
						<span class="strong strong"><strong>redis:</strong></span> Stores live builder logs and the Red Hat Quay tutorial. Also includes the locking mechanism that is required for garbage collection.
					</li><li class="listitem">
						<span class="strong strong"><strong>horizontalpodautoscaler:</strong></span> Adjusts the number of <code class="literal">Quay</code> pods depending on memory/cpu consumption.
					</li><li class="listitem">
						<span class="strong strong"><strong>objectstorage:</strong></span> For storing image layer blobs, utilizes the <code class="literal">ObjectBucketClaim</code> Kubernetes API which is provided by Noobaa or RHOCS.
					</li><li class="listitem">
						<span class="strong strong"><strong>route:</strong></span> Provides an external entrypoint to the Red Hat Quay registry from outside of OpenShift Container Platform.
					</li><li class="listitem">
						<span class="strong strong"><strong>mirror:</strong></span> Configures repository mirror workers to support optional repository mirroring.
					</li><li class="listitem">
						<span class="strong strong"><strong>monitoring:</strong></span> Features include a Grafana dashboard, access to individual metrics, and alerting to notify for frequently restarting Quay pods.
					</li><li class="listitem">
						<span class="strong strong"><strong>tls:</strong></span> Configures whether Red Hat Quay or OpenShift Container Platform handles SSL/TLS.
					</li><li class="listitem">
						<span class="strong strong"><strong>clairpostgres:</strong></span> Configures a managed Clair database.
					</li></ul></div><p>
				The Red Hat Quay Operator handles any required configuration and installation work needed for Red Hat Quay to use the managed components. If the opinionated deployment performed by the Red Hat Quay Operator is unsuitable for your environment, you can provide the Red Hat Quay Operator with <code class="literal">unmanaged</code> resources (overrides) as described in the following sections.
			</p></section><section class="section" id="operator-components-unmanaged"><div class="titlepage"><div><div><h2 class="title">1.4. Using unmanaged components for dependencies</h2></div></div></div><p>
				If you have existing components such as PostgreSQL, Redis, or object storage that you want to use with Red Hat Quay, you first configure them within the Red Hat Quay configuration bundle (<code class="literal">config.yaml</code>). Then, they must be referenced in your <code class="literal">QuayRegistry</code> bundle (as a Kubernetes <code class="literal">Secret</code>) while indicating which components are unmanaged.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The Red Hat Quay config editor can also be used to create or modify an existing config bundle and simplifies the process of updating the Kubernetes <code class="literal">Secret</code>, especially for multiple changes. When Red Hat Quay’s configuration is changed by the config editor and sent to the Red Hat Quay Operator, the deployment is updated to reflect the new configuration.
				</p></div></div></section><section class="section" id="operator-config-bundle-secret"><div class="titlepage"><div><div><h2 class="title">1.5. Config bundle secret</h2></div></div></div><p>
				The <code class="literal">spec.configBundleSecret</code> field is a reference to the <code class="literal">metadata.name</code> of a <code class="literal">Secret</code> in the same namespace as the <code class="literal">QuayRegistry</code>. This <code class="literal">Secret</code> must contain a <code class="literal">config.yaml</code> key/value pair. This <code class="literal">config.yaml</code> file is a Red Hat Quay <code class="literal">config.yaml</code> file. This field is optional, and is auto-filled by the Red Hat Quay Operator if not provided. If provided, it serves as the base set of config fields which are later merged with other fields from any managed components to form a final output <code class="literal">Secret</code>, which is then mounted into the Red Hat Quay application pods.
			</p></section><section class="section" id="operator-prereq"><div class="titlepage"><div><div><h2 class="title">1.6. Prerequisites for Red Hat Quay on OpenShift Container Platform</h2></div></div></div><p>
				Before you begin the deployment of Red Hat Quay Operator on OpenShift Container Platform, you should consider the following.
			</p><section class="section" id="openshift-cluster"><div class="titlepage"><div><div><h3 class="title">1.6.1. OpenShift cluster</h3></div></div></div><p>
					You need a privileged account to an OpenShift Container Platform 4.5 or later cluster on which to deploy the Red Hat Quay Operator. That account must have the ability to create namespaces at the cluster scope.
				</p></section><section class="section" id="resource-requirements"><div class="titlepage"><div><div><h3 class="title">1.6.2. Resource Requirements</h3></div></div></div><p>
					Each Red Hat Quay application pod has the following resource requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							8 Gi of memory
						</li><li class="listitem">
							2000 millicores of CPU.
						</li></ul></div><p>
					The Red Hat Quay Operator creates at least one application pod per Red Hat Quay deployment it manages. Ensure your OpenShift Container Platform cluster has sufficient compute resources for these requirements.
				</p></section><section class="section" id="object-storage"><div class="titlepage"><div><div><h3 class="title">1.6.3. Object Storage</h3></div></div></div><p>
					By default, the Red Hat Quay Operator uses the <code class="literal">ObjectBucketClaim</code> Kubernetes API to provision object storage. Consuming this API decouples the Red Hat Quay Operator from any vendor-specific implementation. Red Hat OpenShift Data Foundation provides this API through its NooBaa component, which will be used in this example.
				</p><p>
					Red Hat Quay can be manually configured to use any of the following supported cloud storage options:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Amazon S3 (see <a class="link" href="https://access.redhat.com/solutions/3680151">S3 IAM Bucket Policy</a> for details on configuring an S3 bucket policy for Red Hat Quay)
						</li><li class="listitem">
							MicroShift Azure Blob Storage
						</li><li class="listitem">
							Google Cloud Storage
						</li><li class="listitem">
							Ceph Object Gateway (RADOS)
						</li><li class="listitem">
							OpenStack Swift
						</li><li class="listitem">
							CloudFront + S3
						</li></ul></div></section></section></section><section class="chapter" id="operator-install"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Installing the Red Hat Quay Operator from the OperatorHub</h1></div></div></div><p>
			Use the following procedure to install the Red Hat Quay Operator from the OpenShift Container Platform OperatorHub.
		</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					Using the OpenShift Container Platform console, select <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
				</li><li class="listitem">
					In the search box, type <span class="strong strong"><strong>Red Hat Quay</strong></span> and select the official Red Hat Quay Operator provided by Red Hat. This directs you to the <span class="strong strong"><strong>Installation</strong></span> page, which outlines the features, prerequisites, and deployment information.
				</li><li class="listitem">
					Select <span class="strong strong"><strong>Install</strong></span>. This directs you to the <span class="strong strong"><strong>Operator Installation</strong></span> page.
				</li><li class="listitem"><p class="simpara">
					The following choices are available for customizing the installation:
				</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
							<span class="strong strong"><strong>Update Channel:</strong></span> Choose the update channel, for example, <code class="literal">stable-3.7</code> for the latest release.
						</li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Installation Mode:</strong></span> Choose <code class="literal">All namespaces on the cluster</code> if you want the Red Hat Quay Operator to be available cluster-wide. Choose <code class="literal">A specific namespace on the cluster</code> if you want it deployed only within a single namespace. It is recommended that you install the Red Hat Quay Operator cluster-wide. If you choose a single namespace, the monitoring component will not be available by default.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Approval Strategy:</strong></span> Choose to approve either automatic or manual updates. Automatic update strategy is recommended.
								</li></ul></div></li></ol></div></li><li class="listitem">
					Select <span class="strong strong"><strong>Install</strong></span>.
				</li></ol></div></section><section class="chapter" id="operator-preconfigure"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Configuring Red Hat Quay before deployment</h1></div></div></div><p>
			The Red Hat Quay Operator can manage all of the Red Hat Quay components when deployed on OpenShift Container Platform. This is the default configuration, however, you can manage one or more components externally when you want more control over the set up.
		</p><p>
			Use the following pattern to configure unmanaged Red Hat Quay components.
		</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					Create a <code class="literal">config.yaml</code> configuration file with the appropriate settings.
				</li><li class="listitem"><p class="simpara">
					Create a <code class="literal">Secret</code> using the configuration file by entering the following command:
				</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml config-bundle-secret</pre></li><li class="listitem"><p class="simpara">
					Create a <code class="literal">quayregistry.yaml</code> file, identifying the unmanaged components and also referencing the created <code class="literal">Secret</code>, for example:
				</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">QuayRegistry</code> YAML file</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: config-bundle-secret
  components:
    - kind: objectstorage
      managed: false</pre>
					</p></div></li><li class="listitem"><p class="simpara">
					Deploy the registry by using the <code class="literal">quayregistry.yaml</code> file:
				</p><pre class="programlisting language-terminal">$ oc create -n quay-enterprise -f quayregistry.yaml</pre></li></ol></div><section class="section" id="config-preconfigure-automation"><div class="titlepage"><div><div><h2 class="title">3.1. Pre-configuring Red Hat Quay for automation</h2></div></div></div><p>
				Red Hat Quay supports several configuration options that enable automation. Users can configure these options before deployment to reduce the need for interaction with the user interface.
			</p><section class="section" id="allowing-the-api-to-create-first-user"><div class="titlepage"><div><div><h3 class="title">3.1.1. Allowing the API to create the first user</h3></div></div></div><p>
					To create the first user, users need to set the FEATURE_USER_INITIALIZE parameter to true and call the /api/v1/user/initialize API. Unlike all other registry API calls that require an OAuth token generated by an OAuth application in an existing organization, the API endpoint does not require authentication.
				</p><p>
					Users can use the API to create a user such as <code class="literal">quayadmin</code> after deploying Red Hat Quay, provided no other users have been created. For more information, see <a class="link" href="#using-the-api-to-create-first-user" title="4.1.3.1. Using the API to create the first user">Using the API to create the first user</a>.
				</p></section><section class="section" id="enabling-general-api-access"><div class="titlepage"><div><div><h3 class="title">3.1.2. Enabling general API access</h3></div></div></div><p>
					Users should set the BROWSER_API_CALLS_XHR_ONLY config option to false to allow general access to the Red Hat Quay registry API.
				</p></section><section class="section" id="adding-super-user"><div class="titlepage"><div><div><h3 class="title">3.1.3. Adding a superuser</h3></div></div></div><p>
					After deploying Red Hat Quay, users can create a user and give the first user administrator privileges with full permissions. Users can configure full permissions in advance by using the SUPER_USER configuration object. For example:
				</p><pre class="programlisting language-yaml">...
SERVER_HOSTNAME: quay-server.example.com
SETUP_COMPLETE: true
SUPER_USERS:
  - quayadmin
...</pre></section><section class="section" id="restricting-user-creation"><div class="titlepage"><div><div><h3 class="title">3.1.4. Restricting user creation</h3></div></div></div><p>
					After users have configured a superuser, they can restrict the ability to create new users to the superuser group by setting FEATURE_USER_CREATION to false. For example:
				</p><pre class="programlisting language-yaml">...
FEATURE_USER_INITIALIZE: true
BROWSER_API_CALLS_XHR_ONLY: false
SUPER_USERS:
- quayadmin
FEATURE_USER_CREATION: false
...</pre></section><section class="section" id="enabling-new-functionality-38"><div class="titlepage"><div><div><h3 class="title">3.1.5. Enabling new functionality in Red Hat Quay 3.8</h3></div></div></div><p>
					To use new Red Hat Quay 3.8 functions, enable some or all of the following features:
				</p><pre class="programlisting language-yaml">...
FEATURE_UI_V2: true
FEATURE_LISTEN_IP_VERSION:
FEATURE_SUPERUSERS_FULL_ACCESS: true
GLOBAL_READONLY_SUPER_USERS:
      -
FEATURE_RESTRICTED_USERS: true
RESTRICTED_USERS_WHITELIST:
      -
...</pre></section><section class="section" id="enabling-new-functionality-37"><div class="titlepage"><div><div><h3 class="title">3.1.6. Enabling new functionality in Red Hat Quay 3.7</h3></div></div></div><p>
					To use new Red Hat Quay 3.7 functions, enable some or all of the following features:
				</p><pre class="programlisting language-yaml">...
FEATURE_QUOTA_MANAGEMENT: true
FEATURE_BUILD_SUPPORT: true
FEATURE_PROXY_CACHE: true
FEATURE_STORAGE_REPLICATION: true
DEFAULT_SYSTEM_REJECT_QUOTA_BYTES: 102400000
...</pre></section><section class="section" id="suggested-configuration-for-automation"><div class="titlepage"><div><div><h3 class="title">3.1.7. Suggested configuration for automation</h3></div></div></div><p>
					The following <code class="literal">config.yaml</code> parameters are suggested for automation:
				</p><pre class="programlisting language-yaml">...
FEATURE_USER_INITIALIZE: true
BROWSER_API_CALLS_XHR_ONLY: false
SUPER_USERS:
- quayadmin
FEATURE_USER_CREATION: false
...</pre></section></section><section class="section" id="operator-storage-preconfig"><div class="titlepage"><div><div><h2 class="title">3.2. Configuring object storage</h2></div></div></div><p>
				You need to configure object storage before installing Red Hat Quay, irrespective of whether you are allowing the Red Hat Quay Operator to manage the storage or managing it yourself.
			</p><p>
				If you want the Red Hat Quay Operator to be responsible for managing storage, see the section on <a class="link" href="#operator-managed-storage" title="3.2.2. Managed storage">Managed storage</a> for information on installing and configuring NooBaa and the Red Hat OpenShift Data Foundations Operator.
			</p><p>
				If you are using a separate storage solution, set <code class="literal">objectstorage</code> as <code class="literal">unmanaged</code> when configuring the Operator. See the following section. <a class="link" href="#operator-unmanaged-storage" title="3.2.1. Unmanaged storage">Unmanaged storage</a>, for details of configuring existing storage.
			</p><section class="section" id="operator-unmanaged-storage"><div class="titlepage"><div><div><h3 class="title">3.2.1. Unmanaged storage</h3></div></div></div><p>
					This section provides configuration examples for unmanaged storage for your convenience. Refer to the Red Hat Quay configuration guide for complete instructions on how to set up object storage.
				</p><section class="section" id="aws-s3-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.1. AWS S3 storage</h4></div></div></div><p>
						Use the following example when configuring AWS S3 storage for your Red Hat Quay deployment.
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
  s3Storage:
    - S3Storage
    - host: s3.us-east-2.amazonaws.com
      s3_access_key: ABCDEFGHIJKLMN
      s3_secret_key: OL3ABCDEFGHIJKLMN
      s3_bucket: quay_bucket
      storage_path: /datastorage/registry
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE:
    - s3Storage</pre></section><section class="section" id="gcp-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.2. Google Cloud storage</h4></div></div></div><p>
						Use the following example when configuring Google Cloud storage for your Red Hat Quay deployment.
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
    googleCloudStorage:
        - GoogleCloudStorage
        - access_key: GOOGQIMFB3ABCDEFGHIJKLMN
          bucket_name: quay-bucket
          secret_key: FhDAYe2HeuAKfvZCAGyOioNaaRABCDEFGHIJKLMN
          storage_path: /datastorage/registry
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE:
    - googleCloudStorage</pre></section><section class="section" id="azure-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.3. Microsoft Azure storage</h4></div></div></div><p>
						Use the following example when configuring Microsoft Azure storage for your Red Hat Quay deployment.
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
  azureStorage:
    - AzureStorage
    - azure_account_name: azure_account_name_here
      azure_container: azure_container_here
      storage_path: /datastorage/registry
      azure_account_key: azure_account_key_here
      sas_token: some/path/
      endpoint_url: https://[account-name].blob.core.usgovcloudapi.net <span id="CO1-1"/><span class="callout">1</span>
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE:
    - azureStorage</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">endpoint_url</code> parameter for Microsoft Azure storage is optional and can be used with Microsoft Azure Government (MAG) endpoints. If left blank, the <code class="literal">endpoint_url</code> will connect to the normal Microsoft Azure region.
							</div><p>
								As of Red Hat Quay 3.7, you must use the Primary endpoint of your MAG Blob service. Using the Secondary endpoint of your MAG Blob service will result in the following error: <code class="literal">AuthenticationErrorDetail:Cannot find the claimed account when trying to GetProperties for the account whusc8-secondary</code>.
							</p></dd></dl></div></section><section class="section" id="ceph-rados-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.4. Ceph/RadosGW Storage</h4></div></div></div><p>
						Use the following example when configuring Ceph/RadosGW storage for your Red Hat Quay deployment.
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
  radosGWStorage: #storage config name
    - RadosGWStorage #actual driver
    - access_key: access_key_here #parameters
      secret_key: secret_key_here
      bucket_name: bucket_name_here
      hostname: hostname_here
      is_secure: 'true'
      port: '443'
      storage_path: /datastorage/registry
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE: #must contain name of the storage config
    - radosGWStorage</pre></section><section class="section" id="swift-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.5. Swift storage</h4></div></div></div><p>
						Use the following example when configuring Swift storage for your Red Hat Quay deployment.
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
  swiftStorage:
    - SwiftStorage
    - swift_user: swift_user_here
      swift_password: swift_password_here
      swift_container: swift_container_here
      auth_url: https://example.org/swift/v1/quay
      auth_version: 1
      ca_cert_path: /conf/stack/swift.cert"
      storage_path: /datastorage/registry
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE:
    - swiftStorage</pre></section><section class="section" id="noobaa-unmanaged-storage-example"><div class="titlepage"><div><div><h4 class="title">3.2.1.6. NooBaa unmanaged storage</h4></div></div></div><p>
						Use the following procedure to deploy NooBaa as your unmanaged storage configuration.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a NooBaa Object Bucket Claim in the Red Hat Quay console by navigating to <span class="strong strong"><strong>Storage</strong></span> → <span class="strong strong"><strong>Object Bucket Claims</strong></span>.
							</li><li class="listitem">
								Retrieve the Object Bucket Claim Data details, including the Access Key, Bucket Name, Endpoint (hostname), and Secret Key.
							</li><li class="listitem"><p class="simpara">
								Create a <code class="literal">config.yaml</code> configuration file that uses the information for the Object Bucket Claim:
							</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
  default:
    - RHOCSStorage
    - access_key: WmrXtSGk8B3nABCDEFGH
      bucket_name: my-noobaa-bucket-claim-8b844191-dc6c-444e-9ea4-87ece0abcdef
      hostname: s3.openshift-storage.svc.cluster.local
      is_secure: true
      port: "443"
      secret_key: X9P5SDGJtmSuHFCMSLMbdNCMfUABCDEFGH+C5QD
      storage_path: /datastorage/registry
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: []
DISTRIBUTED_STORAGE_PREFERENCE:
  - default</pre></li></ol></div><p>
						For more information about configuring an Object Bucket Claim, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.8/html-single/managing_hybrid_and_multicloud_resources/index#object-bucket-claim">Object Bucket Claim</a>.
					</p></section></section><section class="section" id="operator-managed-storage"><div class="titlepage"><div><div><h3 class="title">3.2.2. Managed storage</h3></div></div></div><p>
					If you want the Red Hat Quay Operator to manage object storage for Red Hat Quay, your cluster needs to be capable of providing object storage through the <code class="literal">ObjectBucketClaim</code> API. Using the Red Hat OpenShift Data Foundation Operator, there are two supported options available:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							A standalone instance of the Multi-Cloud Object Gateway backed by a local Kubernetes <code class="literal">PersistentVolume</code> storage
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Not highly available
								</li><li class="listitem">
									Included in the Red Hat Quay subscription
								</li><li class="listitem">
									Does not require a separate subscription for Red Hat OpenShift Data Foundation
								</li></ul></div></li><li class="listitem"><p class="simpara">
							A production deployment of Red Hat OpenShift Data Foundation with scale-out Object Service and Ceph
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Highly available
								</li><li class="listitem">
									Requires a separate subscription for Red Hat OpenShift Data Foundation
								</li></ul></div></li></ul></div><p>
					To use the standalone instance option, continue reading below. For production deployment of Red Hat OpenShift Data Foundation, please refer to the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/">official documentation</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Object storage disk space is allocated automatically by the Red Hat Quay Operator with 50 GiB. This number represents a usable amount of storage for most small to medium Red Hat Quay installations but might not be sufficient for your use cases. Resizing the Red Hat OpenShift Data Foundation volume is currently not handled by the Red Hat Quay Operator. See the section below about resizing managed storage for more details.
					</p></div></div><span style="color: red">&lt;operator-standalone-object-gateway&gt;<span style="color: red">&lt;title&gt;About The Standalone Object Gateway&lt;/title&gt;</span>
				<p>
					As part of a Red Hat Quay subscription, users are entitled to use the <span class="emphasis"><em>Multi-Cloud Object Gateway</em></span> (MCG) component of the Red Hat OpenShift Data Foundation Operator (formerly known as OpenShift Container Storage Operator). This gateway component allows you to provide an S3-compatible object storage interface to {productnamed} backed by Kubernetes <code class="literal">PersistentVolume</code>-based block storage. The usage is limited to a Red Hat Quay deployment managed by the Operator and to the exact specifications of the MCG instance as documented below.
				</p>
				<p>
					Since Red Hat Quay does not support local filesystem storage, users can leverage the gateway in combination with Kubernetes <code class="literal">PersistentVolume</code> storage instead, to provide a supported deployment. A <code class="literal">PersistentVolume</code> is directly mounted on the gateway instance as a backing store for object storage and any block-based <code class="literal">StorageClass</code> is supported.
				</p>
				<p>
					By the nature of <code class="literal">PersistentVolume</code>, this is not a scale-out, highly available solution and does not replace a scale-out storage system like Red Hat OpenShift Data Foundation (ODF). Only a single instance of the gateway is running. If the pod running the gateway becomes unavailable due to rescheduling, updates or unplanned downtime, this will cause temporary degradation of the connected Quay instances.
				</p>
				<section class="section" id="creating-standalone-object-gateway"><div class="titlepage"><div><div><h5 class="title">1. Create A Standalone Object Gateway</h5></div></div></div><p>
						To install t (formerly known as OpenShift Container Storage) Operator and configure a single instance Multi-Cloud Gateway service, follow these steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Open the OpenShift console and select Operators → OperatorHub, then select the OpenShift Data Foundation Operator.
							</li><li class="listitem">
								Select Install. Accept all default options and select Install again.
							</li><li class="listitem"><p class="simpara">
								Within a minute, the Operator will install and create a namespace <code class="literal">openshift-storage</code>. You can confirm it has completed when the <code class="literal">Status</code> column is marked <code class="literal">Succeeded</code>.
							</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
									When the installation of the ODF Operator is complete, you are prompted to create a storage system. Do not follow this instruction. Instead, create NooBaa object storage as outlined the following steps.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Create NooBaa object storage. Save the following YAML to a file called <code class="literal">noobaa.yaml</code>.
							</p><pre class="screen">apiVersion: noobaa.io/v1alpha1
kind: NooBaa
metadata:
  name: noobaa
  namespace: openshift-storage
spec:
 dbResources:
   requests:
     cpu: '0.1'
     memory: 1Gi
 dbType: postgres
 coreResources:
   requests:
     cpu: '0.1'
     memory: 1Gi</pre><p class="simpara">
								This will create a single instance deployment of the <span class="emphasis"><em>Multi-cloud Object Gateway</em></span>.
							</p></li><li class="listitem"><p class="simpara">
								Apply the configuration with the following command:
							</p><pre class="screen">$ oc create -n openshift-storage -f noobaa.yaml
noobaa.noobaa.io/noobaa created</pre></li><li class="listitem"><p class="simpara">
								After a couple of minutes, you should see that the MCG instance has finished provisioning (<code class="literal">PHASE</code> column will be set to <code class="literal">Ready</code>):
							</p><pre class="screen">$ oc get -n openshift-storage noobaas noobaa -w
NAME     MGMT-ENDPOINTS              S3-ENDPOINTS                IMAGE                                                                                                            PHASE   AGE
noobaa   [https://10.0.32.3:30318]   [https://10.0.32.3:31958]   registry.redhat.io/ocs4/mcg-core-rhel8@sha256:56624aa7dd4ca178c1887343c7445a9425a841600b1309f6deace37ce6b8678d   Ready   3d18h</pre></li><li class="listitem"><p class="simpara">
								Next, configure a backing store for the gateway. Save the following YAML to a file called <code class="literal">noobaa-pv-backing-store.yaml</code>.
							</p><div class="formalpara"><p class="title"><strong>noobaa-pv-backing-store.yaml</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: noobaa.io/v1alpha1
kind: BackingStore
metadata:
  finalizers:
  - noobaa.io/finalizer
  labels:
    app: noobaa
  name: noobaa-pv-backing-store
  namespace: openshift-storage
spec:
  pvPool:
    numVolumes: 1
    resources:
      requests:
        storage: 50Gi <span id="CO2-1"/><span class="callout">1</span>
    storageClass: STORAGE-CLASS-NAME <span id="CO2-2"/><span class="callout">2</span>
  type: pv-pool</pre>
								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The overall capacity of the object storage service, adjust as needed
									</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal">StorageClass</code> to use for the <code class="literal">PersistentVolumes</code> requested, delete this property to use the cluster default
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the configuration with the following command:
							</p><pre class="screen">$ oc create -f noobaa-pv-backing-store.yaml
backingstore.noobaa.io/noobaa-pv-backing-store created</pre><p class="simpara">
								This creates the backing store configuration for the gateway. All images in Quay will be stored as objects through the gateway in a <code class="literal">PersistentVolume</code> created by the above configuration.
							</p></li><li class="listitem"><p class="simpara">
								Finally, run the following command to make the <code class="literal">PersistentVolume</code> backing store the default for all <code class="literal">ObjectBucketClaims</code> issued by the Operator.
							</p><pre class="screen">$ oc patch bucketclass noobaa-default-bucket-class --patch '{"spec":{"placementPolicy":{"tiers":[{"backingStores":["noobaa-pv-backing-store"]}]}}}' --type merge -n openshift-storage</pre></li></ol></div><p>
						This concludes the setup of the <span class="emphasis"><em>Multi-Cloud Object Gateway</em></span> instance for Red Hat Quay. Note that this configuration cannot be run in parallel on a cluster with Red Hat OpenShift Data Foundation installed.
					</p></section>
				&lt;/operator-standalone-object-gateway&gt;</span></section></section><section class="section" id="configuring-the-database-poc"><div class="titlepage"><div><div><h2 class="title">3.3. Configuring the database</h2></div></div></div><section class="section" id="operator-unmanaged-postgres"><div class="titlepage"><div><div><h3 class="title">3.3.1. Using an existing PostgreSQL database</h3></div></div></div><p>
					If you are using an externally managed PostgreSQL database, you must manually enable the <code class="literal">pg_trgm</code> extension for a successful deployment.
				</p><p>
					Use the following procedure to deploy an existing PostgreSQL database.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">config.yaml</code> file with the necessary database fields. For example:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">config.yaml</code> file:</strong></p><p>
								
<pre class="programlisting language-yaml">DB_URI: postgresql://test-quay-database:postgres@test-quay-database:5432/test-quay-database</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">Secret</code> using the configuration file:
						</p><pre class="screen">$ kubectl create secret generic --from-file config.yaml=./config.yaml config-bundle-secret</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">QuayRegistry</code> YAML file which marks the <code class="literal">postgres</code> component as <code class="literal">unmanaged</code> and references the created <code class="literal">Secret</code>. For example:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">quayregistry.yaml</code> file</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: config-bundle-secret
  components:
    - kind: postgres
      managed: false</pre>
							</p></div></li><li class="listitem">
							Deploy the registry as detailed in the following sections.
						</li></ol></div></section><section class="section" id="config-fields-db"><div class="titlepage"><div><div><h3 class="title">3.3.2. Database configuration</h3></div></div></div><p>
					This section describes the database configuration fields available for Red Hat Quay deployments.
				</p><section class="section" id="database-uri"><div class="titlepage"><div><div><h4 class="title">3.3.2.1. Database URI</h4></div></div></div><p>
						With Red Hat Quay, connection to the database is configured by using the required <code class="literal">DB_URI</code> field.
					</p><p>
						The following table describes the <code class="literal">DB_URI</code> configuration field:
					</p><div class="table" id="idm45898219744112"><p class="title"><strong>Table 3.1. Database URI</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 17%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45898218912608" scope="col">Field</th><th align="left" valign="top" id="idm45898218911520" scope="col">Type</th><th align="left" valign="top" id="idm45898218910432" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45898218912608">
									<p>
										<span class="strong strong"><strong>DB_URI</strong></span><br/> (Required)
									</p>
									</td><td align="left" valign="top" headers="idm45898218911520">
									<p>
										String
									</p>
									</td><td align="left" valign="top" headers="idm45898218910432">
									<p>
										The URI for accessing the database, including any credentials.
									</p>
									<p>
										Example <code class="literal">DB_URI</code> field:
									</p>
									<p>
										<span class="strong strong"><strong>postgresql://quayuser:quaypass@quay-server.example.com:5432/quay</strong></span>
									</p>
									</td></tr></tbody></table></div></div></section><section class="section" id="database-connection-arguments"><div class="titlepage"><div><div><h4 class="title">3.3.2.2. Database connection arguments</h4></div></div></div><p>
						Optional connection arguments are configured by the <code class="literal">DB_CONNECTION_ARGS</code> parameter. Some of the key-value pairs defined under <code class="literal">DB_CONNECTION_ARGS</code> are generic, while others are database specific.
					</p><p>
						The following table describes database connection arguments:
					</p><div class="table" id="idm45898219142288"><p class="title"><strong>Table 3.2. Database connection arguments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 17%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45898215808960" scope="col">Field</th><th align="left" valign="top" id="idm45898215807872" scope="col">Type</th><th align="left" valign="top" id="idm45898215806784" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45898215808960">
									<p>
										<span class="strong strong"><strong>DB_CONNECTION_ARGS</strong></span>
									</p>
									</td><td align="left" valign="top" headers="idm45898215807872">
									<p>
										Object
									</p>
									</td><td align="left" valign="top" headers="idm45898215806784">
									<p>
										Optional connection arguments for the database, such as timeouts and SSL/TLS.
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45898215808960">
									<p>
										<span class="strong strong"><strong>.autorollback</strong></span>
									</p>
									</td><td align="left" valign="top" headers="idm45898215807872">
									<p>
										Boolean
									</p>
									</td><td align="left" valign="top" headers="idm45898215806784">
									<p>
										Whether to use thread-local connections.<br/> Should always be <code class="literal">true</code>
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45898215808960">
									<p>
										<span class="strong strong"><strong>.threadlocals</strong></span>
									</p>
									</td><td align="left" valign="top" headers="idm45898215807872">
									<p>
										Boolean
									</p>
									</td><td align="left" valign="top" headers="idm45898215806784">
									<p>
										Whether to use auto-rollback connections.<br/> Should always be <code class="literal">true</code>
									</p>
									</td></tr></tbody></table></div></div><section class="section" id="config-fields-postgres"><div class="titlepage"><div><div><h5 class="title">3.3.2.2.1. PostgreSQL SSL/TLS connection arguments</h5></div></div></div><p>
							With SSL/TLS, configuration depends on the database you are deploying. The following example shows a PostgreSQL SSL/TLS configuration:
						</p><pre class="programlisting language-yaml">DB_CONNECTION_ARGS:
  sslmode: verify-ca
  sslrootcert: /path/to/cacert</pre><p>
							The <code class="literal">sslmode</code> option determines whether, or with, what priority a secure SSL/TLS TCP/IP connection will be negotiated with the server. There are six modes:
						</p><div class="table" id="idm45898219719200"><p class="title"><strong>Table 3.3. SSL/TLS options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45898218922576" scope="col">Mode</th><th align="left" valign="top" id="idm45898218921488" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>disable</strong></span>
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Your configuration only tries non-SSL/TLS connections.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>allow</strong></span>
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Your configuration first tries a non-SSL/TLS connection. Upon failure, tries an SSL/TLS connection.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>prefer</strong></span><br/> (Default)
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Your configuration first tries an SSL/TLS connection. Upon failure, tries a non-SSL/TLS connection.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>require</strong></span>
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Your configuration only tries an SSL/TLS connection. If a root CA file is present, it verifies the certificate in the same way as if verify-ca was specified.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>verify-ca</strong></span>
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Your configuration only tries an SSL/TLS connection, and verifies that the server certificate is issued by a trusted certificate authority (CA).
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45898218922576">
										<p>
											<span class="strong strong"><strong>verify-full</strong></span>
										</p>
										</td><td align="left" valign="top" headers="idm45898218921488">
										<p>
											Only tries an SSL/TLS connection, and verifies that the server certificate is issued by a trusted CA and that the requested server hostname matches that in the certificate.
										</p>
										</td></tr></tbody></table></div></div><p>
							For more information on the valid arguments for PostgreSQL, see <a class="link" href="https://www.postgresql.org/docs/current/libpq-connect.html">Database Connection Control Functions</a>.
						</p></section><section class="section" id="mysql-ssl-connection-arguments"><div class="titlepage"><div><div><h5 class="title">3.3.2.2.2. MySQL SSL/TLS connection arguments</h5></div></div></div><p>
							The following example shows a sample MySQL SSL/TLS configuration:
						</p><pre class="screen yaml yaml">DB_CONNECTION_ARGS:
  ssl:
    ca: /path/to/cacert</pre><p>
							Information on the valid connection arguments for MySQL is available at <a class="link" href="https://dev.mysql.com/doc/refman/8.0/en/connecting-using-uri-or-key-value-pairs.html">Connecting to the Server Using URI-Like Strings or Key-Value Pairs</a>.
						</p></section></section></section><section class="section" id="operator-managed-postgres"><div class="titlepage"><div><div><h3 class="title">3.3.3. Using the managed PostgreSQL database</h3></div></div></div><p>
					With Red Hat Quay 3.9, if your database is managed by the {productnmame} Operator, updating from Red Hat Quay 3.8 → 3.9 automatically handles upgrading PostgreSQL 10 to PostgreSQL 13. This automatic procedure works by first upgrading your PostgreSQL 10 to PostgreSQL 12, and then from PostgreSQL 12 to PostgreSQL 13.
				</p><p>
					+
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Users with a managed database are required to upgrade their PostgreSQL database from 10 → 13.
					</p></div></div><p>
					+ If you do not want the Red Hat Quay Operator to upgrade your PostgreSQL deployment from PostgreSQL 10 → 13, you must set the PostgreSQL parameter to <code class="literal">managed: false</code> in your <code class="literal">quayregistry.yaml</code> file. For more information about setting your database to unmanaged, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/configure_red_hat_quay/index#operator-unmanaged-postgres">Using an existing Postgres database</a>.
				</p><p>
					+
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								It is highly recommended to upgrade your PostgreSQL database to version 13. PostgreSQL 10 will reach end of life in May, 2024.
							</li></ul></div></div></div><p>
					+ If you want your PostgreSQL database to match the same version as your Red Hat Enterprise Linux (RHEL) system, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/using-databases#migrating-to-a-rhel-8-version-of-postgresql_using-postgresql">Migrating to a RHEL 8 version of PostgreSQL</a> for RHEL 8 or <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_using_database_servers/using-postgresql_configuring-and-using-database-servers#migrating-to-a-rhel-9-version-of-postgresql_using-postgresql">Migrating to a RHEL 9 version of PostgreSQL</a> for RHEL 9.
				</p><section class="section" id="operator-managed-postgres-recommendations"><div class="titlepage"><div><div><h4 class="title">3.3.3.1. PostgreSQL database recommendations</h4></div></div></div><p>
						The Red Hat Quay team recommends the following for managing your PostgreSQL database.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Database backups should be performed regularly using either the supplied tools on the PostgreSQL image or your own backup infrastructure. The Red Hat Quay Operator does not currently ensure that the PostgreSQL database is backed up.
							</li><li class="listitem">
								Restoring the PostgreSQL database from a backup must be done using PostgreSQL tools and procedures. Be aware that your <code class="literal">Quay</code> pods should not be running while the database restore is in progress.
							</li><li class="listitem">
								Database disk space is allocated automatically by the Red Hat Quay Operator with 50 GiB. This number represents a usable amount of storage for most small to medium Red Hat Quay installations but might not be sufficient for your use cases. Resizing the database volume is currently not handled by the Red Hat Quay Operator.
							</li></ul></div></section></section></section><section class="section" id="operator-preconfig-tls-routes"><div class="titlepage"><div><div><h2 class="title">3.4. Configuring SSL/TLS and Routes</h2></div></div></div><p>
				Support for OpenShift Container Platform Edge-Termination Routes has been added by way of a new managed component, <code class="literal">tls</code>. This separates the <code class="literal">route</code> component from SSL/TLS and allows users to configure both separately.
			</p><p>
				<code class="literal">EXTERNAL_TLS_TERMINATION: true</code> is the opinionated setting.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Managed <code class="literal">tls</code> means that the default cluster wildcard certificate is used.
					</li><li class="listitem">
						Unmanaged <code class="literal">tls</code> means that the user provided key and certificate pair is be injected into the <code class="literal">Route</code>.
					</li></ul></div><p>
				The <code class="literal">ssl.cert</code> and <code class="literal">ssl.key</code> are now moved to a separate, persistent secret, which ensures that the key and certificate pair are not re-generated upon every reconcile. The key and certificate pair are now formatted as <code class="literal">edge</code> routes and mounted to the same directory in the <code class="literal">Quay</code> container.
			</p><p>
				Multiple permutations are possible when configuring SSL/TLS and Routes, but the following rules apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If SSL/TLS is <code class="literal">managed</code>, then your route must also be <code class="literal">managed</code>
					</li><li class="listitem">
						If SSL/TLS is <code class="literal">unmanaged</code> then you must supply certificates, either with the config tool or directly in the config bundle
					</li></ul></div><p>
				The following table describes the valid options:
			</p><div class="table" id="idm45898211720704"><p class="title"><strong>Table 3.4. Valid configuration options for TLS and routes</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 18%; " class="col_1"/><col style="width: 18%; " class="col_2"/><col style="width: 18%; " class="col_3"/><col style="width: 18%; " class="col_4"/><col style="width: 27%; " class="col_5"/></colgroup><thead><tr><th align="left" valign="top" id="idm45898219308880" scope="col">Option</th><th align="left" valign="top" id="idm45898219307792" scope="col">Route</th><th align="left" valign="top" id="idm45898219306704" scope="col">TLS</th><th align="left" valign="top" id="idm45898219305616" scope="col">Certs provided</th><th align="left" valign="top" id="idm45898219195552" scope="col">Result</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45898219308880">
							<p>
								My own load balancer handles TLS
							</p>
							</td><td align="left" valign="top" headers="idm45898219307792">
							<p>
								Managed
							</p>
							</td><td align="left" valign="top" headers="idm45898219306704">
							<p>
								Managed
							</p>
							</td><td align="left" valign="top" headers="idm45898219305616">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45898219195552">
							<p>
								Edge Route with default wildcard cert
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45898219308880">
							<p>
								Red Hat Quay handles TLS
							</p>
							</td><td align="left" valign="top" headers="idm45898219307792">
							<p>
								Managed
							</p>
							</td><td align="left" valign="top" headers="idm45898219306704">
							<p>
								Unmanaged
							</p>
							</td><td align="left" valign="top" headers="idm45898219305616">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45898219195552">
							<p>
								Passthrough route with certs mounted inside the pod
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45898219308880">
							<p>
								Red Hat Quay handles TLS
							</p>
							</td><td align="left" valign="top" headers="idm45898219307792">
							<p>
								Unmanaged
							</p>
							</td><td align="left" valign="top" headers="idm45898219306704">
							<p>
								Unmanaged
							</p>
							</td><td align="left" valign="top" headers="idm45898219305616">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45898219195552">
							<p>
								Certificates are set inside the quay pod but route must be created manually
							</p>
							</td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat Quay 3.7 does not support builders when TLS is managed by the Operator.
				</p></div></div><section class="section" id="creating-config-bundle-secret-tls-cert-key-pair"><div class="titlepage"><div><div><h3 class="title">3.4.1. Creating the config bundle secret with the SSL/TLS cert and key pair</h3></div></div></div><p>
					Use the following procedure to create a config bundle secret that includes your own SSL/TLS certificate and key pair.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Enter the following command to create config bundle secret that includes your own SSL/TLS certificate and key pair:
						</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml --from-file ssl.cert=./ssl.cert --from-file ssl.key=./ssl.key config-bundle-secret</pre></li></ul></div></section></section><section class="section" id="operator-components-unmanaged-other"><div class="titlepage"><div><div><h2 class="title">3.5. Configuring external Redis</h2></div></div></div><p>
				Use the content in this section to set up an external Redis deployment.
			</p><section class="section" id="operator-unmanaged-redis"><div class="titlepage"><div><div><h3 class="title">3.5.1. Using external Redis</h3></div></div></div><p>
					Use the following procedure to set up an external Redis database.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">config.yaml</code> file using the following Redis fields:
						</p><pre class="programlisting language-yaml">BUILDLOGS_REDIS:
    host: quay-server.example.com
    port: 6379
    ssl: false

USER_EVENTS_REDIS:
    host: quay-server.example.com
    port: 6379
    ssl: false</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to create a secret using the configuration file:
						</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml config-bundle-secret</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">quayregistry.yaml</code> file that sets the Redis component to <code class="literal">unmanaged</code> and references the created secret:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: config-bundle-secret
  components:
    - kind: redis
      managed: false</pre></li><li class="listitem">
							Deploy the Red Hat Quay registry.
						</li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/configure_red_hat_quay/index.xml#config-fields-redis">Redis configuration fields</a>
					</p></div></section><section class="section" id="operator-unmanaged-hpa"><div class="titlepage"><div><div><h3 class="title">3.5.2. Horizontal Pod Autoscaler</h3></div></div></div><p>
					Horizontal Pod Autoscalers (HPAs) have been added to the <code class="literal">Clair</code>, <code class="literal">Quay</code>, and <code class="literal">Mirror</code> pods, so that they now automatically scale during load spikes.
				</p><p>
					As HPA is configured by default to be <code class="literal">managed</code>, the number of <code class="literal">Clair</code>, <code class="literal">Quay</code>, and <code class="literal">Mirror</code> pods is set to two. This facilitates the avoidance of downtime when updating or reconfiguring Red Hat Quay by the Operator or during rescheduling events.
				</p><section class="section" id="operator-disabling-hpa"><div class="titlepage"><div><div><h4 class="title">3.5.2.1. Disabling the Horizontal Pod Autoscaler</h4></div></div></div><p>
						To disable autoscaling or create your own <code class="literal">HorizontalPodAutoscaler</code>, specify the component as <code class="literal">unmanaged</code> in the <code class="literal">QuayRegistry</code> instance. For example:
					</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  components:
    - kind: horizontalpodautoscaler
      managed: false</pre></section></section><section class="section" id="operator-unmanaged-route"><div class="titlepage"><div><div><h3 class="title">3.5.3. Disabling Route Component</h3></div></div></div><p>
					Use the following procedure to prevent the Red Hat Quay Operator from creating a route.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Set the component as <code class="literal">unmanaged</code> in the <code class="literal">quayregistry.yaml</code> file:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  components:
    - kind: route
      managed: false</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">config.yaml</code> file to specify that Red Hat Quay handles SSL/TLS. For example:
						</p><pre class="programlisting language-yaml">...
EXTERNAL_TLS_TERMINATION: false
...
SERVER_HOSTNAME: example-registry-quay-quay-enterprise.apps.user1.example.com
...
PREFERRED_URL_SCHEME: https
...</pre><p class="simpara">
							If you do not configure the <code class="literal">unmanaged</code> route correctly, the following error is returned:
						</p><pre class="programlisting language-json">{
  {
    "kind":"QuayRegistry",
    "namespace":"quay-enterprise",
    "name":"example-registry",
    "uid":"d5879ba5-cc92-406c-ba62-8b19cf56d4aa",
    "apiVersion":"quay.redhat.com/v1",
    "resourceVersion":"2418527"
  },
  "reason":"ConfigInvalid",
  "message":"required component `route` marked as unmanaged, but `configBundleSecret` is missing necessary fields"
}</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Disabling the default route means you are now responsible for creating a <code class="literal">Route</code>, <code class="literal">Service</code>, or <code class="literal">Ingress</code> in order to access the Red Hat Quay instance. Additionally, whatever DNS you use must match the <code class="literal">SERVER_HOSTNAME</code> in the Red Hat Quay config.
					</p></div></div></section><section class="section" id="operator-unmanaged-monitoring"><div class="titlepage"><div><div><h3 class="title">3.5.4. Unmanaged monitoring</h3></div></div></div><p>
					If you install the Red Hat Quay Operator in a single namespace, the monitoring component is automatically set to <code class="literal">unmanaged</code>. Use the following reference to explicitly disable monitoring.
				</p><div class="formalpara"><p class="title"><strong>Unmanaged monitoring</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  components:
    - kind: monitoring
      managed: false</pre>
					</p></div><p>
					To enable monitoring in this scenario, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#monitoring-single-namespace">Enabling monitoring when the Red Hat Quay Operator is installed in a single namespace</a>.
				</p></section><section class="section" id="operator-unmanaged-mirroring"><div class="titlepage"><div><div><h3 class="title">3.5.5. Unmanaged mirroring</h3></div></div></div><p>
					To disable mirroring explicitly, use the following YAML configuration:
				</p><div class="formalpara"><p class="title"><strong>Unmanaged mirroring example YAML configuration</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  components:
    - kind: mirroring
      managed: false</pre>
					</p></div></section></section></section><section class="chapter" id="operator-deploy"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Deploying Red Hat Quay using the Red Hat Quay Operator</h1></div></div></div><p>
			The Red Hat Quay Operator can be deployed from the command line or from the OpenShift Container Platform console, however the steps are fundamentally the same.
		</p><section class="section" id="operator-deploy-cli"><div class="titlepage"><div><div><h2 class="title">4.1. Deploying Red Hat Quay from the command line</h2></div></div></div><p>
				Use the following procedure to deploy Red Hat Quay from using the command-line interface (CLI).
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have logged into OpenShift Container Platform using the CLI.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a namespace, for example, <code class="literal">quay-enterprise</code>, by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc new-project quay-enterprise</pre></li><li class="listitem"><p class="simpara">
						Optional. If you want to pre-configure any aspects of your Red Hat Quay deployment, create a <code class="literal">Secret</code> for the config bundle:
					</p><pre class="programlisting language-terminal">$ oc create secret generic quay-enterprise-config-bundle --from-file=config-bundle.tar.gz=/path/to/config-bundle.tar.gz</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">QuayRegistry</code> custom resource in a file called <code class="literal">quayregistry.yaml</code>
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								For a minimal deployment, using all the defaults:
							</p><div class="formalpara"><p class="title"><strong>quayregistry.yaml:</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Optional. If you want to have some components unmanaged, add this information in the <code class="literal">spec</code> field. A minimal deployment might look like the following example:
							</p><div class="formalpara"><p class="title"><strong>Example quayregistry.yaml with unmanaged components</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  components:
    - kind: clair
      managed: false
    - kind: horizontalpodautoscaler
      managed: false
    - kind: mirror
      managed: false
    - kind: monitoring
      managed: false</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Optional. If you have created a config bundle, for example, <code class="literal">init-config-bundle-secret</code>, reference it in the <code class="literal">quayregistry.yaml</code> file:
							</p><div class="formalpara"><p class="title"><strong>Example quayregistry.yaml with a config bundle</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: init-config-bundle-secret</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Optional. If you have a proxy configured, you can add the information using overrides for Red Hat Quay, Clair, and mirroring:
							</p><div class="formalpara"><p class="title"><strong>Example quayregistry.yaml with proxy configured</strong></p><p>
									
<pre class="programlisting language-yaml">  kind: QuayRegistry
  metadata:
    name: quay37
  spec:
    configBundleSecret: config-bundle-secret
    components:
      - kind: objectstorage
        managed: false
      - kind: route
        managed: true
      - kind: mirror
        managed: true
        overrides:
          env:
            - name: DEBUGLOG
              value: "true"
            - name: HTTP_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128
            - name: HTTPS_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128
            - name: NO_PROXY
              value: svc.cluster.local,localhost,quay370.apps.quayperf370.perfscale.devcluster.openshift.com
      - kind: tls
        managed: false
      - kind: clair
        managed: true
        overrides:
          env:
            - name: HTTP_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128
            - name: HTTPS_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128
            - name: NO_PROXY
              value: svc.cluster.local,localhost,quay370.apps.quayperf370.perfscale.devcluster.openshift.com
      - kind: quay
        managed: true
        overrides:
          env:
            - name: DEBUGLOG
              value: "true"
            - name: NO_PROXY
              value: svc.cluster.local,localhost,quay370.apps.quayperf370.perfscale.devcluster.openshift.com
            - name: HTTP_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128
            - name: HTTPS_PROXY
              value: quayproxy.qe.devcluster.openshift.com:3128</pre>
								</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">QuayRegistry</code> in specified namespace:
					</p><pre class="programlisting language-terminal">$ oc create -n quay-enterprise -f quayregistry.yaml</pre></li><li class="listitem"><p class="simpara">
						Enter the following command to see when the <code class="literal">status.registryEndpoint</code> is populated:
					</p><pre class="programlisting language-terminal">$ oc get quayregistry -n quay-enterprise example-registry -o jsonpath="{.status.registryEndpoint}" -w</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information about how to track the progress of your Red Hat Quay deployment, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-deploy#operator-monitor-deploy-cli">Monitoring and debugging the deployment process</a>.
					</li></ul></div><section class="section" id="operator-deploy-view-pods-cli"><div class="titlepage"><div><div><h3 class="title">4.1.1. Viewing created components using the command line</h3></div></div></div><p>
					Use the following procedure to view deployed Red Hat Quay components.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have deployed the Red Hat Quay Operator on {ocp.}
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Enter the following command to view the deployed components:
						</li></ol></div><pre class="screen">$ oc get pods -n quay-enterprise</pre><p>
					+ .Example output
				</p><p>
					+
				</p><pre class="programlisting language-terminal">NAME                                                   READY   STATUS      RESTARTS   AGE
example-registry-clair-app-5ffc9f77d6-jwr9s            1/1     Running     0          3m42s
example-registry-clair-app-5ffc9f77d6-wgp7d            1/1     Running     0          3m41s
example-registry-clair-postgres-54956d6d9c-rgs8l       1/1     Running     0          3m5s
example-registry-quay-app-79c6b86c7b-8qnr2             1/1     Running     4          3m42s
example-registry-quay-app-79c6b86c7b-xk85f             1/1     Running     4          3m41s
example-registry-quay-app-upgrade-5kl5r                0/1     Completed   4          3m50s
example-registry-quay-config-editor-597b47c995-svqrl   1/1     Running     0          3m42s
example-registry-quay-database-b466fc4d7-tfrnx         1/1     Running     2          3m42s
example-registry-quay-mirror-6d9bd78756-6lj6p          1/1     Running     0          2m58s
example-registry-quay-mirror-6d9bd78756-bv6gq          1/1     Running     0          2m58s
example-registry-quay-postgres-init-dzbmx              0/1     Completed   0          3m43s
example-registry-quay-redis-8bd67b647-skgqx            1/1     Running     0          3m42s</pre></section><section class="section" id="operator-deploy-hpa"><div class="titlepage"><div><div><h3 class="title">4.1.2. Horizontal Pod Autoscaling (HPA)</h3></div></div></div><p>
					A default deployment shows the following running pods:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Two pods for the Red Hat Quay application itself (<code class="literal">example-registry-quay-app-*`</code>)
						</li><li class="listitem">
							One Redis pod for Red Hat Quay logging (<code class="literal">example-registry-quay-redis-*</code>)
						</li><li class="listitem">
							One database pod for PostgreSQL used by Red Hat Quay for metadata storage (<code class="literal">example-registry-quay-database-*</code>)
						</li><li class="listitem">
							One pod for the Red Hat Quay config editor (<code class="literal">example-registry-quay-config-editor-*</code>)
						</li><li class="listitem">
							Two <code class="literal">Quay</code> mirroring pods (<code class="literal">example-registry-quay-mirror-*</code>)
						</li><li class="listitem">
							Two pods for the Clair application (<code class="literal">example-registry-clair-app-*</code>)
						</li><li class="listitem">
							One PostgreSQL pod for Clair (<code class="literal">example-registry-clair-postgres-*</code>)
						</li></ul></div><p>
					As HPA is configured by default to be <code class="literal">managed</code>, the number of pods for Quay, Clair and repository mirroring is set to two. This facilitates the avoidance of downtime when updating or reconfiguring Red Hat Quay through the Red Hat Quay Operator or during rescheduling events.
				</p><pre class="programlisting language-terminal">$ oc get hpa -n quay-enterprise
NAME                           REFERENCE                                 TARGETS           MINPODS   MAXPODS   REPLICAS   AGE
example-registry-clair-app     Deployment/example-registry-clair-app     16%/90%, 0%/90%   2         10        2          13d
example-registry-quay-app      Deployment/example-registry-quay-app      31%/90%, 1%/90%   2         20        2          13d
example-registry-quay-mirror   Deployment/example-registry-quay-mirror   27%/90%, 0%/90%   2         20        2          13d</pre></section><section class="section" id="deploy-quay-api"><div class="titlepage"><div><div><h3 class="title">4.1.3. Using the API to deploy Red Hat Quay</h3></div></div></div><p>
					This section introduces using the API to deploy Red Hat Quay.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The config option <code class="literal">FEATURE_USER_INITIALIZE</code> must be set to <code class="literal">true</code>.
						</li><li class="listitem">
							No users can already exist in the database.
						</li></ul></div><p>
					For more information on pre-configuring your Red Hat Quay deployment, see the section <a class="link" href="#config-preconfigure-automation" title="3.1. Pre-configuring Red Hat Quay for automation">Pre-configuring Red Hat Quay for automation</a>
				</p><section class="section" id="using-the-api-to-create-first-user"><div class="titlepage"><div><div><h4 class="title">4.1.3.1. Using the API to create the first user</h4></div></div></div><p>
						Use the following procedure to create the first user in your Red Hat Quay organization.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This procedure requests an OAuth token by specifying <code class="literal">"access_token": true</code>.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								As the root user, install <code class="literal">python39</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ sudo yum install python39</pre></li><li class="listitem"><p class="simpara">
								Upgrade the <code class="literal">pip</code> package manager for Python 3.9:
							</p><pre class="programlisting language-terminal">$ python3.9 -m pip install --upgrade pip</pre></li><li class="listitem"><p class="simpara">
								Use the <code class="literal">pip</code> package manager to install the <code class="literal">bcrypt</code> package:
							</p><pre class="programlisting language-terminal">$ pip install bcrypt</pre></li><li class="listitem"><p class="simpara">
								Generate a secure, hashed password using the <code class="literal">bcrypt</code> package in Python 3.9 by entering the following command:
							</p><pre class="programlisting language-terminal">$ python3.9 -c 'import bcrypt; print(bcrypt.hashpw(b"subquay12345", bcrypt.gensalt(12)).decode("utf-8"))'</pre></li><li class="listitem"><p class="simpara">
								Open your Red Hat Quay configuration file and update the following configuration fields:
							</p><pre class="programlisting language-yaml">FEATURE_USER_INITIALIZE: true
SUPER_USERS:
     -  quayadmin</pre></li><li class="listitem"><p class="simpara">
								Stop the Red Hat Quay service by entering the following command:
							</p><pre class="programlisting language-terminal">$ sudo podman stop quay</pre></li><li class="listitem"><p class="simpara">
								Start the Red Hat Quay service by entering the following command:
							</p><pre class="programlisting language-terminal">$ sudo podman run -d -p 80:8080 -p 443:8443 --name=quay -v $QUAY/config:/conf/stack:Z  -v $QUAY/storage:/datastorage:Z {productrepo}/{quayimage}:{productminv}</pre></li><li class="listitem"><p class="simpara">
								Run the following <code class="literal">CURL</code> command to generate a new user with a username, password, email, and access token:
							</p><pre class="programlisting language-terminal">$ curl -X POST -k  http://quay-server.example.com/api/v1/user/initialize --header 'Content-Type: application/json' --data '{ "username": "quayadmin", "password":"quaypass12345", "email": "quayadmin@example.com", "access_token": true}'</pre><p class="simpara">
								If successful, the command returns an object with the username, email, and encrypted password. For example:
							</p><pre class="programlisting language-yaml">{"access_token":"6B4QTRSTSD1HMIG915VPX7BMEZBVB9GPNY2FC2ED", "email":"quayadmin@example.com","encrypted_password":"1nZMLH57RIE5UGdL/yYpDOHLqiNCgimb6W9kfF8MjZ1xrfDpRyRs9NUnUuNuAitW","username":"quayadmin"} # gitleaks:allow</pre><p class="simpara">
								If a user already exists in the database, an error is returned:
							</p><pre class="programlisting language-terminal">{"message":"Cannot initialize user in a non-empty database"}</pre><p class="simpara">
								If your password is not at least eight characters or contains whitespace, an error is returned:
							</p><pre class="programlisting language-terminal">{"message":"Failed to initialize user: Invalid password, password must be at least 8 characters and contain no whitespace."}</pre></li><li class="listitem"><p class="simpara">
								Log in to your Red Hat Quay deployment by entering the following command:
							</p><pre class="programlisting language-terminal">$ sudo podman login -u quayadmin -p quaypass12345 http://quay-server.example.com --tls-verify=false</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Login Succeeded!</pre>
								</p></div></li></ol></div></section></section><section class="section" id="operator-monitor-deploy-cli"><div class="titlepage"><div><div><h3 class="title">4.1.4. Monitoring and debugging the deployment process</h3></div></div></div><p>
					Users can now troubleshoot problems during the deployment phase. The status in the <code class="literal">QuayRegistry</code> object can help you monitor the health of the components during the deployment an help you debug any problems that may arise.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the following command to check the status of your deployment:
						</p><pre class="programlisting language-terminal">$ oc get quayregistry -n quay-enterprise -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output:</strong></p><p>
								Immediately after deployment, the <code class="literal">QuayRegistry</code> object will show the basic configuration:
							</p></div><pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: quay.redhat.com/v1
  kind: QuayRegistry
  metadata:
    creationTimestamp: "2021-09-14T10:51:22Z"
    generation: 3
    name: example-registry
    namespace: quay-enterprise
    resourceVersion: "50147"
    selfLink: /apis/quay.redhat.com/v1/namespaces/quay-enterprise/quayregistries/example-registry
    uid: e3fc82ba-e716-4646-bb0f-63c26d05e00e
  spec:
    components:
    - kind: postgres
      managed: true
    - kind: clair
      managed: true
    - kind: redis
      managed: true
    - kind: horizontalpodautoscaler
      managed: true
    - kind: objectstorage
      managed: true
    - kind: route
      managed: true
    - kind: mirror
      managed: true
    - kind: monitoring
      managed: true
    - kind: tls
      managed: true
    configBundleSecret: example-registry-config-bundle-kt55s
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc get pods</code> command to view the current state of the deployed components:
						</p><pre class="programlisting language-terminal">$ oc get pods -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output:</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                   READY   STATUS              RESTARTS   AGE
example-registry-clair-app-86554c6b49-ds7bl            0/1     ContainerCreating   0          2s
example-registry-clair-app-86554c6b49-hxp5s            0/1     Running             1          17s
example-registry-clair-postgres-68d8857899-lbc5n       0/1     ContainerCreating   0          17s
example-registry-quay-app-upgrade-h2v7h                0/1     ContainerCreating   0          9s
example-registry-quay-config-editor-5f646cbcb7-lbnc2   0/1     ContainerCreating   0          17s
example-registry-quay-database-66f495c9bc-wqsjf        0/1     ContainerCreating   0          17s
example-registry-quay-mirror-854c88457b-d845g          0/1     Init:0/1            0          2s
example-registry-quay-mirror-854c88457b-fghxv          0/1     Init:0/1            0          17s
example-registry-quay-postgres-init-bktdt              0/1     Terminating         0          17s
example-registry-quay-redis-f9b9d44bf-4htpz            0/1     ContainerCreating   0          17s</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							While the deployment is in progress, the <code class="literal">QuayRegistry</code> object will show the current status. In this instance, database migrations are taking place, and other components are waiting until completion:
						</p><pre class="programlisting language-terminal">  status:
    conditions:
    - lastTransitionTime: "2021-09-14T10:52:04Z"
      lastUpdateTime: "2021-09-14T10:52:04Z"
      message: all objects created/updated successfully
      reason: ComponentsCreationSuccess
      status: "False"
      type: RolloutBlocked
    - lastTransitionTime: "2021-09-14T10:52:05Z"
      lastUpdateTime: "2021-09-14T10:52:05Z"
      message: running database migrations
      reason: MigrationsInProgress
      status: "False"
      type: Available
    configEditorCredentialsSecret: example-registry-quay-config-editor-credentials-btbkcg8dc9
    configEditorEndpoint: https://example-registry-quay-config-editor-quay-enterprise.apps.docs.quayteam.org
    lastUpdated: 2021-09-14 10:52:05.371425635 +0000 UTC
    unhealthyComponents:
      clair:
      - lastTransitionTime: "2021-09-14T10:51:32Z"
        lastUpdateTime: "2021-09-14T10:51:32Z"
        message: 'Deployment example-registry-clair-postgres: Deployment does not have minimum availability.'
        reason: MinimumReplicasUnavailable
        status: "False"
        type: Available
      - lastTransitionTime: "2021-09-14T10:51:32Z"
        lastUpdateTime: "2021-09-14T10:51:32Z"
        message: 'Deployment example-registry-clair-app: Deployment does not have minimum availability.'
        reason: MinimumReplicasUnavailable
        status: "False"
        type: Available
      mirror:
      - lastTransitionTime: "2021-09-14T10:51:32Z"
        lastUpdateTime: "2021-09-14T10:51:32Z"
        message: 'Deployment example-registry-quay-mirror: Deployment does not have minimum availability.'
        reason: MinimumReplicasUnavailable
        status: "False"
        type: Available</pre></li><li class="listitem"><p class="simpara">
							When the deployment process finishes successfully, the status in the <code class="literal">QuayRegistry</code> object shows no unhealthy components:
						</p><pre class="programlisting language-terminal">  status:
    conditions:
    - lastTransitionTime: "2021-09-14T10:52:36Z"
      lastUpdateTime: "2021-09-14T10:52:36Z"
      message: all registry component healthchecks passing
      reason: HealthChecksPassing
      status: "True"
      type: Available
    - lastTransitionTime: "2021-09-14T10:52:46Z"
      lastUpdateTime: "2021-09-14T10:52:46Z"
      message: all objects created/updated successfully
      reason: ComponentsCreationSuccess
      status: "False"
      type: RolloutBlocked
    configEditorCredentialsSecret: example-registry-quay-config-editor-credentials-hg7gg7h57m
    configEditorEndpoint: https://example-registry-quay-config-editor-quay-enterprise.apps.docs.quayteam.org
    currentVersion: {producty}
    lastUpdated: 2021-09-14 10:52:46.104181633 +0000 UTC
    registryEndpoint: https://example-registry-quay-quay-enterprise.apps.docs.quayteam.org
    unhealthyComponents: {}</pre></li></ol></div></section></section><section class="section" id="operator-deploy-ui"><div class="titlepage"><div><div><h2 class="title">4.2. Deploying Red Hat Quay from the OpenShift Container Platform console</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Create a namespace, for example, <code class="literal">quay-enterprise</code>.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>, then select the Quay Operator to navigate to the Operator detail view.
					</li><li class="listitem">
						Click 'Create Instance' on the 'Quay Registry' tile under 'Provided APIs'.
					</li><li class="listitem">
						Optionally change the 'Name' of the <code class="literal">QuayRegistry</code>. This will affect the hostname of the registry. All other fields have been populated with defaults.
					</li><li class="listitem">
						Click 'Create' to submit the <code class="literal">QuayRegistry</code> to be deployed by the Quay Operator.
					</li><li class="listitem">
						You should be redirected to the <code class="literal">QuayRegistry</code> list view. Click on the <code class="literal">QuayRegistry</code> you just created to see the details view.
					</li><li class="listitem">
						Once the 'Registry Endpoint' has a value, click it to access your new Quay registry via the UI. You can now select 'Create Account' to create a user and sign in.
					</li></ol></div><section class="section" id="operator-first-user"><div class="titlepage"><div><div><h3 class="title">4.2.1. Using the Red Hat Quay UI to create the first user</h3></div></div></div><p>
					Use the following procedure to create the first user by the Red Hat Quay UI.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure assumes that the <code class="literal">FEATURE_USER_CREATION</code> config option has not been set to <code class="literal">false.</code> If it is <code class="literal">false</code>, the <code class="literal">Create Account</code> functionality on the UI will be disabled, and you will have to use the API to create the first user.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>, with the appropriate namespace / project.
						</li><li class="listitem"><p class="simpara">
							Click on the newly installed <code class="literal">QuayRegistry</code> object to view the details. For example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/config-editor-details-operator-36.png" alt="QuayRegistry details"/></span>
						</p></li><li class="listitem">
							After the <code class="literal">Registry Endpoint</code> has a value, navigate to this URL in your browser.
						</li><li class="listitem"><p class="simpara">
							Select <span class="strong strong"><strong>Create Account</strong></span> in the Red Hat Quay registry UI to create a user. For example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/create-account-1.png" alt="Create Account"/></span>
						</p></li><li class="listitem"><p class="simpara">
							Enter the details for <span class="strong strong"><strong>Username</strong></span>, <span class="strong strong"><strong>Password</strong></span>, <span class="strong strong"><strong>Email</strong></span>, and then click <span class="strong strong"><strong>Create Account</strong></span>. For example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/create-account-2.png" alt="Enter account details"/></span>
						</p></li></ol></div><p>
					After creating the first user, you are automatically logged in to the Red Hat Quay registry. For example:
				</p><p>
					+ 
					<span class="inlinemediaobject"><img src="images/create-account-3.png" alt="Initial log in"/></span>
				</p></section></section></section><section class="chapter" id="operator-config-cli"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Configuring Red Hat Quay on OpenShift Container Platform</h1></div></div></div><p>
			After deployment, you can configure the Red Hat Quay application by editing the Red Hat Quay configuration bundle secret <code class="literal">spec.configBundleSecret</code>. You can also change the managed status of components in the <code class="literal">spec.components</code> object of the <code class="literal">QuayRegistry</code> resource.
		</p><p>
			Alternatively, you can use the config editor UI to configure the Red Hat Quay application. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#operator-config-ui">Using the config tool to reconfigure Red Hat Quay on OpenShift Container Platform</a>.
		</p><section class="section" id="editing-config-bundle-secret-in-ocp-console"><div class="titlepage"><div><div><h2 class="title">5.1. Editing the config bundle secret in the OpenShift Container Platform console</h2></div></div></div><p>
				Use the following procedure to edit the config bundle secret in the OpenShift Container Platform console.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On the Red Hat Quay Registry overview screen, click the link for the <span class="strong strong"><strong>Config Bundle Secret</strong></span>.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/operator-quay-registry-overview.png" alt="Red Hat Quay Registry overview"/></span>
					</p></li><li class="listitem"><p class="simpara">
						To edit the secret, click <span class="strong strong"><strong>Actions</strong></span> → <span class="strong strong"><strong>Edit Secret</strong></span>.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/operator-config-bundle-edit-secret.png" alt="Edit secret"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Modify the configuration and save the changes.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/operator-save-config-changes.png" alt="Save changes"/></span>
					</p></li><li class="listitem">
						Monitor the deployment to ensure successful completion and that the configuration changes have taken effect.
					</li></ol></div></section><section class="section" id="operator-config-cli-access"><div class="titlepage"><div><div><h2 class="title">5.2. Determining QuayRegistry endpoints and secrets</h2></div></div></div><p>
				Use the following procedure to find <code class="literal">QuayRegistry</code> endpoints and secrets.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						You can examine the <code class="literal">QuayRegistry</code> resource, using <code class="literal">oc describe quayregistry</code> or <code class="literal">oc get quayregistry -o yaml</code>, to find the current endpoints and secrets by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc get quayregistry example-registry -n quay-enterprise -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: example-registry
  namespace: quay-enterprise
  ...
spec:
  components:
  - kind: quay
    managed: true
  ...
  - kind: clairpostgres
    managed: true
  configBundleSecret: init-config-bundle-secret <span id="CO3-1"/><span class="callout">1</span>
status:
  configEditorCredentialsSecret: example-registry-quay-config-editor-credentials-fg2gdgtm24 <span id="CO3-2"/><span class="callout">2</span>
  configEditorEndpoint: https://example-registry-quay-config-editor-quay-enterprise.apps.docs.gcp.quaydev.org <span id="CO3-3"/><span class="callout">3</span>
  currentVersion: 3.7.0
  lastUpdated: 2022-05-11 13:28:38.199476938 +0000 UTC
  registryEndpoint: https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org <span id="CO3-4"/><span class="callout">4</span></pre>
						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The config bundle secret, containing the <code class="literal">config.yaml</code> file and any SSL/TLS certificates.
							</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The secret containing the username (typically <code class="literal">quayconfig</code>) and the password for the config editor tool.
							</div></dd><dt><a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The URL for the config editor tool, for browser access to the config tool, and for the configuration API.
							</div></dd><dt><a href="#CO3-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The URL for your registry, for browser access to the registry UI, and for the registry API endpoint.
							</div></dd></dl></div></li></ol></div><section class="section" id="determining-username-password-config-editor-tool"><div class="titlepage"><div><div><h3 class="title">5.2.1. Locating the username and password for the config editor tool</h3></div></div></div><p>
					Use the following procedure to locate the username and password for the config editor tool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the following command to retrieve the secret:
						</p><pre class="programlisting language-terminal">$ oc get secret -n quay-enterprise example-registry-quay-config-editor-credentials-fg2gdgtm24 -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
data:
  password: SkZwQkVKTUN0a1BUZmp4dA==
  username: cXVheWNvbmZpZw==
kind: Secret</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Decode the username by entering the following command:
						</p><pre class="programlisting language-terminal">$ echo 'cXVheWNvbmZpZw==' | base64 --decode</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">quayconfig</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Decode the password by entering the following command:
						</p><pre class="programlisting language-terminal">$ echo 'SkZwQkVKTUN0a1BUZmp4dA==' | base64 --decode</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">JFpBEJMCtkPTfjxt</pre>
							</p></div></li></ol></div></section></section><section class="section" id="operator-config-cli-download"><div class="titlepage"><div><div><h2 class="title">5.3. Downloading the existing configuration</h2></div></div></div><p>
				The following procedures detail how to download the existing configuration using different strategies.
			</p><section class="section" id="using-config-editor-endpoint"><div class="titlepage"><div><div><h3 class="title">5.3.1. Using the config editor endpoint to download the existing configuration</h3></div></div></div><p>
					Use the following procedure to download the existing configuration through the config editor endpoint.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Enter the following command, specifying the username and password for the config editor, to download the existing configuration:
						</p><pre class="programlisting language-terminal">$ curl -k -u quayconfig:JFpBEJMCtkPTfjxt https://example-registry-quay-config-editor-quay-enterprise.apps.docs.quayteam.org/api/v1/config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">{
    "config.yaml": {
        "ALLOW_PULLS_WITHOUT_STRICT_LOGGING": false,
        "AUTHENTICATION_TYPE": "Database",
        ...
        "USER_RECOVERY_TOKEN_LIFETIME": "30m"
    },
    "certs": {
        "extra_ca_certs/service-ca.crt": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURVVENDQWptZ0F3SUJBZ0lJRE9kWFhuUXFjMUF3RFFZSktvWklodmNOQVFFTEJRQXdOakUwTURJR0ExVUUKQXd3cmIzQmxibk5vYVdaMExYTmxjblpwWTJVdGMyVnlkbWx1WnkxemFXZHVaWEpBTVRZek1UYzNPREV3TXpBZQpGdzB5TVRBNU1UWXdOelF4TkRKYUZ..."
    }
}</pre>
							</p></div></li></ul></div></section><section class="section" id="using-config-bundle-secret"><div class="titlepage"><div><div><h3 class="title">5.3.2. Using the config bundle secret to download the existing configuration</h3></div></div></div><p>
					You can use the config bundle secret to download the existing configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the secret data by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret -n quay-enterprise init-config-bundle-secret -o jsonpath='{.data}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">{
    "config.yaml": "RkVBVFVSRV9VU0 ... MDAwMAo="
}</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Enter the following command to decode the data:
						</p><pre class="programlisting language-terminal">$ echo 'RkVBVFVSRV9VU0 ... MDAwMAo=' | base64 --decode</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">FEATURE_USER_INITIALIZE: true
BROWSER_API_CALLS_XHR_ONLY: false
SUPER_USERS:
- quayadmin
FEATURE_USER_CREATION: false
FEATURE_QUOTA_MANAGEMENT: true
FEATURE_PROXY_CACHE: true
FEATURE_BUILD_SUPPORT: true
DEFAULT_SYSTEM_REJECT_QUOTA_BYTES: 102400000</pre>
							</p></div></li></ol></div></section></section><section class="section" id="operator-custom-ssl-certs-config-bundle"><div class="titlepage"><div><div><h2 class="title">5.4. Using the config bundle to configure custom SSL/TLS certs</h2></div></div></div><p>
				You can configure custom SSL/TLS certificates before the initial deployment, or after Red Hat Quay is deployed on OpenShift Container Platform. This is done by creating or updating the config bundle secret.
			</p><p>
				If you are adding the certificates to an existing deployment, you must include the existing <code class="literal">config.yaml</code> file in the new config bundle secret, even if you are not making any configuration changes.
			</p><p>
				Use the following procedure to add custom SSL/TLS certificates.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your <code class="literal">QuayRegistry</code> YAML file, set <code class="literal">kind: tls</code> to <code class="literal">managed:false</code>, for example:
					</p><pre class="programlisting language-yaml">  - kind: tls
    managed: false</pre></li><li class="listitem"><p class="simpara">
						Navigate to the <span class="strong strong"><strong>Events</strong></span> page, which should reveal that the change is blocked until you set up the appropriate config. For example:
					</p><pre class="programlisting language-yaml">    - lastTransitionTime: '2022-03-28T12:56:49Z'
      lastUpdateTime: '2022-03-28T12:56:49Z'
      message: &gt;-
        required component `tls` marked as unmanaged, but `configBundleSecret`
        is missing necessary fields
      reason: ConfigInvalid
      status: 'True'</pre></li><li class="listitem"><p class="simpara">
						Create the secret using embedded data or by using files.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Embed the configuration details directly in the <code class="literal">Secret</code> resource YAML file. For example:
							</p><div class="formalpara"><p class="title"><strong>custom-ssl-config-bundle.yaml</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: custom-ssl-config-bundle-secret
  namespace: quay-enterprise
data:
  config.yaml: |
    FEATURE_USER_INITIALIZE: true
    BROWSER_API_CALLS_XHR_ONLY: false
    SUPER_USERS:
    - quayadmin
    FEATURE_USER_CREATION: false
    FEATURE_QUOTA_MANAGEMENT: true
    FEATURE_PROXY_CACHE: true
    FEATURE_BUILD_SUPPORT: true
    DEFAULT_SYSTEM_REJECT_QUOTA_BYTES: 102400000
  extra_ca_cert_my-custom-ssl.crt: |
    -----BEGIN CERTIFICATE-----
    MIIDsDCCApigAwIBAgIUCqlzkHjF5i5TXLFy+sepFrZr/UswDQYJKoZIhvcNAQEL
    BQAwbzELMAkGA1UEBhMCSUUxDzANBgNVBAgMBkdBTFdBWTEPMA0GA1UEBwwGR0FM
    ....
    -----END CERTIFICATE-----</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Create the secret from the YAML file:
							</p><pre class="screen">$ oc create  -f custom-ssl-config-bundle.yaml</pre><p class="simpara">
								..
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Alternatively, you can create files containing the desired information, and then create the secret from those files.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Enter the following command to create a generic <code class="literal">Secret</code> object that contains the <code class="literal">config.yaml</code> file and a <code class="literal">custom-ssl.crt</code> file:
							</p><pre class="screen">$ oc create secret generic custom-ssl-config-bundle-secret \
  --from-file=config.yaml \
  --from-file=extra_ca_cert_my-custom-ssl.crt=my-custom-ssl.crt</pre></li><li class="listitem"><p class="simpara">
								Create or update the <code class="literal">QuayRegistry</code> YAML file, referencing the created <code class="literal">Secret</code>, for example:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">QuayRegistry</code> YAML file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: custom-ssl-config-bundle-secret</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Deploy or update the registry using the YAML file by entering the following command:
							</p><pre class="screen">oc apply -f quayregistry.yaml</pre></li></ol></div></li></ol></div></section></section><section class="chapter" id="operator-config-ui"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Using the config tool to reconfigure Red Hat Quay on OpenShift Container Platform</h1></div></div></div><section class="section" id="operator-config-ui-access"><div class="titlepage"><div><div><h2 class="title">6.1. Accessing the config editor</h2></div></div></div><p>
				In the <span class="strong strong"><strong>Details</strong></span> section of the <code class="literal">QuayRegistry</code> object, the endpoint for the config editor is available, along with a link to the <code class="literal">Secret</code> object that contains the credentials for logging into the config editor. For example:
			</p><p>
				<span class="inlinemediaobject"><img src="images/config-editor-details-openshift.png" alt="Config editor details"/></span>
			</p><section class="section" id="retrieving-the-config-editor-credentials"><div class="titlepage"><div><div><h3 class="title">6.1.1. Retrieving the config editor credentials</h3></div></div></div><p>
					Use the following procedure to retrieve the config editor credentials.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Click on the link for the config editor secret:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/config-editor-secret.png" alt="Config editor secret"/></span>
						</p></li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Data</strong></span> section of the <span class="strong strong"><strong>Secret</strong></span> details page, click <span class="strong strong"><strong>Reveal values</strong></span> to see the credentials for logging into the config editor. For example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/config-editor-secret-reveal.png" alt="Config editor secret reveal"/></span>
						</p></li></ol></div></section><section class="section" id="logging-into-config-editor"><div class="titlepage"><div><div><h3 class="title">6.1.2. Logging into the config editor</h3></div></div></div><p>
					Use the following procedure to log into the config editor.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Navigate the config editor endpoint. When prompted, enter the username, for example, <code class="literal">quayconfig</code>, and the password. For example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/config-editor-ui.png" alt="Config editor user interface"/></span>
						</p></li></ul></div></section><section class="section" id="operator-config-ui-change"><div class="titlepage"><div><div><h3 class="title">6.1.3. Changing configuration</h3></div></div></div><p>
					In the following example, you will update your configuration file by changing the default expiration period of deleted tags.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the config editor, locate the <span class="strong strong"><strong>Time Machine</strong></span> section.
						</li><li class="listitem"><p class="simpara">
							Add an expiration period to the <span class="strong strong"><strong>Allowed expiration periods</strong></span> box, for example, <code class="literal">4w</code>:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/ui-time-machine-add.png" alt="Add expiration period"/></span>
						</p></li><li class="listitem">
							Select <span class="strong strong"><strong>Validate Configuration Changes</strong></span> to ensure that the changes are valid.
						</li><li class="listitem"><p class="simpara">
							Apply the changes by pressing <span class="strong strong"><strong>Reconfigure Quay</strong></span>:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/config-editor-reconfigure.png" alt="Reconfigure"/></span>
						</p></li></ol></div><p>
					After applying the changes, the config tool notifies you that the changes made have been submitted to your Red Hat Quay deployment:
				</p><p>
					+ 
					<span class="inlinemediaobject"><img src="images/config-editor-reconfigured.png" alt="Reconfigured"/></span>
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Reconfiguring Red Hat Quay using the config tool UI can lead to the registry being unavailable for a short time while the updated configuration is applied.
					</p></div></div></section></section><section class="section" id="operator-config-ui-monitoring"><div class="titlepage"><div><div><h2 class="title">6.2. Monitoring reconfiguration in the Red Hat Quay UI</h2></div></div></div><p>
				You can monitor the reconfiguration of Red Hat Quay in real-time.
			</p><section class="section" id="reconfiguring-quayregistry-resource"><div class="titlepage"><div><div><h3 class="title">6.2.1. QuayRegistry resource</h3></div></div></div><p>
					After reconfiguring the Red Hat Quay Operator, you can track the progress of the redeployment in the <span class="strong strong"><strong>YAML</strong></span> tab for the specific instance of <code class="literal">QuayRegistry</code>, in this case, <code class="literal">example-registry</code>:
				</p><p>
					<span class="inlinemediaobject"><img src="images/ui-monitor-deploy-update.png" alt="ui monitor deploy update"/></span>
				</p><p>
					Each time the status changes, you will be prompted to reload the data to see the updated version. Eventually, the Red Hat Quay Operator reconciles the changes, and there are be no unhealthy components reported.
				</p><p>
					<span class="inlinemediaobject"><img src="images/ui-monitor-deploy-done.png" alt="ui monitor deploy done"/></span>
				</p></section><span style="color: red">&lt;reconfiguring-events-tab&gt;<span style="color: red">&lt;title&gt;Events&lt;/title&gt;</span>
			<p>
				The <span class="strong strong"><strong>Events</strong></span> tab for the <code class="literal">QuayRegistry</code> shows some events related to the redeployment. For example:
			</p>
			<p>
				<span class="inlinemediaobject"><img src="images/ui-monitor-deploy-streaming-events.png" alt="ui monitor deploy streaming events"/></span>
			</p>
			<p>
				Streaming events, for all resources in the namespace that are affected by the reconfiguration, are available in the OpenShift Container Platform console under <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Events</strong></span>. For example:
			</p>
			<p>
				<span class="inlinemediaobject"><img src="images/ui-monitor-deploy-streaming-events.png" alt="ui monitor deploy streaming events"/></span>
			</p>
			&lt;/reconfiguring-events-tab&gt;</span></section><section class="section" id="operator-config-ui-updated"><div class="titlepage"><div><div><h2 class="title">6.3. Accessing updated information after reconfiguration</h2></div></div></div><p>
				Use the following procedure to access update information after Red Hat Quay reconfiguration.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					As of Red Hat Quay 3.7, reconfiguring Red Hat Quay through the UI no longer generates a new login password. The password now generates only once, and remains the same after reconciling <code class="literal">QuayRegistry</code> objects.
				</p></div></div><section class="section" id="accessing-updated-config-ui"><div class="titlepage"><div><div><h3 class="title">6.3.1. Accessing the updated config.yaml file in the UI</h3></div></div></div><p>
					You can use the config bundle to access the updated <code class="literal">config.yaml</code> file.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <code class="literal">QuayRegistry</code> <span class="strong strong"><strong>Details</strong></span> screen, click on the <span class="strong strong"><strong>Config Bundle Secret</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Data</strong></span> section of the <code class="literal">Secret</code> details screen, click <span class="strong strong"><strong>Reveal values</strong></span> to see the <code class="literal">config.yaml</code> file.
						</li><li class="listitem"><p class="simpara">
							Check that the change has been applied. In this case, <code class="literal">4w</code> should be in the list of <code class="literal">TAG_EXPIRATION_OPTIONS</code>. For example:
						</p><pre class="programlisting language-yaml">...
SERVER_HOSTNAME: example-quay-openshift-operators.apps.docs.quayteam.org
SETUP_COMPLETE: true
SUPER_USERS:
- quayadmin
TAG_EXPIRATION_OPTIONS:
- 2w
- 4w
...</pre></li></ol></div></section></section><section class="section" id="config-ui-custom-ssl-certs"><div class="titlepage"><div><div><h2 class="title">6.4. Custom SSL/TLS certificates UI</h2></div></div></div><p>
				The config tool can be used to load custom certificates to facilitate access to resources like external databases. Select the custom certs to be uploaded, ensuring that they are in PEM format, with an extension <code class="literal">.crt</code>.
			</p><p>
				<span class="inlinemediaobject"><img src="images/ui-custom-ssl-certs.png" alt="Custom SSL/TLS certificates"/></span>
			</p><p>
				The config tool also displays a list of any uploaded certificates. After you upload your custom SSL/TLS cert, it will appear in the list. For example:
			</p><p>
				<span class="inlinemediaobject"><img src="images/ui-custom-ssl-certs-uploaded.png" alt="Custom SSL/TLS certificates"/></span>
			</p></section><section class="section" id="operator-external-access"><div class="titlepage"><div><div><h2 class="title">6.5. External Access to the Registry</h2></div></div></div><p>
				When running on OpenShift Container Platform, the <code class="literal">Routes</code> API is available and is automatically used as a managed component. After creating the <code class="literal">QuayRegistry</code> object, the external access point can be found in the status block of the <code class="literal">QuayRegistry</code> object. For example:
			</p><pre class="programlisting language-yaml">status:
  registryEndpoint: some-quay.my-namespace.apps.mycluster.com</pre></section></section><section class="chapter" id="quay_operator_features"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Quay Operator features</h1></div></div></div><section class="section" id="operator-console-monitoring-alerting"><div class="titlepage"><div><div><h2 class="title">7.1. Console monitoring and alerting</h2></div></div></div><p>
				Red Hat Quay provides support for monitoring instances that were deployed by using the Red Hat Quay Operator, from inside the OpenShift Container Platform console. The new monitoring features include a Grafana dashboard, access to individual metrics, and alerting to notify for frequently restarting <code class="literal">Quay</code> pods.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					To enable the monitoring features, the Red Hat Quay Operator must be installed in <span class="strong strong"><strong>All Namespaces</strong></span> mode.
				</p></div></div><section class="section" id="operator-dashboard"><div class="titlepage"><div><div><h3 class="title">7.1.1. Dashboard</h3></div></div></div><p>
					On the OpenShift Container Platform console, click <span class="strong strong"><strong>Monitoring</strong></span> → <span class="strong strong"><strong>Dashboards</strong></span> and search for the dashboard of your desired Red Hat Quay registry instance:
				</p><p>
					<span class="inlinemediaobject"><img src="images/choose-dashboard.png" alt="Choose Quay dashboard"/></span>
				</p><p>
					The dashboard shows various statistics including the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The number of <span class="strong strong"><strong>Organizations</strong></span>, <span class="strong strong"><strong>Repositories</strong></span>, <span class="strong strong"><strong>Users</strong></span>, and <span class="strong strong"><strong>Robot accounts</strong></span>
						</li><li class="listitem">
							CPU Usage
						</li><li class="listitem">
							Max memory usage
						</li><li class="listitem">
							Rates of pulls and pushes, and authentication requests
						</li><li class="listitem">
							API request rate
						</li><li class="listitem">
							Latencies
						</li></ul></div><p>
					<span class="inlinemediaobject"><img src="images/console-dashboard-1.png" alt="Console dashboard"/></span>
				</p></section><section class="section" id="operator-metrics"><div class="titlepage"><div><div><h3 class="title">7.1.2. Metrics</h3></div></div></div><p>
					You can see the underlying metrics behind the Red Hat Quay dashboard by accessing <span class="strong strong"><strong>Monitoring</strong></span> → <span class="strong strong"><strong>Metrics</strong></span> in the UI. In the <span class="strong strong"><strong>Expression</strong></span> field, enter the text <code class="literal">quay_</code> to see the list of metrics available:
				</p><p>
					<span class="inlinemediaobject"><img src="images/quay-metrics.png" alt="Quay metrics"/></span>
				</p><p>
					Select a sample metric, for example, <code class="literal">quay_org_rows</code>:
				</p><p>
					<span class="inlinemediaobject"><img src="images/quay-metrics-org-rows.png" alt="Number of Quay organizations"/></span>
				</p><p>
					This metric shows the number of organizations in the registry. It is also directly surfaced in the dashboard.
				</p></section><section class="section" id="operator-alerting"><div class="titlepage"><div><div><h3 class="title">7.1.3. Alerting</h3></div></div></div><p>
					An alert is raised if the <code class="literal">Quay</code> pods restart too often. The alert can be configured by accessing the <span class="strong strong"><strong>Alerting</strong></span> rules tab from <span class="strong strong"><strong>Monitoring</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> in the console UI and searching for the Quay-specific alert:
				</p><p>
					<span class="inlinemediaobject"><img src="images/alerting-rules.png" alt="Alerting rules"/></span>
				</p><p>
					Select the <code class="literal">QuayPodFrequentlyRestarting</code> rule detail to configure the alert:
				</p><p>
					<span class="inlinemediaobject"><img src="images/quay-pod-frequently-restarting.png" alt="Alerting rule details"/></span>
				</p></section></section><section class="section" id="clair-vulnerability-scanner"><div class="titlepage"><div><div><h2 class="title">7.2. Clair for Red Hat Quay</h2></div></div></div><p>
				Clair v4 (Clair) is an open source application that leverages static code analyses for parsing image content and reporting vulnerabilities affecting the content. Clair is packaged with Red Hat Quay and can be used in both standalone and Operator deployments. It can be run in highly scalable configurations, where components can be scaled separately as appropriate for enterprise environments.
			</p><section class="section" id="clair-vulnerability-scanner-hosts"><div class="titlepage"><div><div><h3 class="title">7.2.1. Clair vulnerability databases</h3></div></div></div><p>
					Clair uses the following vulnerability databases to report for issues in your images:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Ubuntu Oval database
						</li><li class="listitem">
							Debian Oval database
						</li><li class="listitem">
							Red Hat Enterprise Linux (RHEL) Oval database
						</li><li class="listitem">
							SUSE Oval database
						</li><li class="listitem">
							Oracle Oval database
						</li><li class="listitem">
							Alpine SecDB database
						</li><li class="listitem">
							VMWare Photon OS database
						</li><li class="listitem">
							Amazon Web Services (AWS) UpdateInfo
						</li><li class="listitem">
							Pyup.io (Python) database
						</li></ul></div><p>
					For information about how Clair does security mapping with the different databases, see <a class="link" href="https://quay.github.io/claircore/concepts/severity_mapping.html">ClairCore Severity Mapping</a>.
				</p></section><section class="section" id="clair-quay-operator-overview"><div class="titlepage"><div><div><h3 class="title">7.2.2. Clair on OpenShift Container Platform</h3></div></div></div><p>
					To set up Clair v4 (Clair) on a Red Hat Quay deployment on OpenShift Container Platform, it is recommended to use the Red Hat Quay Operator. By default, the Red Hat Quay Operator will install or upgrade a Clair deployment along with your Red Hat Quay deployment and configure Clair automatically.
				</p></section><section class="section" id="clair-testing"><div class="titlepage"><div><div><h3 class="title">7.2.3. Testing Clair</h3></div></div></div><p>
					Use the following procedure to test Clair on either a standalone Red Hat Quay deployment, or on an OpenShift Container Platform Operator-based deployment.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have deployed the Clair container image.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Pull a sample image by entering the following command:
						</p><pre class="programlisting language-terminal">$ podman pull ubuntu:20.04</pre></li><li class="listitem"><p class="simpara">
							Tag the image to your registry by entering the following command:
						</p><pre class="programlisting language-terminal">$ sudo podman tag docker.io/library/ubuntu:20.04 &lt;quay-server.example.com&gt;/&lt;user-name&gt;/ubuntu:20.04</pre></li><li class="listitem"><p class="simpara">
							Push the image to your Red Hat Quay registry by entering the following command:
						</p><pre class="programlisting language-terminal">$ sudo podman push --tls-verify=false quay-server.example.com/quayadmin/ubuntu:20.04</pre></li><li class="listitem">
							Log in to your Red Hat Quay deployment through the UI.
						</li><li class="listitem">
							Click the repository name, for example, <span class="strong strong"><strong>quayadmin/ubuntu</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the navigation pane, click <span class="strong strong"><strong>Tags</strong></span>.
						</p><div class="formalpara"><p class="title"><strong>Report summary</strong></p><p>
								<span class="inlinemediaobject"><img src="images/clair-reposcan.png" alt="Security scan information appears for scanned repository images"/></span>
							</p></div></li><li class="listitem"><p class="simpara">
							Click the image report, for example, <span class="strong strong"><strong>45 medium</strong></span>, to show a more detailed report:
						</p><div class="formalpara"><p class="title"><strong>Report details</strong></p><p>
								<span class="inlinemediaobject"><img src="images/clair-vulnerabilities.png" alt="See all vulnerabilities or only those that are fixable"/></span>
							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In some cases, Clair shows duplicate reports on images, for example, <code class="literal">ubi8/nodejs-12</code> or <code class="literal">ubi8/nodejs-16</code>. This occurs because vulnerabilities with same name are for different packages. This behavior is expected with Clair vulnerability reporting and will not be addressed as a bug.
							</p></div></div></li></ol></div></section></section><section class="section" id="fips-overview"><div class="titlepage"><div><div><h2 class="title">7.3. Federal Information Processing Standard (FIPS) readiness and compliance</h2></div></div></div><p>
				The Federal Information Processing Standard (FIPS) developed by the National Institute of Standards and Technology (NIST) is regarded as the highly regarded for securing and encrypting sensitive data, notably in highly regulated areas such as banking, healthcare, and the public sector. Red Hat Enterprise Linux (RHEL) and OpenShift Container Platform support the FIPS standard by providing a <span class="emphasis"><em>FIPS mode</em></span>, in which the system only allows usage of specific FIPS-validated cryptographic modules like <code class="literal">openssl</code>. This ensures FIPS compliance.
			</p><p>
				Red Hat Quay supports running on FIPS-enabled RHEL and OpenShift Container Platform environments from Red Hat Quay version 3.5.0.
			</p></section></section><section class="chapter" id="advanced_concepts"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Advanced Concepts</h1></div></div></div><section class="section" id="operator-deploy-infrastructure"><div class="titlepage"><div><div><h2 class="title">8.1. Deploying Quay on infrastructure nodes</h2></div></div></div><p>
				By default, <code class="literal">Quay</code> related pods are placed on arbitrary worker nodes when using the Red Hat Quay Operator to deploy the registry. The OpenShift Container Platform documentation shows how to use machine sets to configure nodes to only host infrastructure components. For more information, see <a class="link" href="https://docs.openshift.com/container-platform/4.13/machine_management/creating-infrastructure-machinesets.html">Creating infrastructure machine sets</a>.
			</p><p>
				If you are not using OpenShift Container Platform <code class="literal">MachineSet</code> resources to deploy infra nodes, the following sections shows you how to manually label and tain nodes for infrastructure purposes. After you have configured your infrastructure nodes, either manually or by using machine sets, you can control the placement of <code class="literal">Quay</code> pods on these nodes using node selectors and tolerations.
			</p><section class="section" id="label-taint-nodes-for-infrastructure-use"><div class="titlepage"><div><div><h3 class="title">8.1.1. Labeling and tainting nodes for infrastructure use</h3></div></div></div><p>
					Use the following procedure to label and taint nodes for infrastructure use.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the following example cluster, there are three master nodes and six worker nodes. For example:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               STATUS   ROLES    AGE     VERSION
user1-jcnp6-master-0.c.quay-devel.internal         Ready    master   3h30m   v1.20.0+ba45583
user1-jcnp6-master-1.c.quay-devel.internal         Ready    master   3h30m   v1.20.0+ba45583
user1-jcnp6-master-2.c.quay-devel.internal         Ready    master   3h30m   v1.20.0+ba45583
user1-jcnp6-worker-b-65plj.c.quay-devel.internal   Ready    worker   3h21m   v1.20.0+ba45583
user1-jcnp6-worker-b-jr7hc.c.quay-devel.internal   Ready    worker   3h21m   v1.20.0+ba45583
user1-jcnp6-worker-c-jrq4v.c.quay-devel.internal   Ready    worker   3h21m   v1.20.0+ba45583
user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal   Ready    worker   3h21m   v1.20.0+ba45583
user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal   Ready    worker   3h22m   v1.20.0+ba45583
user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal   Ready    worker   3h21m   v1.20.0+ba45583</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Label the last three worker nodes for infrastructure use:
						</p><pre class="programlisting language-terminal">$ oc label node --overwrite user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal node-role.kubernetes.io/infra=</pre><pre class="programlisting language-terminal">$ oc label node --overwrite user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal node-role.kubernetes.io/infra=</pre><pre class="programlisting language-terminal">$ oc label node --overwrite user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal node-role.kubernetes.io/infra=</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to list the nodes in the cluster. The last three worker nodes should have the added role <code class="literal">infra</code>. For example:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               STATUS   ROLES          AGE     VERSION
user1-jcnp6-master-0.c.quay-devel.internal         Ready    master         4h14m   v1.20.0+ba45583
user1-jcnp6-master-1.c.quay-devel.internal         Ready    master         4h15m   v1.20.0+ba45583
user1-jcnp6-master-2.c.quay-devel.internal         Ready    master         4h14m   v1.20.0+ba45583
user1-jcnp6-worker-b-65plj.c.quay-devel.internal   Ready    worker         4h6m    v1.20.0+ba45583
user1-jcnp6-worker-b-jr7hc.c.quay-devel.internal   Ready    worker         4h5m    v1.20.0+ba45583
user1-jcnp6-worker-c-jrq4v.c.quay-devel.internal   Ready    worker         4h5m    v1.20.0+ba45583
user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal   Ready    infra,worker   4h6m    v1.20.0+ba45583
user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal   Ready    infra,worker   4h6m    v1.20.0+ba45583
user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal   Ready    infra,worker   4h6m    v1.20.0+ba4558</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							When an <code class="literal">infra</code> node is assigned as a worker, there is a chance that user workloads could get inadvertently assigned to an <code class="literal">infra</code> node. To avoid this, you can apply a tain to the infra node, and then add tolerations for the pods that you want to control. For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal node-role.kubernetes.io/infra:NoSchedule</pre><pre class="programlisting language-terminal">$ oc adm taint nodes user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal node-role.kubernetes.io/infra:NoSchedule</pre><pre class="programlisting language-terminal">$ oc adm taint nodes user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal node-role.kubernetes.io/infra:NoSchedule</pre></li></ol></div></section><section class="section" id="creating-project-with-node-selector"><div class="titlepage"><div><div><h3 class="title">8.1.2. Creating a project with node selector and toleration</h3></div></div></div><p>
					Use the following procedure to create a <code class="literal">Project</code> resource with node selector and toleration.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have already deployed Red Hat Quay using the Operator, remove the installed Operator and any specific namespace(s) you created for the deployment.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Project</code> resource, specifying a node selector and toleration as shown in the following example:
						</p><pre class="programlisting language-yaml">kind: Project
apiVersion: project.openshift.io/v1
metadata:
  name: quay-registry
  annotations:
    openshift.io/node-selector: 'node-role.kubernetes.io/infra='
    scheduler.alpha.kubernetes.io/defaultTolerations: &gt;-
      [{"operator": "Exists", "effect": "NoSchedule", "key":
      "node-role.kubernetes.io/infra"}
      ]</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc apply</code> command to create the <code class="literal">Project</code> resource:
						</p><pre class="programlisting language-terminal">$ oc apply -f quay-registry.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">project.project.openshift.io/quay-registry created</pre>
							</p></div></li></ol></div><p>
					Subsequent resources created in the <code class="literal">quay-registry</code> namespace should now be scheduled on the dedicated infrastructure nodes.
				</p></section><section class="section" id="installing-quay-operator-in-namespace"><div class="titlepage"><div><div><h3 class="title">8.1.3. Installing the Red Hat Quay Operator in the namespace</h3></div></div></div><p>
					Use the following procedure to install the Red Hat Quay Operator in the namesapce.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							When installing the Red Hat Quay Operator, specify the appropriate project namespace explicitly. In this example, it is <code class="literal">quay-registry</code>. This results in the Operator pod itself landing on one of the three infrastructure nodes:
						</p><pre class="programlisting language-terminal">$ oc get pods -n quay-registry -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                    READY   STATUS    RESTARTS   AGE   IP            NODE                                              
quay-operator.v3.4.1-6f6597d8d8-bd4dp   1/1     Running   0          30s   10.131.0.16   user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal</pre>
							</p></div></li></ul></div></section><section class="section" id="creating-the-registry"><div class="titlepage"><div><div><h3 class="title">8.1.4. Creating the Red Hat Quay registry</h3></div></div></div><p>
					Use the following procedure to create the Red Hat Quay registry.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the registry as outlined in Creating a project with node selector and toleration and wait for the deployment to be marked as <code class="literal">Ready</code>. When you list the <code class="literal">Quay</code> pods, you should now see that they have only been scheduled on the three nodes that you have labelled for infrastructure purposes. For example:
						</p><pre class="programlisting language-terminal">$ oc get pods -n quay-registry -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                   READY   STATUS      RESTARTS   AGE     IP            NODE                                                
example-registry-clair-app-789d6d984d-gpbwd            1/1     Running     1          5m57s   10.130.2.80   user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal
example-registry-clair-postgres-7c8697f5-zkzht         1/1     Running     0          4m53s   10.129.2.19   user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal
example-registry-quay-app-56dd755b6d-glbf7             1/1     Running     1          5m57s   10.129.2.17   user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal
example-registry-quay-config-editor-7bf9bccc7b-dpc6d   1/1     Running     0          5m57s   10.131.0.23   user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal
example-registry-quay-database-8dc7cfd69-dr2cc         1/1     Running     0          5m43s   10.129.2.18   user1-jcnp6-worker-c-pwxfp.c.quay-devel.internal
example-registry-quay-mirror-78df886bcc-v75p9          1/1     Running     0          5m16s   10.131.0.24   user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal
example-registry-quay-postgres-init-8s8g9              0/1     Completed   0          5m54s   10.130.2.79   user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal
example-registry-quay-redis-5688ddcdb6-ndp4t           1/1     Running     0          5m56s   10.130.2.78   user1-jcnp6-worker-d-m9gg4.c.quay-devel.internal
quay-operator.v3.4.1-6f6597d8d8-bd4dp                  1/1     Running     0          22m     10.131.0.16   user1-jcnp6-worker-d-h5tv2.c.quay-devel.internal</pre>
							</p></div></li></ol></div></section></section><section class="section" id="monitoring-single-namespace"><div class="titlepage"><div><div><h2 class="title">8.2. Enabling monitoring when the Red Hat Quay Operator is installed in a single namespace</h2></div></div></div><p>
				When the Red Hat Quay Operator is installed in a single namespace, the monitoring component is set to <code class="literal">unmanaged</code>. To configure monitoring, you need to enable it for user-defined namespaces in OpenShift Container Platform.
			</p><p>
				For more information, see the OpenShift Container Platform documentation for <a class="link" href="https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html">Configuring the monitoring stack</a> and <a class="link" href="https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html">Enabling monitoring for user-defined projects</a>.
			</p><p>
				The following sections shows you how to enable monitoring for Red Hat Quay based on the OpenShift Container Platform documentation.
			</p><section class="section" id="creating-cluster-monitoring-config-map"><div class="titlepage"><div><div><h3 class="title">8.2.1. Creating a cluster monitoring config map</h3></div></div></div><p>
					Use the following procedure check if the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object exists.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the following command to check whether the <code class="literal">cluster-monitoring-config</code> ConfigMap object exists:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get configmap cluster-monitoring-config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Error from server (NotFound): configmaps "cluster-monitoring-config" not found</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Optional: If the <code class="literal">ConfigMap</code> object does not exist, create a YAML manifest. In the following example, the file is called <code class="literal">cluster-monitoring-config.yaml</code>.
						</p><pre class="programlisting language-terminal">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |</pre></li><li class="listitem"><p class="simpara">
							Optional: If the <code class="literal">ConfigMap</code> object does not exist, create the <code class="literal">ConfigMap</code> object:
						</p><pre class="programlisting language-terminal">$ oc apply -f cluster-monitoring-config.yaml configmap/cluster-monitoring-config created</pre></li><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal">ConfigMap</code> object exists by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get configmap cluster-monitoring-config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                        DATA   AGE
cluster-monitoring-config   1      12s</pre>
							</p></div></li></ol></div></section><section class="section" id="creating-user-defined-workload-monitoring-config-map"><div class="titlepage"><div><div><h3 class="title">8.2.2. Creating a user-defined workload monitoring ConfigMap object</h3></div></div></div><p>
					Use the following procedure check if the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object exists.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the following command to check whether the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object exists:
						</p><pre class="screen">$ oc -n openshift-user-workload-monitoring get configmap user-workload-monitoring-config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Error from server (NotFound): configmaps "user-workload-monitoring-config" not found</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							If the <code class="literal">ConfigMap</code> object does not exist, create a YAML manifest. In the following example, the file is called <code class="literal">user-workload-monitoring-config.yaml</code>.
						</p><pre class="programlisting language-terminal">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |</pre></li><li class="listitem"><p class="simpara">
							Optional: Create the <code class="literal">ConfigMap</code> object by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f user-workload-monitoring-config.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">configmap/user-workload-monitoring-config created</pre>
							</p></div></li></ol></div></section><section class="section" id="enabling-monitoring-user-defined-projects"><div class="titlepage"><div><div><h3 class="title">8.2.3. Enable monitoring for user-defined projects</h3></div></div></div><p>
					Use the following procedure to enable monitoring for user-defined projects.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the following command to check if monitoring for user-defined projects is running:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-user-workload-monitoring</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">No resources found in openshift-user-workload-monitoring namespace.</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> by entering the following command:
						</p><pre class="screen">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal">enableUserWorkload: true</code> in your <code class="literal">config.yaml</code> file to enable monitoring for user-defined projects on the cluster:
						</p><pre class="programlisting language-yaml">apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
kind: ConfigMap
metadata:
  annotations:</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to save the file, apply the changes, and ensure that the appropriate pods are running:
						</p><pre class="screen">$ oc get pods -n openshift-user-workload-monitoring</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-6f96b4b8f8-gq6rl   2/2     Running   0          15s
prometheus-user-workload-0             5/5     Running   1          12s
prometheus-user-workload-1             5/5     Running   1          12s
thanos-ruler-user-workload-0           3/3     Running   0          8s
thanos-ruler-user-workload-1           3/3     Running   0          8s</pre>
							</p></div></li></ol></div></section><section class="section" id="creating-service-object-expose-quay-metrics"><div class="titlepage"><div><div><h3 class="title">8.2.4. Creating a Service object to expose Red Hat Quay metrics</h3></div></div></div><p>
					Use the following procedure to create a <code class="literal">Service</code> object to expose Red Hat Quay metrics.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file for the Service object:
						</p><pre class="screen">$ cat &lt;&lt;EOF &gt;  quay-service.yaml

apiVersion: v1
kind: Service
metadata:
  annotations:
  labels:
    quay-component: monitoring
    quay-operator/quayregistry: example-registry
  name: example-registry-quay-metrics
  namespace: quay-enterprise
spec:
  ports:
  - name: quay-metrics
    port: 9091
    protocol: TCP
    targetPort: 9091
  selector:
    quay-component: quay-app
    quay-operator/quayregistry: example-registry
  type: ClusterIP
EOF</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Service</code> object by entering the following command:
						</p><pre class="programlisting language-terminal">$  oc apply -f quay-service.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">service/example-registry-quay-metrics created</pre>
							</p></div></li></ol></div></section><section class="section" id="creating-servicemonitor-object"><div class="titlepage"><div><div><h3 class="title">8.2.5. Creating a ServiceMonitor object</h3></div></div></div><p>
					Use the following procedure to configure OpenShift Monitoring to scrape the metrics by creating a <code class="literal">ServiceMonitor</code> resource.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file for the <code class="literal">ServiceMonitor</code> resource:
						</p><pre class="screen">$ cat &lt;&lt;EOF &gt;  quay-service-monitor.yaml

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    quay-operator/quayregistry: example-registry
  name: example-registry-quay-metrics-monitor
  namespace: quay-enterprise
spec:
  endpoints:
  - port: quay-metrics
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      quay-component: monitoring
EOF</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">ServiceMonitor</code> resource by entering the following command:
						</p><pre class="screen">$ oc apply -f quay-service-monitor.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">servicemonitor.monitoring.coreos.com/example-registry-quay-metrics-monitor created</pre>
							</p></div></li></ol></div></section><section class="section" id="view-metrics-in-ocp"><div class="titlepage"><div><div><h3 class="title">8.2.6. Viewing metrics in OpenShift Container Platform</h3></div></div></div><p>
					You can access the metrics in the OpenShift Container Platform console under <span class="strong strong"><strong>Monitoring</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>. In the Expression field, enter <span class="strong strong"><strong>quay_</strong></span> to see the list of metrics available:
				</p><p>
					<span class="inlinemediaobject"><img src="images/metrics-single-namespace.png" alt="Quay metrics"/></span>
				</p><p>
					For example, if you have added users to your registry, select the <span class="strong strong"><strong>quay-users_rows</strong></span> metric:
				</p><p>
					<span class="inlinemediaobject"><img src="images/metrics-single-namespace-users.png" alt="Quay metrics"/></span>
				</p></section></section><section class="section" id="operator-resize-storage"><div class="titlepage"><div><div><h2 class="title">8.3. Resizing Managed Storage</h2></div></div></div><p>
				The Red Hat Quay Operator creates default object storage using the defaults provided by Red Hat OpenShift Data Foundation when creating a <code class="literal">NooBaa</code> object (50 Gib).
			</p><p>
				There are two ways to extend <code class="literal">NooBaa</code> object storage:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						You can resize an existing persistent volume claim (PVC).
					</li><li class="listitem">
						You can add more PVCs to a new storage pool.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Expanding CSI volumes is a Technology Preview feature only. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/storage/expanding-persistent-volumes">https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/storage/expanding-persistent-volumes</a>.
				</p></div></div><section class="section" id="resizing-noobaa-pvc"><div class="titlepage"><div><div><h3 class="title">8.3.1. Resizing the NooBaa PVC</h3></div></div></div><p>
					Use the following procedure to resize the NooBaa PVC.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log into the OpenShift Container Platform console and select <span class="strong strong"><strong>Storage</strong></span> → <span class="strong strong"><strong>Persistent Volume Claims</strong></span>.
						</li><li class="listitem">
							Select the <code class="literal">PersistentVolumeClaim</code> named like <code class="literal">noobaa-default-backing-store-noobaa-pvc-*</code>.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Action</strong></span> menu, select <span class="strong strong"><strong>Expand PVC</strong></span>.
						</li><li class="listitem">
							Enter the new size of the Persistent Volume Claim and select <span class="strong strong"><strong>Expand</strong></span>.
						</li></ol></div><p>
					After a few minutes (depending on the size of the PVC), the expanded size should reflect in the PVC’s <span class="strong strong"><strong>Capacity</strong></span> field.
				</p></section><section class="section" id="adding-another-storage-pool"><div class="titlepage"><div><div><h3 class="title">8.3.2. Adding an additional storage pool</h3></div></div></div><p>
					Use the following procedure to add a second storage pool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log into the OpenShift Container Platform console and select <span class="strong strong"><strong>Networking</strong></span> → <span class="strong strong"><strong>Routes</strong></span>. Verify that the <code class="literal">openshift-storage</code> project is selected.
						</li><li class="listitem">
							Click on the <span class="strong strong"><strong>Location</strong></span> field for the <code class="literal">noobaa-mgmt</code> route.
						</li><li class="listitem">
							Log into the Noobaa Management Console.
						</li><li class="listitem">
							On the main dashboard, select <span class="strong strong"><strong>Add Storage Resources</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Deploy Kubernetes Pool</strong></span>.
						</li><li class="listitem">
							Enter a new pool name and then click <span class="strong strong"><strong>Next</strong></span>.
						</li><li class="listitem">
							Choose the number of pods to manage the storage pool and set the size per node.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Next</strong></span> → <span class="strong strong"><strong>Deploy</strong></span>.
						</li></ol></div><p>
					After a few minutes, the additional storage pool will be added to the NooBaa resources and available for use by Red Hat Quay.
				</p></section></section><section class="section" id="operator-customize-images"><div class="titlepage"><div><div><h2 class="title">8.4. Customizing Default Operator Images</h2></div></div></div><p>
				In certain circumstances, it might be useful to override the default images used by the Red Hat Quay Operator. This can be done by setting one or more environment variables in the Red Hat Quay Operator <code class="literal">ClusterServiceVersion</code>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Using this mechanism is not supported for production Red Hat Quay environments and is strongly encouraged only for development or testing purposes. There is no guarantee your deployment will work correctly when using non-default images with the Red Hat Quay Operator.
				</p></div></div><section class="section" id="custom-environment-variables"><div class="titlepage"><div><div><h3 class="title">8.4.1. Environment Variables</h3></div></div></div><p>
					The following environment variables are used in the Red Hat Quay Operator to override component images:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><tbody><tr><td align="left" valign="top">
								<p>
									Environment Variable
								</p>
								</td><td align="left" valign="top">
								<p>
									Component
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									<code class="literal">RELATED_IMAGE_COMPONENT_QUAY</code>
								</p>
								</td><td align="left" valign="top">
								<p>
									<code class="literal">base</code>
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									<code class="literal">RELATED_IMAGE_COMPONENT_CLAIR</code>
								</p>
								</td><td align="left" valign="top">
								<p>
									<code class="literal">clair</code>
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									<code class="literal">RELATED_IMAGE_COMPONENT_POSTGRES</code>
								</p>
								</td><td align="left" valign="top">
								<p>
									<code class="literal">postgres</code> and <code class="literal">clair</code> databases
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									<code class="literal">RELATED_IMAGE_COMPONENT_REDIS</code>
								</p>
								</td><td align="left" valign="top">
								<p>
									<code class="literal">redis</code>
								</p>
								</td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Overridden images <span class="strong strong"><strong>must</strong></span> be referenced by manifest (@sha256:) and not by tag (:latest).
					</p></div></div></section><section class="section" id="applying-overrides-to-running-operator"><div class="titlepage"><div><div><h3 class="title">8.4.2. Applying overrides to a running Operator</h3></div></div></div><p>
					When the Red Hat Quay Operator is installed in a cluster through the <a class="link" href="https://docs.openshift.com/container-platform/4.6/operators/understanding/olm/olm-understanding-olm.html">Operator Lifecycle Manager (OLM)</a>, the managed component container images can be easily overridden by modifying the <code class="literal">ClusterServiceVersion</code> object.
				</p><p>
					Use the following procedure to apply overrides to a running Operator.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The <code class="literal">ClusterServiceVersion</code> object is Operator Lifecycle Manager’s representation of a running Operator in the cluster. Find the Red Hat Quay Operator’s <code class="literal">ClusterServiceVersion</code> by using a Kubernetes UI or the <code class="literal">kubectl</code>/<code class="literal">oc</code> CLI tool. For example:
						</p><pre class="programlisting language-terminal">$ oc get clusterserviceversions -n &lt;your-namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Using the UI, <code class="literal">oc edit</code>, or another method, modify the Red Hat Quay <code class="literal">ClusterServiceVersion</code> to include the environment variables outlined above to point to the override images:
						</p><p class="simpara">
							<span class="strong strong"><strong>JSONPath</strong></span>: <code class="literal">spec.install.spec.deployments[0].spec.template.spec.containers[0].env</code>
						</p><pre class="programlisting language-yaml">- name: RELATED_IMAGE_COMPONENT_QUAY
  value: quay.io/projectquay/quay@sha256:c35f5af964431673f4ff5c9e90bdf45f19e38b8742b5903d41c10cc7f6339a6d
- name: RELATED_IMAGE_COMPONENT_CLAIR
  value: quay.io/projectquay/clair@sha256:70c99feceb4c0973540d22e740659cd8d616775d3ad1c1698ddf71d0221f3ce6
- name: RELATED_IMAGE_COMPONENT_POSTGRES
  value: centos/postgresql-10-centos7@sha256:de1560cb35e5ec643e7b3a772ebaac8e3a7a2a8e8271d9e91ff023539b4dfb33
- name: RELATED_IMAGE_COMPONENT_REDIS
  value: centos/redis-32-centos7@sha256:06dbb609484330ec6be6090109f1fa16e936afcf975d1cbc5fff3e6c7cae7542</pre></li></ol></div><p>
					Note that this is done at the Operator level, so every QuayRegistry will be deployed using these same overrides.
				</p></section></section><section class="section" id="operator-cloudfront"><div class="titlepage"><div><div><h2 class="title">8.5. AWS S3 CloudFront</h2></div></div></div><p>
				Use the following procedure if you are using AWS S3 Cloudfront for your backend registry storage.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Enter the following command to specify the registry key:
					</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config_awss3cloudfront.yaml --from-file default-cloudfront-signing-key.pem=./default-cloudfront-signing-key.pem test-config-bundle</pre></li></ol></div></section><section class="section" id="clair-advanced-configuration-overview"><div class="titlepage"><div><div><h2 class="title">8.6. Advanced Clair configuration</h2></div></div></div><p>
				Use the procedures in the following sections to configure advanced Clair settings.
			</p><section class="section" id="unmanaged-clair-configuration"><div class="titlepage"><div><div><h3 class="title">8.6.1. Unmanaged Clair configuration</h3></div></div></div><p>
					Red Hat Quay users can run an unmanaged Clair configuration with the Red Hat Quay OpenShift Container Platform Operator. This feature allows users to create an unmanaged Clair database, or run their custom Clair configuration without an unmanaged database.
				</p><p>
					An unmanaged Clair database allows the Red Hat Quay Operator to work in a geo-replicated environment, where multiple instances of the Operator must communicate with the same database. An unmanaged Clair database can also be used when a user requires a highly-available (HA) Clair database that exists outside of a cluster.
				</p><section class="section" id="unmanaging-clair-database"><div class="titlepage"><div><div><h4 class="title">8.6.1.1. Running a custom Clair configuration with an unmanaged Clair database</h4></div></div></div><p>
						Use the following procedure to set your Clair database to unmanaged.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								In the Quay Operator, set the <code class="literal">clairpostgres</code> component of the <code class="literal">QuayRegistry</code> custom resource to <code class="literal">managed: false</code>:
							</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: quay370
spec:
  configBundleSecret: config-bundle-secret
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: clairpostgres
      managed: false</pre></li></ul></div></section><section class="section" id="configuring-custom-clair-database"><div class="titlepage"><div><div><h4 class="title">8.6.1.2. Configuring a custom Clair database with an unmanaged Clair database</h4></div></div></div><p>
						The Red Hat Quay Operator for OpenShift Container Platform allows users to provide their own Clair database.
					</p><p>
						Use the following procedure to create a custom Clair database.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The following procedure sets up Clair with SSL/TLS certifications. To view a similar procedure that does not set up Clair with SSL/TSL certifications, see "Configuring a custom Clair database with a managed Clair configuration".
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a Quay configuration bundle secret that includes the <code class="literal">clair-config.yaml</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml --from-file extra_ca_cert_rds-ca-2019-root.pem=./rds-ca-2019-root.pem --from-file clair-config.yaml=./clair-config.yaml --from-file ssl.cert=./ssl.cert --from-file ssl.key=./ssl.key config-bundle-secret</pre><div class="formalpara"><p class="title"><strong>Example Clair <code class="literal">config.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">indexer:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslrootcert=/run/certs/rds-ca-2019-root.pem sslmode=verify-ca
    layer_scan_concurrency: 6
    migrations: true
    scanlock_retry: 11
log_level: debug
matcher:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslrootcert=/run/certs/rds-ca-2019-root.pem sslmode=verify-ca
    migrations: true
metrics:
    name: prometheus
notifier:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslrootcert=/run/certs/rds-ca-2019-root.pem sslmode=verify-ca
    migrations: true</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The database certificate is mounted under <code class="literal">/run/certs/rds-ca-2019-root.pem</code> on the Clair application pod in the <code class="literal">clair-config.yaml</code>. It must be specified when configuring your <code class="literal">clair-config.yaml</code>.
										</li><li class="listitem">
											An example <code class="literal">clair-config.yaml</code> can be found at <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/quay_operator_features#clair-openshift-config">Clair on OpenShift config</a>.
										</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">clair-config.yaml</code> file to your bundle secret, for example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: config-bundle-secret
  namespace: quay-enterprise
data:
  config.yaml: &lt;base64 encoded Quay config&gt;
  clair-config.yaml: &lt;base64 encoded Clair config&gt;
  extra_ca_cert_&lt;name&gt;: &lt;base64 encoded ca cert&gt;
  clair-ssl.crt: &gt;-
  clair-ssl.key: &gt;-</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									When updated, the provided <code class="literal">clair-config.yaml</code> file is mounted into the Clair pod. Any fields not provided are automatically populated with defaults using the Clair configuration module.
								</p></div></div></li><li class="listitem"><p class="simpara">
								You can check the status of your Clair pod by clicking the commit in the <span class="strong strong"><strong>Build History</strong></span> page, or by running <code class="literal">oc get pods -n &lt;namespace&gt;</code>. For example:
							</p><pre class="screen">$ oc get pods -n &lt;namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">NAME                                               READY   STATUS    RESTARTS   AGE
f192fe4a-c802-4275-bcce-d2031e635126-9l2b5-25lg2   1/1     Running   0          7s</pre>
								</p></div></li></ol></div></section></section><section class="section" id="custom-clair-configuration-managed-database"><div class="titlepage"><div><div><h3 class="title">8.6.2. Running a custom Clair configuration with a managed Clair database</h3></div></div></div><p>
					In some cases, users might want to run a custom Clair configuration with a managed Clair database. This is useful in the following scenarios:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When a user wants to disable specific updater resources.
						</li><li class="listitem"><p class="simpara">
							When a user is running Red Hat Quay in an disconnected environment. For more information about running Clair in a disconnected environment, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#clair-openshift-airgap-database">Configuring access to the Clair database in the air-gapped OpenShift cluster</a>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										If you are running Red Hat Quay in an disconnected environment, the <code class="literal">airgap</code> parameter of your <code class="literal">clair-config.yaml</code> must be set to <code class="literal">true</code>.
									</li><li class="listitem">
										If you are running Red Hat Quay in an disconnected environment, you should disable all updater components.
									</li></ul></div></div></div></li></ul></div><section class="section" id="managed-clair-database"><div class="titlepage"><div><div><h4 class="title">8.6.2.1. Setting a Clair database to managed</h4></div></div></div><p>
						Use the following procedure to set your Clair database to managed.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								In the Quay Operator, set the <code class="literal">clairpostgres</code> component of the <code class="literal">QuayRegistry</code> custom resource to <code class="literal">managed: true</code>:
							</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: quay370
spec:
  configBundleSecret: config-bundle-secret
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: clairpostgres
      managed: true</pre></li></ul></div></section><section class="section" id="configuring-custom-clair-database-managed"><div class="titlepage"><div><div><h4 class="title">8.6.2.2. Configuring a custom Clair database with a managed Clair configuration</h4></div></div></div><p>
						The Red Hat Quay Operator for OpenShift Container Platform allows users to provide their own Clair database.
					</p><p>
						Use the following procedure to create a custom Clair database.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a Quay configuration bundle secret that includes the <code class="literal">clair-config.yaml</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml --from-file extra_ca_cert_rds-ca-2019-root.pem=./rds-ca-2019-root.pem --from-file clair-config.yaml=./clair-config.yaml config-bundle-secret</pre><div class="formalpara"><p class="title"><strong>Example Clair <code class="literal">config.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">indexer:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslmode=disable
    layer_scan_concurrency: 6
    migrations: true
    scanlock_retry: 11
log_level: debug
matcher:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslmode=disable
    migrations: true
metrics:
    name: prometheus
notifier:
    connstring: host=quay-server.example.com port=5432 dbname=quay user=quayrdsdb password=quayrdsdb sslmode=disable
    migrations: true</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The database certificate is mounted under <code class="literal">/run/certs/rds-ca-2019-root.pem</code> on the Clair application pod in the <code class="literal">clair-config.yaml</code>. It must be specified when configuring your <code class="literal">clair-config.yaml</code>.
										</li><li class="listitem">
											An example <code class="literal">clair-config.yaml</code> can be found at <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/quay_operator_features#clair-openshift-config">Clair on OpenShift config</a>.
										</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">clair-config.yaml</code> file to your bundle secret, for example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: config-bundle-secret
  namespace: quay-enterprise
data:
  config.yaml: &lt;base64 encoded Quay config&gt;
  clair-config.yaml: &lt;base64 encoded Clair config&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											When updated, the provided <code class="literal">clair-config.yaml</code> file is mounted into the Clair pod. Any fields not provided are automatically populated with defaults using the Clair configuration module.
										</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
								You can check the status of your Clair pod by clicking the commit in the <span class="strong strong"><strong>Build History</strong></span> page, or by running <code class="literal">oc get pods -n &lt;namespace&gt;</code>. For example:
							</p><pre class="screen">$ oc get pods -n &lt;namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">NAME                                               READY   STATUS    RESTARTS   AGE
f192fe4a-c802-4275-bcce-d2031e635126-9l2b5-25lg2   1/1     Running   0          7s</pre>
								</p></div></li></ol></div></section></section><section class="section" id="clair-disconnected-environments"><div class="titlepage"><div><div><h3 class="title">8.6.3. Clair in disconnected environments</h3></div></div></div><p>
					Clair uses a set of components called <span class="emphasis"><em>updaters</em></span> to handle the fetching and parsing of data from various vulnerability databases. Updaters are set up by default to pull vulnerability data directly from the internet and work for immediate use. However, some users might require Red Hat Quay to run in a disconnected environment, or an environment without direct access to the internet. Clair supports disconnected environments by working with different types of update workflows that take network isolation into consideration. This works by using the <code class="literal">clairctl</code> command line interface tool, which obtains updater data from the internet by using an open host, securely transferring the data to an isolated host, and then important the updater data on the isolated host into Clair.
				</p><p>
					Use this guide to deploy Clair in a disconnected environment.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Currently, Clair enrichment data is CVSS data. Enrichment data is currently unsupported in disconnected environments.
					</p></div></div><p>
					For more information about Clair updaters, see "Clair updaters".
				</p><section class="section" id="clair-disconnected-ocp-configuration"><div class="titlepage"><div><div><h4 class="title">8.6.3.1. Setting up Clair in a disconnected OpenShift Container Platform cluster</h4></div></div></div><p>
						Use the following procedures to set up an OpenShift Container Platform provisioned Clair pod in a disconnected OpenShift Container Platform cluster.
					</p><section class="section" id="clair-clairctl-ocp"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.1. Installing the clairctl command line utility tool for OpenShift Container Platform deployments</h5></div></div></div><p>
							Use the following procedure to install the <code class="literal">clairctl</code> CLI tool for OpenShift Container Platform deployments.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Install the <code class="literal">clairctl</code> program for a Clair deployment in an OpenShift Container Platform cluster by entering the following command:
								</p><pre class="programlisting language-terminal">$ oc -n quay-enterprise exec example-registry-clair-app-64dd48f866-6ptgw -- cat /usr/bin/clairctl &gt; clairctl</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Unofficially, the <code class="literal">clairctl</code> tool can be downloaded
									</p></div></div></li><li class="listitem"><p class="simpara">
									Set the permissions of the <code class="literal">clairctl</code> file so that it can be executed and run by the user, for example:
								</p><pre class="programlisting language-terminal">$ chmod u+x ./clairctl</pre></li></ol></div></section><section class="section" id="clair-openshift-config"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.2. Retrieving and decoding the Clair configuration secret for Clair deployments on OpenShift Container Platform</h5></div></div></div><p>
							Use the following procedure to retrieve and decode the configuration secret for an OpenShift Container Platform provisioned Clair instance on OpenShift Container Platform.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Enter the following command to retrieve and decode the configuration secret, and then save it to a Clair configuration YAML:
								</p><pre class="programlisting language-terminal">$ oc get secret -n quay-enterprise example-registry-clair-config-secret  -o "jsonpath={$.data['config\.yaml']}" | base64 -d &gt; clair-config.yaml</pre></li><li class="listitem"><p class="simpara">
									Update the <code class="literal">clair-config.yaml</code> file so that the <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code>, for example:
								</p><pre class="programlisting language-yaml">---
indexer:
  airgap: true
---
matcher:
  disable_updaters: true
---</pre></li></ol></div></section><section class="section" id="clair-export-bundle"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.3. Exporting the updaters bundle from a connected Clair instance</h5></div></div></div><p>
							Use the following procedure to export the updaters bundle from a Clair instance that has access to the internet.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have retrieved and decoded the Clair configuration secret, and saved it to a Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									From a Clair instance that has access to the internet, use the <code class="literal">clairctl</code> CLI tool with your configuration file to export the updaters bundle. For example:
								</p><pre class="programlisting language-terminal">$ ./clairctl --config ./config.yaml export-updaters updates.gz</pre></li></ul></div></section><section class="section" id="clair-openshift-airgap-database"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.4. Configuring access to the Clair database in the disconnected OpenShift Container Platform cluster</h5></div></div></div><p>
							Use the following procedure to configure access to the Clair database in your disconnected OpenShift Container Platform cluster.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have retrieved and decoded the Clair configuration secret, and saved it to a Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									You have exported the updaters bundle from a Clair instance that has access to the internet.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Determine your Clair database service by using the <code class="literal">oc</code> CLI tool, for example:
								</p><pre class="programlisting language-terminal">$ oc get svc -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                  TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                             AGE
example-registry-clair-app            ClusterIP      172.30.224.93    &lt;none&gt;        80/TCP,8089/TCP                     4d21h
example-registry-clair-postgres       ClusterIP      172.30.246.88    &lt;none&gt;        5432/TCP                            4d21h
...</pre>
									</p></div></li><li class="listitem"><p class="simpara">
									Forward the Clair database port so that it is accessible from the local machine. For example:
								</p><pre class="programlisting language-terminal">$ oc port-forward -n quay-enterprise service/example-registry-clair-postgres 5432:5432</pre></li><li class="listitem"><p class="simpara">
									Update your Clair <code class="literal">config.yaml</code> file, for example:
								</p><pre class="programlisting language-yaml">indexer:
    connstring: host=localhost port=5432 dbname=postgres user=postgres password=postgres sslmode=disable <span id="CO4-1"/><span class="callout">1</span>
    scanlock_retry: 10
    layer_scan_concurrency: 5
    migrations: true
    scanner:
    repo:
      rhel-repository-scanner: <span id="CO4-2"/><span class="callout">2</span>
        repo2cpe_mapping_file: /data/cpe-map.json
    package:
      rhel_containerscanner: <span id="CO4-3"/><span class="callout">3</span>
        name2repos_mapping_file: /data/repo-map.json</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace the value of the <code class="literal">host</code> in the multiple <code class="literal">connstring</code> fields with <code class="literal">localhost</code>.
										</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											For more information about the <code class="literal">rhel-repository-scanner</code> parameter, see "Mapping repositories to Common Product Enumeration information".
										</div></dd><dt><a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											For more information about the <code class="literal">rhel_containerscanner</code> parameter, see "Mapping repositories to Common Product Enumeration information".
										</div></dd></dl></div></li></ol></div></section><section class="section" id="clair-openshift-airgap-import-bundle"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.5. Importing the updaters bundle into the disconnected OpenShift Container Platform cluster</h5></div></div></div><p>
							Use the following procedure to import the updaters bundle into your disconnected OpenShift Container Platform cluster.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have retrieved and decoded the Clair configuration secret, and saved it to a Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									You have exported the updaters bundle from a Clair instance that has access to the internet.
								</li><li class="listitem">
									You have transferred the updaters bundle into your disconnected environment.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use the <code class="literal">clairctl</code> CLI tool to import the updaters bundle into the Clair database that is deployed by OpenShift Container Platform. For example:
								</p><pre class="programlisting language-terminal">$ ./clairctl --config ./clair-config.yaml import-updaters updates.gz</pre></li></ul></div></section></section><section class="section" id="clair-disconnected-standalone-configuration"><div class="titlepage"><div><div><h4 class="title">8.6.3.2. Setting up a self-managed deployment of Clair for a disconnected OpenShift Container Platform cluster</h4></div></div></div><p>
						Use the following procedures to set up a self-managed deployment of Clair for a disconnected OpenShift Container Platform cluster.
					</p><section class="section" id="clair-clairctl-standalone"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.1. Installing the clairctl command line utility tool for a self-managed Clair deployment on OpenShift Container Platform</h5></div></div></div><p>
							Use the following procedure to install the <code class="literal">clairctl</code> CLI tool for self-managed Clair deployments on OpenShift Container Platform.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Install the <code class="literal">clairctl</code> program for a self-managed Clair deployment by using the <code class="literal">podman cp</code> command, for example:
								</p><pre class="programlisting language-terminal">$ sudo podman cp clairv4:/usr/bin/clairctl ./clairctl</pre></li><li class="listitem"><p class="simpara">
									Set the permissions of the <code class="literal">clairctl</code> file so that it can be executed and run by the user, for example:
								</p><pre class="programlisting language-terminal">$ chmod u+x ./clairctl</pre></li></ol></div></section><section class="section" id="clair-standalone-config-location"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.2. Deploying a self-managed Clair container for disconnected OpenShift Container Platform clusters</h5></div></div></div><p>
							Use the following procedure to deploy a self-managed Clair container for disconnected OpenShift Container Platform clusters.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Create a folder for your Clair configuration file, for example:
								</p><pre class="programlisting language-terminal">$ mkdir /etc/clairv4/config/</pre></li><li class="listitem"><p class="simpara">
									Create a Clair configuration file with the <code class="literal">disable_updaters</code> parameter set to <code class="literal">true</code>, for example:
								</p><pre class="programlisting language-yaml">---
indexer:
  airgap: true
---
matcher:
  disable_updaters: true
---</pre></li><li class="listitem"><p class="simpara">
									Start Clair by using the container image, mounting in the configuration from the file you created:
								</p><pre class="screen">$ sudo podman run -it --rm --name clairv4 \
-p 8081:8081 -p 8088:8088 \
-e CLAIR_CONF=/clair/config.yaml \
-e CLAIR_MODE=combo \
-v /etc/clairv4/config:/clair:Z \
registry.redhat.io/quay/clair-rhel8:v3.9.0</pre></li></ol></div></section><section class="section" id="clair-export-bundle-standalone"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.3. Exporting the updaters bundle from a connected Clair instance</h5></div></div></div><p>
							Use the following procedure to export the updaters bundle from a Clair instance that has access to the internet.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have deployed Clair.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									From a Clair instance that has access to the internet, use the <code class="literal">clairctl</code> CLI tool with your configuration file to export the updaters bundle. For example:
								</p><pre class="programlisting language-terminal">$ ./clairctl --config ./config.yaml export-updaters updates.gz</pre></li></ul></div></section><section class="section" id="clair-openshift-airgap-database-standalone"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.4. Configuring access to the Clair database in the disconnected OpenShift Container Platform cluster</h5></div></div></div><p>
							Use the following procedure to configure access to the Clair database in your disconnected OpenShift Container Platform cluster.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have deployed Clair.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									You have exported the updaters bundle from a Clair instance that has access to the internet.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Determine your Clair database service by using the <code class="literal">oc</code> CLI tool, for example:
								</p><pre class="programlisting language-terminal">$ oc get svc -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                  TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                             AGE
example-registry-clair-app            ClusterIP      172.30.224.93    &lt;none&gt;        80/TCP,8089/TCP                     4d21h
example-registry-clair-postgres       ClusterIP      172.30.246.88    &lt;none&gt;        5432/TCP                            4d21h
...</pre>
									</p></div></li><li class="listitem"><p class="simpara">
									Forward the Clair database port so that it is accessible from the local machine. For example:
								</p><pre class="programlisting language-terminal">$ oc port-forward -n quay-enterprise service/example-registry-clair-postgres 5432:5432</pre></li><li class="listitem"><p class="simpara">
									Update your Clair <code class="literal">config.yaml</code> file, for example:
								</p><pre class="programlisting language-yaml">indexer:
    connstring: host=localhost port=5432 dbname=postgres user=postgres password=postgres sslmode=disable <span id="CO5-1"/><span class="callout">1</span>
    scanlock_retry: 10
    layer_scan_concurrency: 5
    migrations: true
    scanner:
    repo:
      rhel-repository-scanner: <span id="CO5-2"/><span class="callout">2</span>
        repo2cpe_mapping_file: /data/cpe-map.json
    package:
      rhel_containerscanner: <span id="CO5-3"/><span class="callout">3</span>
        name2repos_mapping_file: /data/repo-map.json</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace the value of the <code class="literal">host</code> in the multiple <code class="literal">connstring</code> fields with <code class="literal">localhost</code>.
										</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											For more information about the <code class="literal">rhel-repository-scanner</code> parameter, see "Mapping repositories to Common Product Enumeration information".
										</div></dd><dt><a href="#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											For more information about the <code class="literal">rhel_containerscanner</code> parameter, see "Mapping repositories to Common Product Enumeration information".
										</div></dd></dl></div></li></ol></div></section><section class="section" id="clair-openshift-airgap-import-bundle-standalone"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.5. Importing the updaters bundle into the disconnected OpenShift Container Platform cluster</h5></div></div></div><p>
							Use the following procedure to import the updaters bundle into your disconnected OpenShift Container Platform cluster.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed the <code class="literal">clairctl</code> command line utility tool.
								</li><li class="listitem">
									You have deployed Clair.
								</li><li class="listitem">
									The <code class="literal">disable_updaters</code> and <code class="literal">airgap</code> parameters are set to <code class="literal">true</code> in your Clair <code class="literal">config.yaml</code> file.
								</li><li class="listitem">
									You have exported the updaters bundle from a Clair instance that has access to the internet.
								</li><li class="listitem">
									You have transferred the updaters bundle into your disconnected environment.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use the <code class="literal">clairctl</code> CLI tool to import the updaters bundle into the Clair database that is deployed by OpenShift Container Platform:
								</p><pre class="programlisting language-terminal">$ ./clairctl --config ./clair-config.yaml import-updaters updates.gz</pre></li></ul></div></section></section></section><section class="section" id="clair-crda-configuration"><div class="titlepage"><div><div><h3 class="title">8.6.4. Enabling Clair CRDA</h3></div></div></div><p>
					Java scanning depends on a public, Red Hat provided API service called Code Ready Dependency Analytics (CRDA). CRDA is only available with internet access and is not enabled by default.
				</p><p>
					Use the following procedure to integrate the CRDA service with a custom API key and enable CRDA for Java and Python scanning.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat Quay 3.7 or greater
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Submit <a class="link" href="https://developers.redhat.com/content-gateway/link/3872178">the API key request form</a> to obtain the Quay-specific CRDA remote matcher.
						</li><li class="listitem"><p class="simpara">
							Set the CRDA configuration in your <code class="literal">clair-config.yaml</code> file:
						</p><pre class="programlisting language-terminal">matchers:
  config:
    crda:
      url: https://gw.api.openshift.io/api/v2/
      key: &lt;CRDA_API_KEY&gt; <span id="CO6-1"/><span class="callout">1</span>
      source: &lt;QUAY_SERVER_HOSTNAME&gt; <span id="CO6-2"/><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Insert the Quay-specific CRDA remote matcher from <a class="link" href="https://developers.redhat.com/content-gateway/link/3872178">the API key request form</a> here.
								</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The hostname of your Quay server.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="mapping-repositories-to-cpe-information"><div class="titlepage"><div><div><h3 class="title">8.6.5. Mapping repositories to Common Product Enumeration information</h3></div></div></div><p>
					Clair’s Red Hat Enterprise Linux (RHEL) scanner relies on a Common Product Enumeration (CPE) file to map RPM packages to the corresponding security data to produce matching results. These files are owned by product security and updated daily.
				</p><p>
					The CPE file must be present, or access to the file must be allowed, for the scanner to properly process RPM packages. If the file is not present, RPM packages installed in the container image will not be scanned.
				</p><div class="table" id="idm45898213905424"><p class="title"><strong>Table 8.1. Clair CPE mapping files</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45898213900592" scope="col">CPE</th><th align="left" valign="top" id="idm45898213899504" scope="col">Link to JSON mapping file</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45898213900592">
								<p>
									<code class="literal">repos2cpe</code>
								</p>
								</td><td align="left" valign="top" headers="idm45898213899504">
								<p>
									<a class="link" href="https://www.redhat.com/security/data/metrics/repository-to-cpe.json">Red Hat Repository-to-CPE JSON</a>
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45898213900592">
								<p>
									<code class="literal">names2repos</code>
								</p>
								</td><td align="left" valign="top" headers="idm45898213899504">
								<p>
									<a class="link" href="https://access.redhat.com/security/data/metrics/container-name-repos-map.json">Red Hat Name-to-Repos JSON</a>.
								</p>
								</td></tr></tbody></table></div></div><p>
					In addition to uploading CVE information to the database for disconnected Clair installations, you must also make the mapping file available locally:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For standalone Red Hat Quay and Clair deployments, the mapping file must be loaded into the Clair pod.
						</li><li class="listitem">
							For Red Hat Quay Operator deployments on OpenShift Container Platform and Clair deployments, you must set the Clair component to <code class="literal">unamanged</code>. Then, Clair must be deployed manually, setting the configuration to load a local copy of the mapping file.
						</li></ul></div><section class="section" id="mapping-repositories-to-cpe-configuration"><div class="titlepage"><div><div><h4 class="title">8.6.5.1. Mapping repositories to Common Product Enumeration example configuration</h4></div></div></div><p>
						Use the <code class="literal">repo2cpe_mapping_file</code> and <code class="literal">name2repos_mapping_file</code> fields in your Clair configuration to include the CPE JSON mapping files. For example:
					</p><pre class="programlisting language-yaml">indexer:
 scanner:
    repo:
      rhel-repository-scanner:
        repo2cpe_mapping_file: /data/cpe-map.json
    package:
      rhel_containerscanner:
        name2repos_mapping_file: /data/repo-map.json</pre><p>
						For more information, see <a class="link" href="https://www.redhat.com/en/blog/how-accurately-match-oval-security-data-installed-rpms">How to accurately match OVAL security data to installed RPMs</a>.
					</p></section></section></section></section><section class="chapter" id="red-hat-quay-builders-enhancement"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Red Hat Quay build enhancements</h1></div></div></div><p>
			Red Hat Quay builds can be run on virtualized platforms. Backwards compatibility to run previous build configurations are also available.
		</p><section class="section" id="red-hat-quay-builds-architecture"><div class="titlepage"><div><div><h2 class="title">9.1. Red Hat Quay enhanced build architecture</h2></div></div></div><p>
				The following image shows the expected design flow and architecture of the enhanced build features:
			</p><p>
				<span class="inlinemediaobject"><img src="images/quay-builds-architecture.png" alt="Enhanced Quay builds architecture"/></span>
			</p><p>
				With this enhancement, the build manager first creates the <code class="literal">Job Object</code>. Then, the <code class="literal">Job Object</code> then creates a pod using the <code class="literal">quay-builder-image</code>. The <code class="literal">quay-builder-image</code> will contain the <code class="literal">quay-builder binary</code> and the <code class="literal">Podman</code> service. The created pod runs as <code class="literal">unprivileged</code>. The <code class="literal">quay-builder binary</code> then builds the image while communicating status and retrieving build information from the Build Manager.
			</p></section><section class="section" id="red-hat-quay-build-limitations"><div class="titlepage"><div><div><h2 class="title">9.2. Red Hat Quay build limitations</h2></div></div></div><p>
				Running builds in Red Hat Quay in an unprivileged context might cause some commands that were working under the previous build strategy to fail. Attempts to change the build strategy could potentially cause performance issues and reliability with the build.
			</p><p>
				Running builds directly in a container does not have the same isolation as using virtual machines. Changing the build environment might also caused builds that were previously working to fail.
			</p></section><section class="section" id="builders-virtual-environment"><div class="titlepage"><div><div><h2 class="title">9.3. Creating a Red Hat Quay builders environment with OpenShift Container Platform</h2></div></div></div><p>
				The procedures in this section explain how to create a Red Hat Quay virtual builders environment with OpenShift Container Platform.
			</p><section class="section" id="openshift-tls-component"><div class="titlepage"><div><div><h3 class="title">9.3.1. OpenShift Container Platform TLS component</h3></div></div></div><p>
					The <code class="literal">tls</code> component allows you to control TLS configuration.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Red Hat Quay 3.9 does not support builders when the TLS component is managed by the Operator.
					</p></div></div><p>
					If you set <code class="literal">tls</code> to <code class="literal">unmanaged</code>, you supply your own <code class="literal">ssl.cert</code> and <code class="literal">ssl.key</code> files. In this instance, if you want your cluster to support builders, you must add both the Quay route and the builder route name to the SAN list in the cert, or use a wildcard.
				</p><p>
					To add the builder route, use the following format:
				</p><pre class="programlisting language-bash">[quayregistry-cr-name]-quay-builder-[ocp-namespace].[ocp-domain-name]:443</pre></section><section class="section" id="red-hat-quay-quota-builders-establishment"><div class="titlepage"><div><div><h3 class="title">9.3.2. Using OpenShift Container Platform for Red Hat Quay builders</h3></div></div></div><p>
					Builders require SSL/TLS certificates. For more information about SSL/TLS certificates, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/advanced_red_hat_quay_deployment#using_ssl_to_protect_connections_to_red_hat_quay">Adding TLS certificates to the Red Hat Quay container</a>.
				</p><p>
					If you are using Amazon Web Service (AWS) S3 storage, you must modify your storage bucket in the AWS console, prior to running builders. See "Modifying your AWS S3 storage bucket" in the following section for the required parameters.
				</p><section class="section" id="red-hat-quay-setting-up-builders"><div class="titlepage"><div><div><h4 class="title">9.3.2.1. Preparing OpenShift Container Platform for virtual builders</h4></div></div></div><p>
						Use the following procedure to prepare OpenShift Container Platform for Red Hat Quay virtual builders.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									This procedure assumes you already have a cluster provisioned and a Quay Operator running.
								</li><li class="listitem">
									This procedure is for setting up a virtual namespace on OpenShift Container Platform.
								</li></ul></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to your Red Hat Quay cluster using a cluster administrator account.
							</li><li class="listitem"><p class="simpara">
								Create a new project where your virtual builders will be run, for example, <code class="literal">virtual-builders</code>, by running the following command:
							</p><pre class="programlisting language-terminal">$ oc new-project virtual-builders</pre></li><li class="listitem"><p class="simpara">
								Create a <code class="literal">ServiceAccount</code> in the project that will be used to run builds by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create sa -n virtual-builders quay-builder</pre></li><li class="listitem"><p class="simpara">
								Provide the created service account with editing permissions so that it can run the build:
							</p><pre class="programlisting language-terminal">$ oc adm policy -n virtual-builders add-role-to-user edit system:serviceaccount:virtual-builders:quay-builder</pre></li><li class="listitem"><p class="simpara">
								Grant the Quay builder <code class="literal">anyuid scc</code> permissions by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc adm policy -n virtual-builders add-scc-to-user anyuid -z quay-builder</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									This action requires cluster admin privileges. This is required because builders must run as the Podman user for unprivileged or rootless builds to work.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Obtain the token for the Quay builder service account.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										If using OpenShift Container Platform 4.10 or an earlier version, enter the following command:
									</p><pre class="programlisting language-terminal">oc sa get-token -n virtual-builders quay-builder</pre></li><li class="listitem"><p class="simpara">
										If using OpenShift Container Platform 4.11 or later, enter the following command:
									</p><pre class="programlisting language-terminal">$ oc create token quay-builder -n virtual-builders</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">eyJhbGciOiJSUzI1NiIsImtpZCI6IldfQUJkaDVmb3ltTHZ0dGZMYjhIWnYxZTQzN2dJVEJxcDJscldSdEUtYWsifQ...</pre>
										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Determine the builder route by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc get route -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                  HOST/PORT                                                                    PATH   SERVICES                              PORT   TERMINATION     WILDCARD
...
example-registry-quay-builder         example-registry-quay-builder-quay-enterprise.apps.docs.quayteam.org                example-registry-quay-app             grpc   edge/Redirect   None
...</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Generate a self-signed SSL/TlS certificate with the .crt extension by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc extract cm/kube-root-ca.crt -n openshift-apiserver</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">ca.crt</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Rename the <code class="literal">ca.crt</code> file to <code class="literal">extra_ca_cert_build_cluster.crt</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ mv ca.crt extra_ca_cert_build_cluster.crt</pre></li><li class="listitem"><p class="simpara">
								Locate the secret for you configuration bundle in the <span class="strong strong"><strong>Console</strong></span>, and select <span class="strong strong"><strong>Actions</strong></span> → <span class="strong strong"><strong>Edit Secret</strong></span> and add the appropriate builder configuration:
							</p><pre class="programlisting language-yaml">FEATURE_USER_INITIALIZE: true
BROWSER_API_CALLS_XHR_ONLY: false
SUPER_USERS:
- &lt;superusername&gt;
FEATURE_USER_CREATION: false
FEATURE_QUOTA_MANAGEMENT: true
FEATURE_BUILD_SUPPORT: True
BUILDMAN_HOSTNAME: &lt;sample_build_route&gt; <span id="CO7-1"/><span class="callout">1</span>
BUILD_MANAGER:
  - ephemeral
  - ALLOWED_WORKER_COUNT: 1
    ORCHESTRATOR_PREFIX: buildman/production/
    JOB_REGISTRATION_TIMEOUT: 3600 <span id="CO7-2"/><span class="callout">2</span>
    ORCHESTRATOR:
      REDIS_HOST: &lt;sample_redis_hostname&gt; <span id="CO7-3"/><span class="callout">3</span>
      REDIS_PASSWORD: ""
      REDIS_SSL: false
      REDIS_SKIP_KEYSPACE_EVENT_SETUP: false
    EXECUTORS:
      - EXECUTOR: kubernetesPodman
        NAME: openshift
        BUILDER_NAMESPACE: &lt;sample_builder_namespace&gt; <span id="CO7-4"/><span class="callout">4</span>
        SETUP_TIME: 180
        MINIMUM_RETRY_THRESHOLD: 0
        BUILDER_CONTAINER_IMAGE: &lt;sample_builder_container_image&gt; <span id="CO7-5"/><span class="callout">5</span>
        # Kubernetes resource options
        K8S_API_SERVER: &lt;sample_k8s_api_server&gt; <span id="CO7-6"/><span class="callout">6</span>
        K8S_API_TLS_CA: &lt;sample_crt_file&gt; <span id="CO7-7"/><span class="callout">7</span>
        VOLUME_SIZE: 8G
        KUBERNETES_DISTRIBUTION: openshift
        CONTAINER_MEMORY_LIMITS: 300m <span id="CO7-8"/><span class="callout">8</span>
        CONTAINER_CPU_LIMITS: 1G <span id="CO7-9"/><span class="callout">9</span>
        CONTAINER_MEMORY_REQUEST: 300m <span id="CO7-10"/><span class="callout">10</span>
        CONTAINER_CPU_REQUEST: 1G <span id="CO7-11"/><span class="callout">11</span>
        NODE_SELECTOR_LABEL_KEY: ""
        NODE_SELECTOR_LABEL_VALUE: ""
        SERVICE_ACCOUNT_NAME: &lt;sample_service_account_name&gt;
        SERVICE_ACCOUNT_TOKEN: &lt;sample_account_token&gt; <span id="CO7-12"/><span class="callout">12</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The build route is obtained by running <code class="literal">oc get route -n</code> with the name of your OpenShift Operator’s namespace. A port must be provided at the end of the route, and it should use the following format: <code class="literal">[quayregistry-cr-name]-quay-builder-[ocp-namespace].[ocp-domain-name]:443</code>.
									</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										If the <code class="literal">JOB_REGISTRATION_TIMEOUT</code> parameter is set too low, you might receive the following error: <code class="literal">failed to register job to build manager: rpc error: code = Unauthenticated desc = Invalid build token: Signature has expired</code>. It is suggested that this parameter be set to at least 240.
									</div></dd><dt><a href="#CO7-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										If your Redis host has a password or SSL/TLS certificates, you must update accordingly.
									</div></dd><dt><a href="#CO7-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Set to match the name of your virtual builders namespace, for example, <code class="literal">virtual-builders</code>.
									</div></dd><dt><a href="#CO7-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										For early access, the <code class="literal">BUILDER_CONTAINER_IMAGE</code> is currently <code class="literal">quay.io/projectquay/quay-builder:3.7.0-rc.2</code>. Note that this might change during the early access window. If this happens, customers are alerted.
									</div></dd><dt><a href="#CO7-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The <code class="literal">K8S_API_SERVER</code> is obtained by running <code class="literal">oc cluster-info</code>.
									</div></dd><dt><a href="#CO7-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										You must manually create and add your custom CA cert, for example, <code class="literal">K8S_API_TLS_CA: /conf/stack/extra_ca_certs/build_cluster.crt</code>.
									</div></dd><dt><a href="#CO7-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Defaults to <code class="literal">5120Mi</code> if left unspecified.
									</div></dd><dt><a href="#CO7-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										For virtual builds, you must ensure that there are enough resources in your cluster. Defaults to <code class="literal">1000m</code> if left unspecified.
									</div></dd><dt><a href="#CO7-10"><span class="callout">10</span></a> </dt><dd><div class="para">
										Defaults to <code class="literal">3968Mi</code> if left unspecified.
									</div></dd><dt><a href="#CO7-11"><span class="callout">11</span></a> </dt><dd><div class="para">
										Defaults to <code class="literal">500m</code> if left unspecified.
									</div></dd><dt><a href="#CO7-12"><span class="callout">12</span></a> </dt><dd><div class="para">
										Obtained when running <code class="literal">oc create sa</code>.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample configuration</strong></p><p>
									
<pre class="programlisting language-yaml">FEATURE_USER_INITIALIZE: true
BROWSER_API_CALLS_XHR_ONLY: false
SUPER_USERS:
- quayadmin
FEATURE_USER_CREATION: false
FEATURE_QUOTA_MANAGEMENT: true
FEATURE_BUILD_SUPPORT: True
BUILDMAN_HOSTNAME: example-registry-quay-builder-quay-enterprise.apps.docs.quayteam.org:443
BUILD_MANAGER:
  - ephemeral
  - ALLOWED_WORKER_COUNT: 1
    ORCHESTRATOR_PREFIX: buildman/production/
    JOB_REGISTRATION_TIMEOUT: 3600
    ORCHESTRATOR:
      REDIS_HOST: example-registry-quay-redis
      REDIS_PASSWORD: ""
      REDIS_SSL: false
      REDIS_SKIP_KEYSPACE_EVENT_SETUP: false
    EXECUTORS:
      - EXECUTOR: kubernetesPodman
        NAME: openshift
        BUILDER_NAMESPACE: virtual-builders
        SETUP_TIME: 180
        MINIMUM_RETRY_THRESHOLD: 0
        BUILDER_CONTAINER_IMAGE: quay.io/projectquay/quay-builder:3.7.0-rc.2
        # Kubernetes resource options
        K8S_API_SERVER: api.docs.quayteam.org:6443
        K8S_API_TLS_CA: /conf/stack/extra_ca_certs/build_cluster.crt
        VOLUME_SIZE: 8G
        KUBERNETES_DISTRIBUTION: openshift
        CONTAINER_MEMORY_LIMITS: 1G
        CONTAINER_CPU_LIMITS: 1080m
        CONTAINER_MEMORY_REQUEST: 1G
        CONTAINER_CPU_REQUEST: 580m
        NODE_SELECTOR_LABEL_KEY: ""
        NODE_SELECTOR_LABEL_VALUE: ""
        SERVICE_ACCOUNT_NAME: quay-builder
        SERVICE_ACCOUNT_TOKEN: "eyJhbGciOiJSUzI1NiIsImtpZCI6IldfQUJkaDVmb3ltTHZ0dGZMYjhIWnYxZTQzN2dJVEJxcDJscldSdEUtYWsifQ"</pre>
								</p></div></li></ol></div></section><section class="section" id="red-hat-quay-manual-ssl-for-builders"><div class="titlepage"><div><div><h4 class="title">9.3.2.2. Manually adding SSL/TLS certificates</h4></div></div></div><p>
						Due to a known issue with the configuration tool, you must manually add your custom SSL/TLS certificates to properly run builders. Use the following procedure to manually add custom SSL/TLS certificates.
					</p><p>
						For more information creating SSL/TLS certificates, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/advanced_red_hat_quay_deployment#using_ssl_to_protect_connections_to_red_hat_quay">Adding TLS certificates to the Red Hat Quay container</a>.
					</p><section class="section" id="create-sign-certificates"><div class="titlepage"><div><div><h5 class="title">9.3.2.2.1. Creating and signing certificates</h5></div></div></div><p>
							Use the following procedure to create and sign an SSL/TLS certificate.
						</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Create a certificate authority and sign a certificate. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/advanced_red_hat_quay_deployment#create-a-ca-and-sign-a-certificate">Create a Certificate Authority and sign a certificate</a>.
								</p><div class="formalpara"><p class="title"><strong>openssl.cnf</strong></p><p>
										
<pre class="programlisting language-terminal">[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = example-registry-quay-quay-enterprise.apps.docs.quayteam.org <span id="CO8-1"/><span class="callout">1</span>
DNS.2 = example-registry-quay-builder-quay-enterprise.apps.docs.quayteam.org <span id="CO8-2"/><span class="callout">2</span></pre>
									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											An <code class="literal">alt_name</code> for the URL of your Red Hat Quay registry must be included.
										</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											An <code class="literal">alt_name</code> for the <code class="literal">BUILDMAN_HOSTNAME</code>
										</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample commands</strong></p><p>
										
<pre class="programlisting language-terminal">$ openssl genrsa -out rootCA.key 2048
$ openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem
$ openssl genrsa -out ssl.key 2048
$ openssl req -new -key ssl.key -out ssl.csr
$ openssl x509 -req -in ssl.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out ssl.cert -days 356 -extensions v3_req -extfile openssl.cnf</pre>
									</p></div></li></ul></div></section><section class="section" id="setting-tls-unmanaged"><div class="titlepage"><div><div><h5 class="title">9.3.2.2.2. Setting TLS to unmanaged</h5></div></div></div><p>
							Use the following procedure to set <code class="literal">king:tls</code> to unmanaged.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									In your Red Hat Quay Registry YAML, set <code class="literal">kind: tls</code> to <code class="literal">managed: false</code>:
								</p><pre class="programlisting language-yaml">  - kind: tls
    managed: false</pre></li><li class="listitem"><p class="simpara">
									On the <span class="strong strong"><strong>Events</strong></span> page, the change is blocked until you set up the appropriate <code class="literal">config.yaml</code> file. For example:
								</p><pre class="programlisting language-yaml">    - lastTransitionTime: '2022-03-28T12:56:49Z'
      lastUpdateTime: '2022-03-28T12:56:49Z'
      message: &gt;-
        required component `tls` marked as unmanaged, but `configBundleSecret`
        is missing necessary fields
      reason: ConfigInvalid
      status: 'True'</pre></li></ol></div></section><section class="section" id="creating-temporary-secrets"><div class="titlepage"><div><div><h5 class="title">9.3.2.2.3. Creating temporary secrets</h5></div></div></div><p>
							Use the following procedure to create temporary secrets for the CA certificate.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Create a secret in your default namespace for the CA certificate:
								</p><pre class="screen">$ oc create secret generic -n quay-enterprise temp-crt --from-file extra_ca_cert_build_cluster.crt</pre></li><li class="listitem"><p class="simpara">
									Create a secret in your default namespace for the <code class="literal">ssl.key</code> and <code class="literal">ssl.cert</code> files:
								</p><pre class="screen">$ oc create secret generic -n quay-enterprise quay-config-ssl --from-file ssl.cert --from-file ssl.key</pre></li></ol></div></section><section class="section" id="copying-secret-data-to-config"><div class="titlepage"><div><div><h5 class="title">9.3.2.2.4. Copying secret data to the configuration YAML</h5></div></div></div><p>
							Use the following procedure to copy secret data to your <code class="literal">config.yaml</code> file.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
									Locate the new secrets in the console UI at <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
								</li><li class="listitem"><p class="simpara">
									For each secret, locate the YAML view:
								</p><pre class="programlisting language-yaml">kind: Secret
apiVersion: v1
metadata:
  name: temp-crt
  namespace: quay-enterprise
  uid: a4818adb-8e21-443a-a8db-f334ace9f6d0
  resourceVersion: '9087855'
  creationTimestamp: '2022-03-28T13:05:30Z'
...
data:
  extra_ca_cert_build_cluster.crt: &gt;-
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURNakNDQWhxZ0F3SUJBZ0l....
type: Opaque</pre><pre class="programlisting language-yaml">kind: Secret
apiVersion: v1
metadata:
  name: quay-config-ssl
  namespace: quay-enterprise
  uid: 4f5ae352-17d8-4e2d-89a2-143a3280783c
  resourceVersion: '9090567'
  creationTimestamp: '2022-03-28T13:10:34Z'
...
data:
  ssl.cert: &gt;-
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVaakNDQTA2Z0F3SUJBZ0lVT...
  ssl.key: &gt;-
    LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBc...
type: Opaque</pre></li><li class="listitem"><p class="simpara">
									Locate the secret for your Red Hat Quay registry configuration bundle in the UI, or through the command line by running a command like the following:
								</p><pre class="programlisting language-terminal">$ oc get quayregistries.quay.redhat.com -o jsonpath="{.items[0].spec.configBundleSecret}{'\n'}"  -n quay-enterprise</pre></li><li class="listitem"><p class="simpara">
									In the OpenShift Container Platform console, select the YAML tab for your configuration bundle secret, and add the data from the two secrets you created:
								</p><pre class="programlisting language-yaml">kind: Secret
apiVersion: v1
metadata:
  name: init-config-bundle-secret
  namespace: quay-enterprise
  uid: 4724aca5-bff0-406a-9162-ccb1972a27c1
  resourceVersion: '4383160'
  creationTimestamp: '2022-03-22T12:35:59Z'
...
data:
  config.yaml: &gt;-
    RkVBVFVSRV9VU0VSX0lOSVRJQUxJWkU6IHRydWUKQlJ...
  extra_ca_cert_build_cluster.crt: &gt;-
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURNakNDQWhxZ0F3SUJBZ0ldw....
  ssl.cert: &gt;-
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVaakNDQTA2Z0F3SUJBZ0lVT...
  ssl.key: &gt;-
    LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBc...
type: Opaque</pre></li><li class="listitem">
									Click <span class="strong strong"><strong>Save</strong></span>.
								</li><li class="listitem"><p class="simpara">
									Enter the following command to see if your pods are restarting:
								</p><pre class="programlisting language-terminal">$ oc get pods -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                                   READY   STATUS              RESTARTS   AGE
...
example-registry-quay-app-6786987b99-vgg2v             0/1     ContainerCreating   0          2s
example-registry-quay-app-7975d4889f-q7tvl             1/1     Running             0          5d21h
example-registry-quay-app-7975d4889f-zn8bb             1/1     Running             0          5d21h
example-registry-quay-app-upgrade-lswsn                0/1     Completed           0          6d1h
example-registry-quay-config-editor-77847fc4f5-nsbbv   0/1     ContainerCreating   0          2s
example-registry-quay-config-editor-c6c4d9ccd-2mwg2    1/1     Running             0          5d21h
example-registry-quay-database-66969cd859-n2ssm        1/1     Running             0          6d1h
example-registry-quay-mirror-764d7b68d9-jmlkk          1/1     Terminating         0          5d21h
example-registry-quay-mirror-764d7b68d9-jqzwg          1/1     Terminating         0          5d21h
example-registry-quay-redis-7cc5f6c977-956g8           1/1     Running             0          5d21h</pre>
									</p></div></li><li class="listitem"><p class="simpara">
									After your Red Hat Quay registry has reconfigured, enter the following command to check if the Red Hat Quay app pods are running:
								</p><pre class="programlisting language-terminal">$ oc get pods -n quay-enterprise</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">example-registry-quay-app-6786987b99-sz6kb             1/1     Running            0          7m45s
example-registry-quay-app-6786987b99-vgg2v             1/1     Running            0          9m1s
example-registry-quay-app-upgrade-lswsn                0/1     Completed          0          6d1h
example-registry-quay-config-editor-77847fc4f5-nsbbv   1/1     Running            0          9m1s
example-registry-quay-database-66969cd859-n2ssm        1/1     Running            0          6d1h
example-registry-quay-mirror-758fc68ff7-5wxlp          1/1     Running            0          8m29s
example-registry-quay-mirror-758fc68ff7-lbl82          1/1     Running            0          8m29s
example-registry-quay-redis-7cc5f6c977-956g8           1/1     Running            0          5d21h</pre>
									</p></div></li><li class="listitem"><p class="simpara">
									In your browser, access the registry endpoint and validate that the certificate has been updated appropriately. For example:
								</p><pre class="programlisting language-terminal">Common Name (CN)	example-registry-quay-quay-enterprise.apps.docs.quayteam.org
Organisation (O)	DOCS
Organisational Unit (OU)	QUAY</pre></li></ol></div></section></section><section class="section" id="red-hat-quay-builders-ui"><div class="titlepage"><div><div><h4 class="title">9.3.2.3. Using the UI to create a build trigger</h4></div></div></div><p>
						Use the following procedure to use the UI to create a build trigger.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to your Red Hat Quay repository.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create New Repository</strong></span> and create a new registry, for example, <code class="literal">testrepo</code>.
							</li><li class="listitem"><p class="simpara">
								On the <span class="strong strong"><strong>Repositories</strong></span> page, click the <span class="strong strong"><strong>Builds</strong></span> tab on the navigation pane. Alternatively, use the corresponding URL directly:
							</p><pre class="screen">https://example-registry-quay-quay-enterprise.apps.docs.quayteam.org/repository/quayadmin/testrepo?tab=builds</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									In some cases, the builder might have issues resolving hostnames. This issue might be related to the <code class="literal">dnsPolicy</code> being set to <code class="literal">default</code> on the job object. Currently, there is no workaround for this issue. It will be resolved in a future version of Red Hat Quay.
								</p></div></div></li><li class="listitem">
								Click <span class="strong strong"><strong>Create Build Trigger</strong></span> → <span class="strong strong"><strong>Custom Git Repository Push</strong></span>.
							</li><li class="listitem"><p class="simpara">
								Enter the HTTPS or SSH style URL used to clone your Git repository, then click <span class="strong strong"><strong>Continue</strong></span>. For example:
							</p><pre class="screen">https://github.com/gabriel-rh/actions_test.git</pre></li><li class="listitem">
								Check <span class="strong strong"><strong>Tag manifest with the branch or tag name</strong></span> and then click <span class="strong strong"><strong>Continue</strong></span>.
							</li><li class="listitem">
								Enter the location of the Dockerfile to build when the trigger is invoked, for example, <code class="literal">/Dockerfile</code> and click <span class="strong strong"><strong>Continue</strong></span>.
							</li><li class="listitem">
								Enter the location of the context for the Docker build, for example, <code class="literal">/</code>, and click <span class="strong strong"><strong>Continue</strong></span>.
							</li><li class="listitem">
								If warranted, create a Robot Account. Otherwise, click <span class="strong strong"><strong>Continue</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Continue</strong></span> to verify the parameters.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Builds</strong></span> page, click <span class="strong strong"><strong>Options</strong></span> icon of your Trigger Name, and then click <span class="strong strong"><strong>Run Trigger Now</strong></span>.
							</li><li class="listitem">
								Enter a commit SHA from the Git repository and click <span class="strong strong"><strong>Start Build</strong></span>.
							</li><li class="listitem"><p class="simpara">
								You can check the status of your build by clicking the commit in the <span class="strong strong"><strong>Build History</strong></span> page, or by running <code class="literal">oc get pods -n virtual-builders</code>. For example:
							</p><pre class="screen">$ oc get pods -n virtual-builders</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">NAME                                               READY   STATUS    RESTARTS   AGE
f192fe4a-c802-4275-bcce-d2031e635126-9l2b5-25lg2   1/1     Running   0          7s</pre>
								</p></div><pre class="programlisting language-terminal">$ oc get pods -n virtual-builders</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">NAME                                               READY   STATUS        RESTARTS   AGE
f192fe4a-c802-4275-bcce-d2031e635126-9l2b5-25lg2   1/1     Terminating   0          9s</pre>
								</p></div><pre class="screen">$ oc get pods -n virtual-builders</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">No resources found in virtual-builders namespace.</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								When the build is finished, you can check the status of the tag under <span class="strong strong"><strong>Tags</strong></span> on the navigation pane.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									With early access, full build logs and timestamps of builds are currently unavailable.
								</p></div></div></li></ol></div></section><section class="section" id="red-hat-quay-s3-bucket-modify"><div class="titlepage"><div><div><h4 class="title">9.3.2.4. Modifying your AWS S3 storage bucket</h4></div></div></div><p>
						If you are using AWS S3 storage, you must change your storage bucket in the AWS console, prior to running builders.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to your AWS console at <a class="link" href="https://s3.console.aws.amazon.com">s3.console.aws.com</a>.
							</li><li class="listitem">
								In the search bar, search for <code class="literal">S3</code> and then click <span class="strong strong"><strong>S3</strong></span>.
							</li><li class="listitem">
								Click the name of your bucket, for example, <code class="literal">myawsbucket</code>.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Permissions</strong></span> tab.
							</li><li class="listitem"><p class="simpara">
								Under <span class="strong strong"><strong>Cross-origin resource sharing (CORS)</strong></span>, include the following parameters:
							</p><pre class="programlisting language-yaml">  [
      {
          "AllowedHeaders": [
              "Authorization"
          ],
          "AllowedMethods": [
              "GET"
          ],
          "AllowedOrigins": [
              "*"
          ],
          "ExposeHeaders": [],
          "MaxAgeSeconds": 3000
      },
      {
          "AllowedHeaders": [
              "Content-Type",
              "x-amz-acl",
              "origin"
          ],
          "AllowedMethods": [
              "PUT"
          ],
          "AllowedOrigins": [
              "*"
          ],
          "ExposeHeaders": [],
          "MaxAgeSeconds": 3000
      }
  ]</pre></li></ol></div></section></section></section></section><section class="chapter" id="georepl-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Geo-replication</h1></div></div></div><p>
			Geo-replication allows multiple, geographically distributed Red Hat Quay deployments to work as a single registry from the perspective of a client or user. It significantly improves push and pull performance in a globally-distributed Red Hat Quay setup. Image data is asynchronously replicated in the background with transparent failover and redirect for clients.
		</p><p>
			Deployments of Red Hat Quay with geo-replication is supported on standalone and Operator deployments.
		</p><section class="section" id="arch-georpl-features"><div class="titlepage"><div><div><h2 class="title">10.1. Geo-replication features</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						When geo-replication is configured, container image pushes will be written to the preferred storage engine for that Red Hat Quay instance. This is typically the nearest storage backend within the region.
					</li><li class="listitem">
						After the initial push, image data will be replicated in the background to other storage engines.
					</li><li class="listitem">
						The list of replication locations is configurable and those can be different storage backends.
					</li><li class="listitem">
						An image pull will always use the closest available storage engine, to maximize pull performance.
					</li><li class="listitem">
						If replication has not been completed yet, the pull will use the source storage backend instead.
					</li></ul></div></section><section class="section" id="arch-georepl-prereqs"><div class="titlepage"><div><div><h2 class="title">10.2. Geo-replication requirements and constraints</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						In geo-replicated setups, Red Hat Quay requires that all regions are able to read and write to all other region’s object storage. Object storage must be geographically accessible by all other regions.
					</li><li class="listitem">
						In case of an object storage system failure of one geo-replicating site, that site’s Red Hat Quay deployment must be shut down so that clients are redirected to the remaining site with intact storage systems by a global load balancer. Otherwise, clients will experience pull and push failures.
					</li><li class="listitem">
						Red Hat Quay has no internal awareness of the health or availability of the connected object storage system. If the object storage system of one site becomes unavailable, there will be no automatic redirect to the remaining storage system, or systems, of the remaining site, or sites.
					</li><li class="listitem">
						Geo-replication is asynchronous. The permanent loss of a site incurs the loss of the data that has been saved in that sites' object storage system but has not yet been replicated to the remaining sites at the time of failure.
					</li><li class="listitem"><p class="simpara">
						A single database, and therefore all metadata and Red Hat Quay configuration, is shared across all regions.
					</p><p class="simpara">
						Geo-replication does not replicate the database. In the event of an outage, Red Hat Quay with geo-replication enabled will not failover to another database.
					</p></li><li class="listitem">
						A single Redis cache is shared across the entire Red Hat Quay setup and needs to accessible by all Red Hat Quay pods.
					</li><li class="listitem">
						The exact same configuration should be used across all regions, with exception of the storage backend, which can be configured explicitly using the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environment variable.
					</li><li class="listitem">
						Geo-replication requires object storage in each region. It does not work with local storage.
					</li><li class="listitem">
						Each region must be able to access every storage engine in each region, which requires a network path.
					</li><li class="listitem">
						Alternatively, the storage proxy option can be used.
					</li><li class="listitem">
						The entire storage backend, for example, all blobs, is replicated. Repository mirroring, by contrast, can be limited to a repository, or an image.
					</li><li class="listitem">
						All Red Hat Quay instances must share the same entrypoint, typically through a load balancer.
					</li><li class="listitem">
						All Red Hat Quay instances must have the same set of superusers, as they are defined inside the common configuration file.
					</li><li class="listitem">
						Geo-replication requires your Clair configuration to be set to <code class="literal">unmanaged</code>. An unmanaged Clair database allows the Red Hat Quay Operator to work in a geo-replicated environment, where multiple instances of the Red Hat Quay Operator must communicate with the same database. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#clair-unmanaged">Advanced Clair configuration</a>.
					</li><li class="listitem">
						Geo-Replication requires SSL/TLS certificates and keys. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/index#using_ssl_to_protect_connections_to_red_hat_quay">Using SSL/TLS to protect connections to Red Hat Quay</a>.
					</li></ul></div><p>
				If the above requirements cannot be met, you should instead use two or more distinct Red Hat Quay deployments and take advantage of repository mirroring functions.
			</p></section><section class="section" id="georepl-arch-operator"><div class="titlepage"><div><div><h2 class="title">10.3. Geo-replication using the Red Hat Quay Operator</h2></div></div></div><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_georeplication_openshift-temp.png" alt="Geo-replication architecture"/></span>
			</p><p>
				In the example shown above, the Red Hat Quay Operator is deployed in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines.
			</p><p>
				Because the Operator now manages the Clair security scanner and its database separately, geo-replication setups can be leveraged so that they do not manage the Clair database. Instead, an external shared database would be used. Red Hat Quay and Clair support several providers and vendors of PostgreSQL, which can be found in the Red Hat Quay 3.x <a class="link" href="https://access.redhat.com/articles/4067991">test matrix</a>. Additionally, the Operator also supports custom Clair configurations that can be injected into the deployment, which allows users to configure Clair with the connection credentials for the external database.
			</p><section class="section" id="georepl-deploy-operator"><div class="titlepage"><div><div><h3 class="title">10.3.1. Setting up geo-replication on OpenShift Container Platform</h3></div></div></div><p>
					Use the following procedure to set up geo-replication on OpenShift Container Platform.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Deploy a postgres instance for Red Hat Quay.
						</li><li class="listitem"><p class="simpara">
							Login to the database by entering the following command:
						</p><pre class="programlisting language-terminal">psql -U &lt;username&gt; -h &lt;hostname&gt; -p &lt;port&gt; -d &lt;database_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Create a database for Red Hat Quay named <code class="literal">quay</code>. For example:
						</p><pre class="programlisting language-terminal">CREATE DATABASE quay;</pre></li><li class="listitem"><p class="simpara">
							Enable pg_trm extension inside the database
						</p><pre class="programlisting language-terminal">\c quay;
CREATE EXTENSION IF NOT EXISTS pg_trgm;</pre></li><li class="listitem"><p class="simpara">
							Deploy a Redis instance:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Deploying a Redis instance might be unnecessary if your cloud provider has its own service.
									</li><li class="listitem">
										Deploying a Redis instance is required if you are leveraging Builders.
									</li></ul></div></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Deploy a VM for Redis
								</li><li class="listitem">
									Verify that it is accessible from the clusters where Red Hat Quay is running
								</li><li class="listitem">
									Port 6379/TCP must be open
								</li><li class="listitem"><p class="simpara">
									Run Redis inside the instance
								</p><pre class="programlisting language-terminal">sudo dnf install -y podman
podman run -d --name redis -p 6379:6379 redis</pre></li></ol></div></li><li class="listitem">
							Create two object storage backends, one for each cluster. Ideally, one object storage bucket will be close to the first, or primary, cluster, and the other will run closer to the second, or secondary, cluster.
						</li><li class="listitem">
							Deploy the clusters with the same config bundle, using environment variable overrides to select the appropriate storage backend for an individual cluster.
						</li><li class="listitem">
							Configure a load balancer to provide a single entry point to the clusters.
						</li></ol></div><section class="section" id="configuring-geo-repl"><div class="titlepage"><div><div><h4 class="title">10.3.1.1. Configuring geo-replication for the Red Hat Quay Operator on OpenShift Container Platform</h4></div></div></div><p>
						Use the following procedure to configure geo-replication for the Red Hat Quay Operator.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">config.yaml</code> file that is shared between clusters. This <code class="literal">config.yaml</code> file contains the details for the common PostgreSQL, Redis and storage backends:
							</p><div class="formalpara"><p class="title"><strong>Geo-replication <code class="literal">config.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">SERVER_HOSTNAME: &lt;georep.quayteam.org or any other name&gt; <span id="CO9-1"/><span class="callout">1</span>
DB_CONNECTION_ARGS:
  autorollback: true
  threadlocals: true
DB_URI: postgresql://postgres:password@10.19.0.1:5432/quay <span id="CO9-2"/><span class="callout">2</span>
BUILDLOGS_REDIS:
  host: 10.19.0.2
  port: 6379
USER_EVENTS_REDIS:
  host: 10.19.0.2
  port: 6379
DISTRIBUTED_STORAGE_CONFIG:
  usstorage:
    - GoogleCloudStorage
    - access_key: GOOGQGPGVMASAAMQABCDEFG
      bucket_name: georep-test-bucket-0
      secret_key: AYWfEaxX/u84XRA2vUX5C987654321
      storage_path: /quaygcp
  eustorage:
    - GoogleCloudStorage
    - access_key: GOOGQGPGVMASAAMQWERTYUIOP
      bucket_name: georep-test-bucket-1
      secret_key: AYWfEaxX/u84XRA2vUX5Cuj12345678
      storage_path: /quaygcp
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS:
  - usstorage
  - eustorage
DISTRIBUTED_STORAGE_PREFERENCE:
  - usstorage
  - eustorage
FEATURE_STORAGE_REPLICATION: true</pre>
								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										A proper <code class="literal">SERVER_HOSTNAME</code> must be used for the route and must match the hostname of the global load balancer.
									</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										To retrieve the configuration file for a Clair instance deployed using the OpenShift Container Platform Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/quay_operator_features#clair-openshift-config">Retrieving the Clair config</a>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">configBundleSecret</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml georep-config-bundle</pre></li><li class="listitem"><p class="simpara">
								In each of the clusters, set the <code class="literal">configBundleSecret</code> and use the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environmental variable override to configure the appropriate storage for that cluster. For example:
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">config.yaml</code> file between both deployments must match. If making a change to one cluster, it must also be changed in the other.
								</p></div></div><div class="formalpara"><p class="title"><strong>US cluster <code class="literal">QuayRegistry</code> example</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: georep-config-bundle
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: postgres
      managed: false
    - kind: clairpostgres
      managed: false
    - kind: redis
      managed: false
    - kind: quay
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: usstorage
    - kind: mirror
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: usstorage</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Because SSL/TLS is unmanaged, and the route is managed, you must supply the certificates with either with the config tool or directly in the config bundle. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-preconfigure#operator-preconfig-tls-routes">Configuring TLS and routes</a>.
								</p></div></div><div class="formalpara"><p class="title"><strong>European cluster</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: georep-config-bundle
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: postgres
      managed: false
    - kind: clairpostgres
      managed: false
    - kind: redis
      managed: false
    - kind: quay
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: eustorage
    - kind: mirror
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: eustorage</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Because SSL/TLS is unmanaged, and the route is managed, you must supply the certificates with either with the config tool or directly in the config bundle. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-preconfigure#operator-preconfig-tls-routes">Configuring TLS and routes</a>.
								</p></div></div></li></ol></div></section></section><section class="section" id="georepl-mixed-storage"><div class="titlepage"><div><div><h3 class="title">10.3.2. Mixed storage for geo-replication</h3></div></div></div><p>
					Red Hat Quay geo-replication supports the use of different and multiple replication targets, for example, using AWS S3 storage on public cloud and using Ceph storage on premise. This complicates the key requirement of granting access to all storage backends from all Red Hat Quay pods and cluster nodes. As a result, it is recommended that you use the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A VPN to prevent visibility of the internal storage, <span class="emphasis"><em>or</em></span>
						</li><li class="listitem">
							A token pair that only allows access to the specified bucket used by Red Hat Quay
						</li></ul></div><p>
					This results in the public cloud instance of Red Hat Quay having access to on-premise storage, but the network will be encrypted, protected, and will use ACLs, thereby meeting security requirements.
				</p><p>
					If you cannot implement these security measures, it might be preferable to deploy two distinct Red Hat Quay registries and to use repository mirroring as an alternative to geo-replication.
				</p></section></section><section class="section" id="upgrading-geo-repl-quay-operator"><div class="titlepage"><div><div><h2 class="title">10.4. Upgrading a geo-replication deployment of the Red Hat Quay Operator</h2></div></div></div><p>
				Use the following procedure to upgrade your geo-replicated Red Hat Quay Operator.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When upgrading geo-replicated Red Hat Quay Operator deployments to the next y-stream release (for example, Red Hat Quay 3.7 → Red Hat Quay 3.8), you must stop operations before upgrading.
						</li><li class="listitem">
							There is intermittent downtime down upgrading from one y-stream release to the next.
						</li><li class="listitem">
							It is highly recommended to back up your Red Hat Quay Operator deployment before upgrading.
						</li></ul></div></div></div><div class="admonition note"><div class="admonition_header">Procedure</div><div><p>
					This procedure assumes that you are running the Red Hat Quay Operator on three (or more) systems. For this procedure, we will assume three systems named <code class="literal">System A,</code> <code class="literal">System B,</code> and <code class="literal">System C</code>. <code class="literal">System A</code> will serve as the primary system in which the Red Hat Quay Operator is deployed.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On System B and System C, scale down your Red Hat Quay Operator deployment. This is done by disabling auto scaling and overriding the replica county for Red Hat Quay, mirror workers, and Clair (if it is managed). Use the following <code class="literal">quayregistry.yaml</code> file as a reference:
					</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <span id="CO10-1"/><span class="callout">1</span>
    - kind: quay
      managed: true
      overrides: <span id="CO10-2"/><span class="callout">2</span>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Disable auto scaling of Quay, Clair and Mirroring workers
							</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Set the replica count to 0 for components accessing the database and objectstorage
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must keep the Red Hat Quay Operator running on System A. Do not update the <code class="literal">quayregistry.yaml</code> file on System A.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Wait for the <code class="literal">registry-quay-app</code>, <code class="literal">registry-quay-mirror</code>, and <code class="literal">registry-clair-app</code> pods to disappear. Enter the following command to check their status:
					</p><pre class="programlisting language-terminal">oc get pods -n &lt;quay-namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">quay-operator.v3.7.1-6f9d859bd-p5ftc               1/1     Running     0             12m
quayregistry-clair-postgres-7487f5bd86-xnxpr       1/1     Running     1 (12m ago)   12m
quayregistry-quay-app-upgrade-xq2v6                0/1     Completed   0             12m
quayregistry-quay-config-editor-6dfdcfc44f-hlvwm   1/1     Running     0             73s
quayregistry-quay-redis-84f888776f-hhgms           1/1     Running     0             12m</pre>
						</p></div></li><li class="listitem">
						On System A, initiate a Red Hat Quay Operator upgrade to the latest y-stream version. This is a manual process. For more information about upgrading installed Operators, see <a class="link" href="https://docs.openshift.com/container-platform/4.12/operators/admin/olm-upgrading-operators.html">Upgrading installed Operators</a>. For more information about Red Hat Quay upgrade paths, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-upgrade#upgrading_the_quay_operator">Upgrading the Red Hat Quay Operator</a>.
					</li><li class="listitem">
						After the new Red Hat Quay Operator is installed, the necessary upgrades on the cluster are automatically completed. Afterwards, new Red Hat Quay pods are started with the latest y-stream version. Additionally, new <code class="literal">Quay</code> pods are scheduled and started.
					</li><li class="listitem"><p class="simpara">
						Confirm that the update has properly worked by navigating to the Red Hat Quay UI:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								In the <span class="strong strong"><strong>OpenShift</strong></span> console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>, and click the <span class="strong strong"><strong>Registry Endpoint</strong></span> link.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Do not execute the following step until the Red Hat Quay UI is available. Do not upgrade the Red Hat Quay Operator on System B and on System C until the UI is available on System A.
								</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
						After confirming that the update has properly worked on System A, initiate the Red Hat Quay Operator on System B and on System C. The Operator upgrade results in an upgraded Red Hat Quay installation, and the pods are restarted.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Because the database schema is correct for the new y-stream installation, the new pods on System B and on System C should quickly start.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="backing-up-and-restoring-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Backing up and restoring Red Hat Quay managed by the Red Hat Quay Operator</h1></div></div></div><p>
			Use the content within this section to back up and restore Red Hat Quay when managed by the Red Hat Quay Operator on OpenShift Container Platform
		</p><section class="section" id="backing-up-red-hat-quay-operator"><div class="titlepage"><div><div><h2 class="title">11.1. Backing up Red Hat Quay</h2></div></div></div><p>
				This procedure describes how to create a backup of Red Hat Quay deployed on OpenShift Container Platform using the Red Hat Quay Operator
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A healthy Red Hat Quay deployment on OpenShift Container Platform using the Red Hat Quay Operator. The status condition <code class="literal">Available</code> is set to <code class="literal">true</code>.
					</li><li class="listitem">
						The components <code class="literal">quay</code>, <code class="literal">postgres</code> and <code class="literal">objectstorage</code> are set to <code class="literal">managed: true</code>
					</li><li class="listitem">
						If the component <code class="literal">clair</code> is set to <code class="literal">managed: true</code> the component <code class="literal">clairpostgres</code> is also set to <code class="literal">managed: true</code> (starting with Red Hat Quay Operator v3.7 or later)
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If your deployment contains partially unmanaged database or storage components and you are using external services for PostgreSQL or S3-compatible object storage to run your Red Hat Quay deployment, you must refer to the service provider or vendor documentation to create a backup of the data. You can refer to the tools described in this guide as a starting point on how to backup your external PostgreSQL database or object storage.
				</p></div></div><section class="section" id="quay-configuration-backup"><div class="titlepage"><div><div><h3 class="title">11.1.1. Red Hat Quay configuration backup</h3></div></div></div><p>
					Use the following procedure to back up your Red Hat Quay configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To back the <code class="literal">QuayRegistry</code> custom resource by exporting it, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get quayregistry &lt;quay-registry-name&gt; -n &lt;quay-namespace&gt; -o yaml &gt; quay-registry.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the resulting <code class="literal">quayregistry.yaml</code> and remove the status section and the following metadata fields:
						</p><pre class="programlisting language-yaml">  metadata.creationTimestamp
  metadata.finalizers
  metadata.generation
  metadata.resourceVersion
  metadata.uid</pre></li><li class="listitem"><p class="simpara">
							Backup the managed keys secret by entering the following command:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you are running a version older than Red Hat Quay 3.7.0, this step can be skipped. Some secrets are automatically generated while deploying Red Hat Quay for the first time. These are stored in a secret called <code class="literal">&lt;quay-registry-name&gt;-quay-registry-managed-secret-keys</code> in the namespace of the <code class="literal">QuayRegistry</code> resource.
							</p></div></div><pre class="programlisting language-terminal">$ oc get secret -n &lt;quay-namespace&gt; &lt;quay-registry-name&gt;-quay-registry-managed-secret-keys -o yaml &gt; managed-secret-keys.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the resulting <code class="literal">managed-secret-keys.yaml</code> file and remove the entry <code class="literal">metadata.ownerReferences</code>. Your <code class="literal">managed-secret-keys.yaml</code> file should look similar to the following:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: &lt;quayname&gt;-quay-registry-managed-secret-keys
  namespace: &lt;quay-namespace&gt;
data:
  CONFIG_EDITOR_PW: &lt;redacted&gt;
  DATABASE_SECRET_KEY: &lt;redacted&gt;
  DB_ROOT_PW: &lt;redacted&gt;
  DB_URI: &lt;redacted&gt;
  SECRET_KEY: &lt;redacted&gt;
  SECURITY_SCANNER_V4_PSK: &lt;redacted&gt;</pre><p class="simpara">
							All information under the <code class="literal">data</code> property should remain the same.
						</p></li><li class="listitem"><p class="simpara">
							Redirect the current <code class="literal">Quay</code> configuration file by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret -n &lt;quay-namespace&gt;  $(oc get quayregistry &lt;quay-registry-name&gt; -n &lt;quay-namespace&gt;  -o jsonpath='{.spec.configBundleSecret}') -o yaml &gt; config-bundle.yaml</pre></li><li class="listitem"><p class="simpara">
							Backup the <code class="literal">/conf/stack/config.yaml</code> file mounted inside of the <code class="literal">Quay</code> pods:
						</p><pre class="programlisting language-terminal">$ oc exec -it quay-pod-name -- cat /conf/stack/config.yaml &gt; quay-config.yaml</pre></li></ol></div></section><section class="section" id="scaling-down-quay-deployment"><div class="titlepage"><div><div><h3 class="title">11.1.2. Scaling down your Red Hat Quay deployment</h3></div></div></div><p>
					Use the following procedure to scale down your Red Hat Quay deployment.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This step is needed to create a consistent backup of the state of your Red Hat Quay deployment. Do not omit this step, including in setups where PostgreSQL databases and/or S3-compatible object storage are provided by external services (unmanaged by the Red Hat Quay Operator).
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale down your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale down the Red Hat Quay deployment by disabling auto scaling and overriding the replica count for Red Hat Quay, mirror workers, and Clair (if managed). Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <span id="CO11-1"/><span class="callout">1</span>
    - kind: quay
      managed: true
      overrides: <span id="CO11-2"/><span class="callout">2</span>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Disable auto scaling of Quay, Clair and Mirroring workers
										</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Set the replica count to 0 for components accessing the database and objectstorage
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier</strong></span>: Scale down the Red Hat Quay deployment by scaling down the Red Hat Quay Operator first and then the managed Red Hat Quay resources:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt;|awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-app/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-mirror/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/clair-app/ {print $1}') -n &lt;quay-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Wait for the <code class="literal">registry-quay-app</code>, <code class="literal">registry-quay-mirror</code> and <code class="literal">registry-clair-app</code> pods (depending on which components you set to be managed by the Red Hat Quay Operator) to disappear. You can check their status by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">quay-operator.v3.7.1-6f9d859bd-p5ftc               1/1     Running     0             12m
quayregistry-clair-postgres-7487f5bd86-xnxpr       1/1     Running     1 (12m ago)   12m
quayregistry-quay-app-upgrade-xq2v6                0/1     Completed   0             12m
quayregistry-quay-config-editor-6dfdcfc44f-hlvwm   1/1     Running     0             73s
quayregistry-quay-database-859d5445ff-cqthr        1/1     Running     0             12m
quayregistry-quay-redis-84f888776f-hhgms           1/1     Running     0             12m</pre>
							</p></div></li></ol></div></section><section class="section" id="backing-up-managed-database"><div class="titlepage"><div><div><h3 class="title">11.1.3. Backing up the Red Hat Quay managed database</h3></div></div></div><p>
					Use the following procedure to back up the Red Hat Quay managed database.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your Red Hat Quay deployment is configured with external, or unmanged, PostgreSQL database(s), refer to your vendor’s documentation on how to create a consistent backup of these databases.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Identify the Quay PostgreSQL pod name:
						</p><pre class="programlisting language-terminal">$ oc get pod -l quay-component=postgres -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].metadata.name}'</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">quayregistry-quay-database-59f54bb7-58xs7</pre></li><li class="listitem"><p class="simpara">
							Obtain the Quay database name:
						</p><pre class="programlisting language-terminal">$ oc -n &lt;quay-namespace&gt; rsh $(oc get pod -l app=quay -o NAME -n &lt;quay-namespace&gt; |head -n 1) cat /conf/stack/config.yaml|awk -F"/" '/^DB_URI/ {print $4}'
quayregistry-quay-database</pre></li><li class="listitem"><p class="simpara">
							Download a backup database:
						</p><pre class="programlisting language-terminal">$ oc exec quayregistry-quay-database-59f54bb7-58xs7 -- /usr/bin/pg_dump -C quayregistry-quay-database  &gt; backup.sql</pre></li></ol></div><section class="section" id="backing-up-managed-object-storage"><div class="titlepage"><div><div><h4 class="title">11.1.3.1. Backing up the Red Hat Quay managed object storage</h4></div></div></div><p>
						Use the following procedure to back up the Red Hat Quay managed object storage. The instructions in this section apply to the following configurations:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Standalone, multi-cloud object gateway configurations
							</li><li class="listitem">
								OpenShift Data Foundations storage requires that the Red Hat Quay Operator provisioned an S3 object storage bucket from, through the ObjectStorageBucketClaim API
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If your Red Hat Quay deployment is configured with external (unmanged) object storage, refer to your vendor’s documentation on how to create a copy of the content of Quay’s storage bucket.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Decode and export the <code class="literal">AWS_ACCESS_KEY_ID</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt;  -o jsonpath='{.items[0].data.AWS_ACCESS_KEY_ID}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
								Decode and export the <code class="literal">AWS_SECRET_ACCESS_KEY_ID</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.AWS_SECRET_ACCESS_KEY}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
								Create a new directory:
							</p><pre class="programlisting language-terminal">$ mkdir blobs</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can also use <a class="link" href="https://rclone.org/">rclone</a> or <a class="link" href="https://s3tools.org/s3cmd">sc3md</a> instead of the AWS command line utility.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Copy all blobs to the directory by entering the following command:
							</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint https://$(oc get route s3 -n openshift-storage  -o jsonpath='{.spec.host}')  s3://$(oc get cm -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.BUCKET_NAME}') ./blobs</pre></li></ol></div></section></section><section class="section" id="scaling-up-quay-deployment"><div class="titlepage"><div><div><h3 class="title">11.1.4. Scale the Red Hat Quay deployment back up</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale up your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale up the Red Hat Quay deployment by re-enabling auto scaling, if desired, and removing the replica overrides for Quay, mirror workers and Clair as applicable. Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: true <span id="CO12-1"/><span class="callout">1</span>
    - kind: quay <span id="CO12-2"/><span class="callout">2</span>
      managed: true
    - kind: clair
      managed: true
    - kind: mirror
      managed: true
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Re-enables auto scaling of Quay, Clair and Mirroring workers again (if desired)
										</div></dd><dt><a href="#CO12-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replica overrides are removed again to scale the Quay components back up
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale up the Red Hat Quay deployment by scaling up the Red Hat Quay Operator again:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=1 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt; | awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: registry
  namespace: &lt;quay-namespace&gt;
  ...
spec:
  ...
status:
  - lastTransitionTime: '2022-06-20T05:31:17Z'
    lastUpdateTime: '2022-06-20T17:31:13Z'
    message: All components reporting as healthy
    reason: HealthChecksPassing
    status: 'True'
    type: Available</pre></li></ol></div></section></section><section class="section" id="restoring-up-red-hat-quay"><div class="titlepage"><div><div><h2 class="title">11.2. Restoring Red Hat Quay</h2></div></div></div><p>
				Use the following procedures to restore Red Hat Quay when the Red Hat Quay Operator manages the database. It should be performed after a backup of your Red Hat Quay registry has been performed. See <a class="link" href="#backing-up-red-hat-quay-operator" title="11.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> for more information.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Red Hat Quay is deployed on OpenShift Container Platform using the Red Hat Quay Operator.
					</li><li class="listitem">
						A backup of the Red Hat Quay configuration managed by the Red Hat Quay Operator has been created following the instructions in the <a class="link" href="#backing-up-red-hat-quay-operator" title="11.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> section
					</li><li class="listitem">
						Your Red Hat Quay database has been backed up.
					</li><li class="listitem">
						The object storage bucket used by Red Hat Quay has been backed up.
					</li><li class="listitem">
						The components <code class="literal">quay</code>, <code class="literal">postgres</code> and <code class="literal">objectstorage</code> are set to <code class="literal">managed: true</code>
					</li><li class="listitem">
						If the component <code class="literal">clair</code> is set to <code class="literal">managed: true</code>, the component <code class="literal">clairpostgres</code> is also set to <code class="literal">managed: true</code> (starting with Red Hat Quay Operator v3.7 or later)
					</li><li class="listitem">
						There is no running Red Hat Quay deployment managed by the Red Hat Quay Operator in the target namespace on your OpenShift Container Platform cluster
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If your deployment contains partially unmanaged database or storage components and you are using external services for PostgreSQL or S3-compatible object storage to run your Red Hat Quay deployment, you must refer to the service provider or vendor documentation to restore their data from a backup prior to restore Red Hat Quay
				</p></div></div><section class="section" id="restoring-quay-and-configuration-from-backup"><div class="titlepage"><div><div><h3 class="title">11.2.1. Restoring Red Hat Quay and its configuration from a backup</h3></div></div></div><p>
					Use the following procedure to restore Red Hat Quay and its configuration files from a backup.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						These instructions assume you have followed the process in the <a class="link" href="#backing-up-red-hat-quay-operator" title="11.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> guide and create the backup files with the same names.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Restore the backed up Red Hat Quay configuration by entering the following command:
						</li></ol></div><pre class="screen">$ oc create -f ./config-bundle.yaml</pre><p>
					+
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you receive the error <code class="literal">Error from server (AlreadyExists): error when creating "./config-bundle.yaml": secrets "config-bundle-secret" already exists</code>, you must delete your existing resource with <code class="literal">$ oc delete Secret config-bundle-secret -n &lt;quay-namespace&gt;</code> and recreate it with <code class="literal">$ oc create -f ./config-bundle.yaml</code>.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Restore the generated keys from the backup by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f ./managed-secret-keys.yaml</pre></li><li class="listitem"><p class="simpara">
							Restore the <code class="literal">QuayRegistry</code> custom resource:
						</p><pre class="programlisting language-terminal">$ oc create -f ./quay-registry.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment and wait for it to be available:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay-namespace&gt;</pre></li></ol></div></section><section class="section" id="scale-down-quay-deployment"><div class="titlepage"><div><div><h3 class="title">11.2.2. Scaling down your Red Hat Quay deployment</h3></div></div></div><p>
					Use the following procedure to scale down your Red Hat Quay deployment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale down your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale down the Red Hat Quay deployment by disabling auto scaling and overriding the replica count for Quay, mirror workers and Clair (if managed). Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <span id="CO13-1"/><span class="callout">1</span>
    - kind: quay
      managed: true
      overrides: <span id="CO13-2"/><span class="callout">2</span>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Disable auto scaling of Quay, Clair and Mirroring workers
										</div></dd><dt><a href="#CO13-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Set the replica count to 0 for components accessing the database and objectstorage
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale down the Red Hat Quay deployment by scaling down the Red Hat Quay Operator first and then the managed Red Hat Quay resources:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt;|awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-app/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-mirror/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/clair-app/ {print $1}') -n &lt;quay-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Wait for the <code class="literal">registry-quay-app</code>, <code class="literal">registry-quay-mirror</code> and <code class="literal">registry-clair-app</code> pods (depending on which components you set to be managed by Red Hat Quay Operator) to disappear. You can check their status by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">registry-quay-config-editor-77847fc4f5-nsbbv   1/1     Running            0          9m1s
registry-quay-database-66969cd859-n2ssm        1/1     Running            0          6d1h
registry-quay-redis-7cc5f6c977-956g8           1/1     Running            0          5d21h</pre></li></ol></div></section><section class="section" id="restoring-quay-database"><div class="titlepage"><div><div><h3 class="title">11.2.3. Restoring your Red Hat Quay database</h3></div></div></div><p>
					Use the following procedure to restore your Red Hat Quay database.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Identify your <code class="literal">Quay</code> database pod by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod -l quay-component=postgres -n  &lt;quay-namespace&gt; -o jsonpath='{.items[0].metadata.name}'</pre><p class="simpara">
							Example output:
						</p><pre class="screen">quayregistry-quay-database-59f54bb7-58xs7</pre></li><li class="listitem"><p class="simpara">
							Upload the backup by copying it from the local environment and into the pod:
						</p><pre class="screen">$ oc cp ./backup.sql -n &lt;quay-namespace&gt; registry-quay-database-66969cd859-n2ssm:/tmp/backup.sql</pre></li><li class="listitem"><p class="simpara">
							Open a remote terminal to the database by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc rsh -n &lt;quay-namespace&gt; registry-quay-database-66969cd859-n2ssm</pre></li><li class="listitem"><p class="simpara">
							Enter psql by running the following command:
						</p><pre class="programlisting language-terminal">bash-4.4$ psql</pre></li><li class="listitem"><p class="simpara">
							You can list the database by running the following command:
						</p><pre class="screen">postgres=# \l</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">                                                  List of databases
           Name            |           Owner            | Encoding |  Collate   |   Ctype    |   Access privileges
----------------------------+----------------------------+----------+------------+------------+-----------------------
postgres                   | postgres                   | UTF8     | en_US.utf8 | en_US.utf8 |
quayregistry-quay-database | quayregistry-quay-database | UTF8     | en_US.utf8 | en_US.utf8 |</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Drop the database by entering the following command:
						</p><pre class="programlisting language-terminal">postgres=# DROP DATABASE "quayregistry-quay-database";</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">DROP DATABASE</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Exit the postgres CLI to re-enter bash-4.4:
						</p><pre class="programlisting language-terminal">\q</pre></li><li class="listitem"><p class="simpara">
							Redirect your PostgreSQL database to your backup database:
						</p><pre class="programlisting language-terminal">sh-4.4$ psql &lt; /tmp/backup.sql</pre></li><li class="listitem"><p class="simpara">
							Exit bash by entering the following command:
						</p><pre class="programlisting language-terminal">sh-4.4$ exit</pre></li></ol></div></section><section class="section" id="restoring-quay-object-storage-data"><div class="titlepage"><div><div><h3 class="title">11.2.4. Restore your Red Hat Quay object storage data</h3></div></div></div><p>
					Use the following procedure to restore your Red Hat Quay object storage data.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Export the <code class="literal">AWS_ACCESS_KEY_ID</code> by entering the following command:
						</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt;  -o jsonpath='{.items[0].data.AWS_ACCESS_KEY_ID}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Export the <code class="literal">AWS_SECRET_ACCESS_KEY</code> by entering the following command:
						</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.AWS_SECRET_ACCESS_KEY}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Upload all blobs to the bucket by running the following command:
						</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint https://$(oc get route s3 -n openshift-storage  -o jsonpath='{.spec.host}') ./blobs  s3://$(oc get cm -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.BUCKET_NAME}')</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use <a class="link" href="https://rclone.org/">rclone</a> or <a class="link" href="https://s3tools.org/s3cmd">sc3md</a> instead of the AWS command line utility.
					</p></div></div></section><section class="section" id="scaling-up-quay"><div class="titlepage"><div><div><h3 class="title">11.2.5. Scaling up your Red Hat Quay deployment</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale up your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale up the Red Hat Quay deployment by re-enabling auto scaling, if desired, and removing the replica overrides for Quay, mirror workers and Clair as applicable. Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: true <span id="CO14-1"/><span class="callout">1</span>
    - kind: quay <span id="CO14-2"/><span class="callout">2</span>
      managed: true
    - kind: clair
      managed: true
    - kind: mirror
      managed: true
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Re-enables auto scaling of Red Hat Quay, Clair and mirroring workers again (if desired)
										</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replica overrides are removed again to scale the Red Hat Quay components back up
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale up the Red Hat Quay deployment by scaling up the Red Hat Quay Operator again:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=1 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt; | awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: registry
  namespace: &lt;quay-namespace&gt;
  ...
spec:
  ...
status:
  - lastTransitionTime: '2022-06-20T05:31:17Z'
    lastUpdateTime: '2022-06-20T17:31:13Z'
    message: All components reporting as healthy
    reason: HealthChecksPassing
    status: 'True'
    type: Available</pre></li></ol></div></section></section></section><section class="chapter" id="operator-ipv6-dual-stack"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Deploying IPv6 on the Red Hat Quay Operator</h1></div></div></div><p>
			Your Red Hat Quay Operator deployment can now be served in locations that only support IPv6, such as Telco and Edge environments.
		</p><p>
			For a list of known limitations, see <a class="link" href="#operator-ipv6-limitations-38" title="12.2. IPv6 limitations">IPv6 limitations</a>
		</p><section class="section" id="proc-manage-enabling-ipv6"><div class="titlepage"><div><div><h2 class="title">12.1. Enabling the IPv6 protocol family</h2></div></div></div><p>
				Use the following procedure to enable IPv6 support on your standalone Red Hat Quay deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have updated Red Hat Quay to 3.8.
					</li><li class="listitem">
						Your host and container software platform (Docker, Podman) must be configured to support IPv6.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">FEATURE_LISTEN_IP_VERSION</code> parameter and set it to <code class="literal">IPv6</code>, for example:
					</p><pre class="programlisting language-yaml">---
FEATURE_GOOGLE_LOGIN: false
FEATURE_INVITE_ONLY_USER_CREATION: false
FEATURE_LISTEN_IP_VERSION: IPv6
FEATURE_MAILING: false
FEATURE_NONSUPERUSER_TEAM_SYNCING_SETUP: false
---</pre></li><li class="listitem">
						Start, or restart, your Red Hat Quay deployment.
					</li><li class="listitem"><p class="simpara">
						Check that your deployment is listening to IPv6 by entering the following command:
					</p><pre class="programlisting language-terminal">$ curl &lt;quay_endpoint&gt;/health/instance
{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</pre></li></ol></div><p>
				After enabling IPv6 in your deployment’s <code class="literal">config.yaml</code>, all Red Hat Quay features can be used as normal, so long as your environment is configured to use IPv6 and is not hindered by the <a class="link" href="#operator-ipv6-limitations-38" title="12.2. IPv6 limitations">IPv6 and dual-stack limitations</a>.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					If your environment is configured to IPv4, but the <code class="literal">FEATURE_LISTEN_IP_VERSION</code> configuration field is set to <code class="literal">IPv6</code>, Red Hat Quay will fail to deploy.
				</p></div></div></section><section class="section" id="operator-ipv6-limitations-38"><div class="titlepage"><div><div><h2 class="title">12.2. IPv6 limitations</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Currently, attempting to configure your Red Hat Quay deployment with the common Microsoft Azure Blob Storage configuration will not work on IPv6 single stack environments. Because the endpoint of Microsoft Azure Blob Storage does not support IPv6, there is no workaround in place for this issue.
					</p><p class="simpara">
						For more information, see <a class="link" href="https://issues.redhat.com/browse/PROJQUAY-4433">PROJQUAY-4433</a>.
					</p></li><li class="listitem"><p class="simpara">
						Currently, attempting to configure your Red Hat Quay deployment with Amazon S3 CloudFront will not work on IPv6 single stack environments. Because the endpoint of Amazon S3 CloudFront does not support IPv6, there is no workaround in place for this issue.
					</p><p class="simpara">
						For more information, see <a class="link" href="https://issues.redhat.com/browse/PROJQUAY-4470">PROJQUAY-4470</a>.
					</p></li><li class="listitem">
						Currently, Red Hat OpenShift Data Foundation is unsupported when Red Hat Quay is deployed on IPv6 single stack environments. As a result, Red Hat OpenShift Data Foundation cannot be used in IPv6 environments. This limitation is scheduled to be fixed in a future version of OpenShift Data Foundations.
					</li><li class="listitem">
						Currently, dual-stack (IPv4 and IPv6) support does not work on Red Hat Quay OpenShift Container Platform deployments. When Red Hat Quay 3.8 is deployed on OpenShift Container Platform with dual-stack support enabled, the Quay Route generated by the Red Hat Quay Operator only generates an IPv4 address, and not an IPv6 address. As a result, clients with an IPv6 address cannot access the Red Hat Quay application on OpenShift Container Platform. This limitation is scheduled to be fixed in a future version of OpenShift Container Platform.
					</li></ul></div></section></section><section class="chapter" id="operator-upgrade"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Upgrading the Red Hat Quay Operator Overview</h1></div></div></div><p>
			The Red Hat Quay Operator follows a <span class="emphasis"><em>synchronized versioning</em></span> scheme, which means that each version of the Operator is tied to the version of Red Hat Quay and the components that it manages. There is no field on the <code class="literal">QuayRegistry</code> custom resource which sets the version of Red Hat Quay to <code class="literal">deploy</code>; the Operator can only deploy a single version of all components. This scheme was chosen to ensure that all components work well together and to reduce the complexity of the Operator needing to know how to manage the lifecycles of many different versions of Red Hat Quay on Kubernetes.
		</p><section class="section" id="operator-lifecycle-manager"><div class="titlepage"><div><div><h2 class="title">13.1. Operator Lifecycle Manager</h2></div></div></div><p>
				The Red Hat Quay Operator should be installed and upgraded using the <a class="link" href="https://docs.openshift.com/container-platform/3.9/operators/understanding/olm/olm-understanding-olm.html">Operator Lifecycle Manager (OLM)</a>. When creating a <code class="literal">Subscription</code> with the default <code class="literal">approvalStrategy: Automatic</code>, OLM will automatically upgrade the Red Hat Quay Operator whenever a new version becomes available.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					When the Red Hat Quay Operator is installed by Operator Lifecycle Manager, it might be configured to support automatic or manual upgrades. This option is shown on the <span class="strong strong"><strong>Operator Hub</strong></span> page for the Red Hat Quay Operator during installation. It can also be found in the Red Hat Quay Operator <code class="literal">Subscription</code> object by the <code class="literal">approvalStrategy</code> field. Choosing <code class="literal">Automatic</code> means that your Red Hat Quay Operator will automatically be upgraded whenever a new Operator version is released. If this is not desirable, then the <code class="literal">Manual</code> approval strategy should be selected.
				</p></div></div></section><section class="section" id="upgrading-quay-operator"><div class="titlepage"><div><div><h2 class="title">13.2. Upgrading the Quay Operator</h2></div></div></div><p>
				The standard approach for upgrading installed Operators on OpenShift Container Platform is documented at <a class="link" href="https://docs.openshift.com/container-platform/4.7/operators/admin/olm-upgrading-operators.html">Upgrading installed Operators</a>.
			</p><p>
				In general, Red Hat Quay supports upgrades from a prior (N-1) minor version only. For example, upgrading directly from Red Hat Quay 3.0.5 to the latest version of 3.5 is not supported. Instead, users would have to upgrade as follows:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						3.0.5 → 3.1.3
					</li><li class="listitem">
						3.1.3 → 3.2.2
					</li><li class="listitem">
						3.2.2 → 3.3.4
					</li><li class="listitem">
						3.3.4 → 3.4.z
					</li><li class="listitem">
						3.4.z → 3.5.z
					</li></ol></div><p>
				This is required to ensure that any necessary database migrations are done correctly and in the right order during the upgrade.
			</p><p>
				In some cases, Red Hat Quay supports direct, single-step upgrades from prior (N-2, N-3) minor versions. This exception to the normal, prior minor version-only, upgrade simplifies the upgrade procedure for customers on older releases. The following upgrade paths are supported:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						3.3.z → 3.6.z
					</li><li class="listitem">
						3.4.z → 3.6.z
					</li><li class="listitem">
						3.4.z → 3.7.z
					</li><li class="listitem">
						3.5.z → 3.7.z
					</li><li class="listitem">
						3.7.z → 3.8.z
					</li><li class="listitem">
						3.8.z → 3.9.z
					</li></ol></div><p>
				For users on standalone deployments of Red Hat Quay wanting to upgrade to 3.9, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.9/html-single/upgrade_red_hat_quay/index#standalone_upgrade">Standalone upgrade</a> guide.
			</p><section class="section" id="upgrading-red-hat-quay"><div class="titlepage"><div><div><h3 class="title">13.2.1. Upgrading Quay</h3></div></div></div><p>
					To update Red Hat Quay from one minor version to the next, for example, 3.4 → 3.5, you must change the update channel for the Red Hat Quay Operator.
				</p><p>
					For <code class="literal">z</code> stream upgrades, for example, 3.4.2 → 3.4.3, updates are released in the major-minor channel that the user initially selected during install. The procedure to perform a <code class="literal">z</code> stream upgrade depends on the <code class="literal">approvalStrategy</code> as outlined above. If the approval strategy is set to <code class="literal">Automatic</code>, the Quay Operator will upgrade automatically to the newest <code class="literal">z</code> stream. This results in automatic, rolling Quay updates to newer <code class="literal">z</code> streams with little to no downtime. Otherwise, the update must be manually approved before installation can begin.
				</p></section><section class="section" id="upgrade-33-36"><div class="titlepage"><div><div><h3 class="title">13.2.2. Upgrading directly from 3.3.z or 3.4.z to 3.6</h3></div></div></div><p>
					The following section provides important information when upgrading from Red Hat Quay 3.3.z or 3.4.z to 3.6.
				</p><section class="section" id="upgrading-edge-routing-enabled"><div class="titlepage"><div><div><h4 class="title">13.2.2.1. Upgrading with edge routing enabled</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Previously, when running a 3.3.z version of Red Hat Quay with edge routing enabled, users were unable to upgrade to 3.4.z versions of Red Hat Quay. This has been resolved with the release of Red Hat Quay 3.6.
							</li><li class="listitem"><p class="simpara">
								When upgrading from 3.3.z to 3.6, if <code class="literal">tls.termination</code> is set to <code class="literal">none</code> in your Red Hat Quay 3.3.z deployment, it will change to HTTPS with TLS edge termination and use the default cluster wildcard certificate. For example:
							</p><pre class="programlisting language-yaml">apiVersion: redhatcop.redhat.io/v1alpha1
kind: QuayEcosystem
metadata:
  name: quay33
spec:
  quay:
    imagePullSecretName: redhat-pull-secret
    enableRepoMirroring: true
    image: quay.io/quay/quay:v3.3.4-2
    ...
    externalAccess:
      hostname: quayv33.apps.devcluster.openshift.com
      tls:
        termination: none
    database:
...</pre></li></ul></div></section><section class="section" id="upgrading-with-tls-cert-key-pairs-without-san"><div class="titlepage"><div><div><h4 class="title">13.2.2.2. Upgrading with custom SSL/TLS certificate/key pairs without Subject Alternative Names</h4></div></div></div><p>
						There is an issue for customers using their own SSL/TLS certificate/key pairs without Subject Alternative Names (SANs) when upgrading from Red Hat Quay 3.3.4 to Red Hat Quay 3.6 directly. During the upgrade to Red Hat Quay 3.6, the deployment is blocked, with the error message from the Red Hat Quay Operator pod logs indicating that the Red Hat Quay SSL/TLS certificate must have SANs.
					</p><p>
						If possible, you should regenerate your SSL/TLS certificates with the correct hostname in the SANs. A possible workaround involves defining an environment variable in the <code class="literal">quay-app</code>, <code class="literal">quay-upgrade</code> and <code class="literal">quay-config-editor</code> pods after upgrade to enable CommonName matching:
					</p><pre class="screen"> GODEBUG=x509ignoreCN=0</pre><p>
						The <code class="literal">GODEBUG=x509ignoreCN=0</code> flag enables the legacy behavior of treating the CommonName field on X.509 certificates as a hostname when no SANs are present. However, this workaround is not recommended, as it will not persist across a redeployment.
					</p></section><section class="section" id="configuring-clair-v4-upgrading-from-33-34-to-36"><div class="titlepage"><div><div><h4 class="title">13.2.2.3. Configuring Clair v4 when upgrading from 3.3.z or 3.4.z to 3.6 using the Red Hat Quay Operator</h4></div></div></div><p>
						To set up Clair v4 on a new Red Hat Quay deployment on OpenShift Container Platform, it is highly recommended to use the Red Hat Quay Operator. By default, the Red Hat Quay Operator will install or upgrade a Clair deployment along with your Red Hat Quay deployment and configure Clair automatically.
					</p><p>
						For instructions about setting up Clair v4 in a disconnected OpenShift Container Platform cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.9/html-single/manage_red_hat_quay/index#clair-openshift">Setting Up Clair on a Red Hat Quay OpenShift deployment</a>.
					</p></section></section><section class="section" id="swift-config-upgrading-from-33-to-36"><div class="titlepage"><div><div><h3 class="title">13.2.3. Swift configuration when upgrading from 3.3.z to 3.6</h3></div></div></div><p>
					When upgrading from Red Hat Quay 3.3.z to 3.6.z, some users might receive the following error: <code class="literal">Switch auth v3 requires tenant_id (string) in os_options</code>. As a workaround, you can manually update your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> to add the <code class="literal">os_options</code> and <code class="literal">tenant_id</code> parameters:
				</p><pre class="programlisting language-yaml">  DISTRIBUTED_STORAGE_CONFIG:
    brscale:
    - SwiftStorage
    - auth_url: http://****/v3
      auth_version: "3"
      os_options:
        tenant_id: ****
        project_name: ocp-base
        user_domain_name: Default
      storage_path: /datastorage/registry
      swift_container: ocp-svc-quay-ha
      swift_password: *****
      swift_user: *****</pre></section><section class="section" id="changin-update-channel-for-operator"><div class="titlepage"><div><div><h3 class="title">13.2.4. Changing the update channel for the Red Hat Quay Operator</h3></div></div></div><p>
					The subscription of an installed Operator specifies an update channel, which is used to track and receive updates for the Operator. To upgrade the Red Hat Quay Operator to start tracking and receiving updates from a newer channel, change the update channel in the <span class="strong strong"><strong>Subscription</strong></span> tab for the installed Red Hat Quay Operator. For subscriptions with an <code class="literal">Automatic</code> approval strategy, the upgrade begins automatically and can be monitored on the page that lists the Installed Operators.
				</p></section><section class="section" id="manually-approving-pending-operator-upgrade"><div class="titlepage"><div><div><h3 class="title">13.2.5. Manually approving a pending Operator upgrade</h3></div></div></div><p>
					If an installed Operator has the approval strategy in its subscription set to <code class="literal">Manual</code>, when new updates are released in its current update channel, the update must be manually approved before installation can begin. If the Red Hat Quay Operator has a pending upgrade, this status will be displayed in the list of Installed Operators. In the <code class="literal">Subscription</code> tab for the Red Hat Quay Operator, you can preview the install plan and review the resources that are listed as available for upgrade. If satisfied, click <code class="literal">Approve</code> and return to the page that lists Installed Operators to monitor the progress of the upgrade.
				</p><p>
					The following image shows the <span class="strong strong"><strong>Subscription</strong></span> tab in the UI, including the update <code class="literal">Channel</code>, the <code class="literal">Approval</code> strategy, the <code class="literal">Upgrade status</code> and the <code class="literal">InstallPlan</code>:
				</p><p>
					<span class="inlinemediaobject"><img src="images/update-channel-approval-strategy.png" alt="Subscription tab including upgrade Channel and Approval strategy"/></span>
				</p><p>
					The list of Installed Operators provides a high-level summary of the current Quay installation:
				</p><p>
					<span class="inlinemediaobject"><img src="images/installed-operators-list.png" alt="Installed Operators"/></span>
				</p></section></section><section class="section" id="upgrading-quayregistry"><div class="titlepage"><div><div><h2 class="title">13.3. Upgrading a QuayRegistry</h2></div></div></div><p>
				When the Red Hat Quay Operator starts, it immediately looks for any <code class="literal">QuayRegistries</code> it can find in the namespace(s) it is configured to watch. When it finds one, the following logic is used:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If <code class="literal">status.currentVersion</code> is unset, reconcile as normal.
					</li><li class="listitem">
						If <code class="literal">status.currentVersion</code> equals the Operator version, reconcile as normal.
					</li><li class="listitem">
						If <code class="literal">status.currentVersion</code> does not equal the Operator version, check if it can be upgraded. If it can, perform upgrade tasks and set the <code class="literal">status.currentVersion</code> to the Operator’s version once complete. If it cannot be upgraded, return an error and leave the <code class="literal">QuayRegistry</code> and its deployed Kubernetes objects alone.
					</li></ul></div></section><section class="section" id="upgrading-quayecosystem"><div class="titlepage"><div><div><h2 class="title">13.4. Upgrading a QuayEcosystem</h2></div></div></div><p>
				Upgrades are supported from previous versions of the Operator which used the <code class="literal">QuayEcosystem</code> API for a limited set of configurations. To ensure that migrations do not happen unexpectedly, a special label needs to be applied to the <code class="literal">QuayEcosystem</code> for it to be migrated. A new <code class="literal">QuayRegistry</code> will be created for the Operator to manage, but the old <code class="literal">QuayEcosystem</code> will remain until manually deleted to ensure that you can roll back and still access Quay in case anything goes wrong. To migrate an existing <code class="literal">QuayEcosystem</code> to a new <code class="literal">QuayRegistry</code>, use the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Add <code class="literal">"quay-operator/migrate": "true"</code> to the <code class="literal">metadata.labels</code> of the <code class="literal">QuayEcosystem</code>.
					</p><pre class="programlisting language-terminal">$ oc edit quayecosystem &lt;quayecosystemname&gt;</pre><pre class="programlisting language-yaml">metadata:
  labels:
    quay-operator/migrate: "true"</pre></li><li class="listitem">
						Wait for a <code class="literal">QuayRegistry</code> to be created with the same <code class="literal">metadata.name</code> as your <code class="literal">QuayEcosystem</code>. The <code class="literal">QuayEcosystem</code> will be marked with the label <code class="literal">"quay-operator/migration-complete": "true"</code>.
					</li><li class="listitem">
						After the <code class="literal">status.registryEndpoint</code> of the new <code class="literal">QuayRegistry</code> is set, access Red Hat Quay and confirm that all data and settings were migrated successfully.
					</li><li class="listitem">
						If everything works correctly, you can delete the <code class="literal">QuayEcosystem</code> and Kubernetes garbage collection will clean up all old resources.
					</li></ol></div><section class="section" id="reverting-quayecosystem-upgrade"><div class="titlepage"><div><div><h3 class="title">13.4.1. Reverting QuayEcosystem Upgrade</h3></div></div></div><p>
					If something goes wrong during the automatic upgrade from <code class="literal">QuayEcosystem</code> to <code class="literal">QuayRegistry</code>, follow these steps to revert back to using the <code class="literal">QuayEcosystem</code>:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Delete the <code class="literal">QuayRegistry</code> using either the UI or <code class="literal">kubectl</code>:
						</p><pre class="programlisting language-terminal">$ kubectl delete -n &lt;namespace&gt; quayregistry &lt;quayecosystem-name&gt;</pre></li><li class="listitem">
							If external access was provided using a <code class="literal">Route</code>, change the <code class="literal">Route</code> to point back to the original <code class="literal">Service</code> using the UI or <code class="literal">kubectl</code>.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your <code class="literal">QuayEcosystem</code> was managing the PostgreSQL database, the upgrade process will migrate your data to a new PostgreSQL database managed by the upgraded Operator. Your old database will not be changed or removed but Red Hat Quay will no longer use it once the migration is complete. If there are issues during the data migration, the upgrade process will exit and it is recommended that you continue with your database as an unmanaged component.
					</p></div></div></section><section class="section" id="supported-quayecossytem-configurations-for-upgrades"><div class="titlepage"><div><div><h3 class="title">13.4.2. Supported QuayEcosystem Configurations for Upgrades</h3></div></div></div><p>
					The Red Hat Quay Operator reports errors in its logs and in <code class="literal">status.conditions</code> if migrating a <code class="literal">QuayEcosystem</code> component fails or is unsupported. All unmanaged components should migrate successfully because no Kubernetes resources need to be adopted and all the necessary values are already provided in Red Hat Quay’s <code class="literal">config.yaml</code> file.
				</p><p>
					<span class="strong strong"><strong>Database</strong></span>
				</p><p>
					Ephemeral database not supported (<code class="literal">volumeSize</code> field must be set).
				</p><p>
					<span class="strong strong"><strong>Redis</strong></span>
				</p><p>
					Nothing special needed.
				</p><p>
					<span class="strong strong"><strong>External Access</strong></span>
				</p><p>
					Only passthrough <code class="literal">Route</code> access is supported for automatic migration. Manual migration required for other methods.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">LoadBalancer</code> without custom hostname: After the <code class="literal">QuayEcosystem</code> is marked with label <code class="literal">"quay-operator/migration-complete": "true"</code>, delete the <code class="literal">metadata.ownerReferences</code> field from existing <code class="literal">Service</code> <span class="emphasis"><em>before</em></span> deleting the <code class="literal">QuayEcosystem</code> to prevent Kubernetes from garbage collecting the <code class="literal">Service</code> and removing the load balancer. A new <code class="literal">Service</code> will be created with <code class="literal">metadata.name</code> format <code class="literal">&lt;QuayEcosystem-name&gt;-quay-app</code>. Edit the <code class="literal">spec.selector</code> of the existing <code class="literal">Service</code> to match the <code class="literal">spec.selector</code> of the new <code class="literal">Service</code> so traffic to the old load balancer endpoint will now be directed to the new pods. You are now responsible for the old <code class="literal">Service</code>; the Quay Operator will not manage it.
						</li><li class="listitem">
							<code class="literal">LoadBalancer</code>/<code class="literal">NodePort</code>/<code class="literal">Ingress</code> with custom hostname: A new <code class="literal">Service</code> of type <code class="literal">LoadBalancer</code> will be created with <code class="literal">metadata.name</code> format <code class="literal">&lt;QuayEcosystem-name&gt;-quay-app</code>. Change your DNS settings to point to the <code class="literal">status.loadBalancer</code> endpoint provided by the new <code class="literal">Service</code>.
						</li></ul></div><p>
					<span class="strong strong"><strong>Clair</strong></span>
				</p><p>
					Nothing special needed.
				</p><p>
					<span class="strong strong"><strong>Object Storage</strong></span>
				</p><p>
					<code class="literal">QuayEcosystem</code> did not have a managed object storage component, so object storage will always be marked as unmanaged. Local storage is not supported.
				</p><p>
					<span class="strong strong"><strong>Repository Mirroring</strong></span>
				</p><p>
					Nothing special needed.
				</p><h2 id="additional_resources">Additional resources</h2><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For more details on the Red Hat Quay Operator, see the upstream <a class="link" href="https://github.com/quay/quay-operator/">quay-operator</a> project.
						</li></ul></div></section></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm45898215151520"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>