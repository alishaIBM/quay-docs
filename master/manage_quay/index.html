<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Manage Red Hat Quay</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.4"/><meta name="description" content="Manage Red Hat Quay"/><link rel="next" href="#idm45621345778704" title="Preface"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm45621345347632"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Quay</span> <span class="productnumber">3.9</span></div><div><h1 class="title">Manage Red Hat Quay</h1></div><div><h2 class="subtitle">Manage Red Hat Quay</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm45621347306288">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Manage Red Hat Quay
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="preface"><a href="#idm45621345778704">Preface</a></span></li><li><span class="chapter"><a href="#advanced-quay-configuration">1. Advanced Red Hat Quay configuration</a></span><ul><li><span class="section"><a href="#using-the-config-tool">1.1. Using Red Hat Quay Config Tool to modify Red Hat Quay</a></span><ul><li><span class="section"><a href="#running-config-tool-from-quay-operator">1.1.1. Running the Config Tool from the Red Hat Quay Operator</a></span></li><li><span class="section"><a href="#running-config-tool-from-cli">1.1.2. Running the Config Tool from the command line</a></span></li><li><span class="section"><a href="#deploying-config-tool-using-tls">1.1.3. Deploying the config tool using TLS certificates</a></span></li></ul></li><li><span class="section"><a href="#overview-advanced-config">1.2. Using the API to modify Red Hat Quay</a></span></li><li><span class="section"><a href="#editing-config-file-to-modify-quay">1.3. Editing the <code class="literal">config.yaml</code> file to modify Red Hat Quay</a></span><ul><li><span class="section"><a href="#add-name-and-company-to-quay-sign-in">1.3.1. Add name and company to Red Hat Quay sign-in</a></span></li><li><span class="section"><a href="#disable-tls-protocols">1.3.2. Disable TLS Protocols</a></span></li><li><span class="section"><a href="#rate-limit-api-calls">1.3.3. Rate limit API calls</a></span></li><li><span class="section"><a href="#adjust-database-connection-pool">1.3.4. Adjust database connection pooling</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#config-using-api">2. Using the configuration API</a></span><ul><li><span class="section"><a href="#retrieving_the_default_configuration">2.1. Retrieving the default configuration</a></span></li><li><span class="section"><a href="#retrieving_the_current_configuration">2.2. Retrieving the current configuration</a></span></li><li><span class="section"><a href="#validating_configuration_using_the_api">2.3. Validating configuration using the API</a></span></li><li><span class="section"><a href="#determining_the_required_fields">2.4. Determining the required fields</a></span></li></ul></li><li><span class="chapter"><a href="#release-notifications">3. Getting Red Hat Quay release notifications</a></span></li><li><span class="chapter"><a href="#using-ssl-to-protect-quay">4. Using SSL to protect connections to Red Hat Quay</a></span><ul><li><span class="section"><a href="#introduction-using-ssl">4.1. Using SSL/TLS</a></span></li><li><span class="section"><a href="#create-a-ca-and-sign-a-certificate">4.2. Creating a certificate authority and signing a certificate</a></span><ul><li><span class="section"><a href="#creating-a-certificate-authority">4.2.1. Creating a certificate authority</a></span></li><li><span class="section"><a href="#signing-a-certificate">4.2.2. Signing a certificate</a></span></li></ul></li><li><span class="section"><a href="#configuring-ssl-using-cli">4.3. Configuring SSL using the command line interface</a></span></li><li><span class="section"><a href="#configuring-ssl-using-ui">4.4. Configuring SSL/TLS using the Red Hat Quay UI</a></span></li><li><span class="section"><a href="#testing_ssl_configuration_using_the_command_line">4.5. Testing SSL configuration using the command line</a></span></li><li><span class="section"><a href="#testing_ssl_configuration_using_the_browser">4.6. Testing SSL configuration using the browser</a></span></li><li><span class="section"><a href="#configuring_podman_to_trust_the_certificate_authority">4.7. Configuring podman to trust the Certificate Authority</a></span></li><li><span class="section"><a href="#configuring-system-trust-ca">4.8. Configuring the system to trust the certificate authority</a></span></li></ul></li><li><span class="chapter"><a href="#config-custom-ssl-certs-manual">5. Adding TLS Certificates to the Red Hat Quay Container</a></span><ul><li><span class="section"><a href="#add-certificates-to-quay-container">5.1. Add TLS certificates to Red Hat Quay</a></span></li><li><span class="section"><a href="#config-custom-ssl-cert-kubernetes">5.2. Adding custom SSL/TLS certificates when Red Hat Quay is deployed on Kubernetes</a></span></li></ul></li><li><span class="chapter"><a href="#proc_manage-log-storage">6. Configuring action log storage for Elasticsearch and Splunk</a></span><ul><li><span class="section"><a href="#proc_manage-log-storage-elasticsearch">6.1. Configuring action log storage for Elasticsearch</a></span></li><li><span class="section"><a href="#proc_manage-log-storage-splunk">6.2. Configuring action log storage for Splunk</a></span><ul><li><span class="section"><a href="#proc_installing-creating-username-splunk">6.2.1. Installing and creating a username for Splunk</a></span></li><li><span class="section"><a href="#proc_generating-splunk-token">6.2.2. Generating a Splunk token</a></span></li><li><span class="section"><a href="#proc_splunk-config">6.2.3. Configuring Red Hat Quay to use Splunk</a></span></li><li><span class="section"><a href="#proc_splunk-action-log">6.2.4. Creating an action log</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#clair-vulnerability-scanner">7. Clair for Red Hat Quay</a></span><ul><li><span class="section"><a href="#clair-vulnerability-scanner-hosts">7.1. Clair vulnerability databases</a></span></li><li><span class="section"><a href="#clair-standalone-configure">7.2. Setting up Clair on standalone Red Hat Quay deployments</a></span></li><li><span class="section"><a href="#clair-quay-operator-overview">7.3. Clair on OpenShift Container Platform</a></span></li><li><span class="section"><a href="#clair-testing">7.4. Testing Clair</a></span></li></ul></li><li><span class="chapter"><a href="#quay-bridge-operator">8. Integrating Red Hat Quay into OpenShift Container Platform with the Quay Bridge Operator</a></span><ul><li><span class="section"><a href="#setting-up-quay-for-qbo">8.1. Setting up Red Hat Quay for the Quay Bridge Operator</a></span></li><li><span class="section"><a href="#installing-qbo-on-ocp">8.2. Installing the Quay Bridge Operator on OpenShift Container Platform</a></span></li><li><span class="section"><a href="#creating-ocp-secret-for-oauth-token">8.3. Creating an OpenShift Container Platform secret for the OAuth token</a></span></li><li><span class="section"><a href="#creating-quay-integration-cr">8.4. Creating the QuayIntegration custom resource</a></span><ul><li><span class="section"><a href="#optional_creating_the_quayintegration_custom_resource_using_the_cli">8.4.1. Optional: Creating the QuayIntegration custom resource using the CLI</a></span></li></ul></li><li><span class="section"><a href="#quay-integration-config-fields">8.5. QuayIntegration configuration fields</a></span></li></ul></li><li><span class="chapter"><a href="#repo-mirroring-in-red-hat-quay">9. Repository mirroring</a></span><ul><li><span class="section"><a href="#arch-mirroring-intro">9.1. Repository mirroring</a></span></li><li><span class="section"><a href="#mirroring-versus-georepl">9.2. Repository mirroring compared to geo-replication</a></span></li><li><span class="section"><a href="#arch-mirroring-using">9.3. Using repository mirroring</a></span></li><li><span class="section"><a href="#mirroring_configuration_ui">9.4. Mirroring configuration UI</a></span></li><li><span class="section"><a href="#config-fields-mirroring">9.5. Mirroring configuration fields</a></span></li><li><span class="section"><a href="#mirroring-worker">9.6. Mirroring worker</a></span></li><li><span class="section"><a href="#mirroring-creating-repo">9.7. Creating a mirrored repository</a></span><ul><li><span class="section"><a href="#mirroring-repository-mirroring-settings">9.7.1. Repository mirroring settings</a></span></li><li><span class="section"><a href="#mirroring-advanced-settings">9.7.2. Advanced settings</a></span></li><li><span class="section"><a href="#mirroring-synchronize-now">9.7.3. Synchronize now</a></span></li></ul></li><li><span class="section"><a href="#arch-mirroring-events">9.8. Event notifications for mirroring</a></span></li><li><span class="section"><a href="#mirroring-tag-patterns">9.9. Mirroring tag patterns</a></span><ul><li><span class="section"><a href="#pattern_syntax">9.9.1. Pattern syntax</a></span></li><li><span class="section"><a href="#example_tag_patterns">9.9.2. Example tag patterns</a></span></li></ul></li><li><span class="section"><a href="#mirroring-working-with">9.10. Working with mirrored repositories</a></span></li><li><span class="section"><a href="#arch-mirroring-recommend">9.11. Repository mirroring recommendations</a></span></li></ul></li><li><span class="chapter"><a href="#proc_manage-ipv6-dual-stack">10. IPv6 and dual-stack deployments</a></span><ul><li><span class="section"><a href="#proc-manage-enabling-ipv6">10.1. Enabling the IPv6 protocol family</a></span></li><li><span class="section"><a href="#proc-manageenabling-dual-stack">10.2. Enabling the dual-stack protocol family</a></span></li><li><span class="section"><a href="#proc_manage-ipv6-limitations-38">10.3. IPv6 and dua-stack limitations</a></span></li></ul></li><li><span class="chapter"><a href="#ldap-authentication-setup-for-quay-enterprise">11. LDAP Authentication Setup for Red Hat Quay</a></span><ul><li><span class="section"><a href="#ldap-considerations">11.1. Considerations when enabling LDAP</a></span></li><li><span class="section"><a href="#setup-ldap-configuration">11.2. Configuring LDAP for Red Hat Quay</a></span></li><li><span class="section"><a href="#ldap-restricted-users-enabling">11.3. Enabling the LDAP_RESTRICTED_USER_FILTER configuration field</a></span></li><li><span class="section"><a href="#ldap-super-users-enabling">11.4. Enabling the LDAP_SUPERUSER_FILTER configuration field</a></span></li><li><span class="section"><a href="#common-ldap-configuration-issues">11.5. Common LDAP configuration issues</a></span></li><li><span class="section"><a href="#ldap-configuration-fields-link">11.6. LDAP configuration fields</a></span></li></ul></li><li><span class="chapter"><a href="#configuring-oidc-authentication">12. Configuring OIDC for Red Hat Quay</a></span><ul><li><span class="section"><a href="#configuring-red-hat-sso-oidc">12.1. Configuring Red Hat Single Sign-On for Red Hat Quay</a></span><ul><li><span class="section"><a href="#configuring-red-hat-sso-using-config-tool">12.1.1. Configuring the Red Hat Single Sign-On Operator for the Red Hat Quay Operator</a></span></li><li><span class="section"><a href="#configuring-quay-operator-use-redhat-sso">12.1.2. Configuring the Red Hat Quay Operator to use Red Hat Single Sign-On</a></span></li></ul></li><li><span class="section"><a href="#configuring-azuread-oidc">12.2. Configuring Azure AD OIDC for Red Hat Quay</a></span><ul><li><span class="section"><a href="#configuring-azuread-using-config-tool">12.2.1. Configuring Azure AD by using the Red Hat Quay config tool</a></span></li><li><span class="section"><a href="#configuring-azuread-updating-config-yaml">12.2.2. Configuring Azure AD by updating the Red Hat Quay config.yaml file</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#prometheus-metrics-under-quay-enterprise">13. Prometheus and Grafana metrics under Red Hat Quay</a></span><ul><li><span class="section"><a href="#exposing-the-prometheus-endpoint">13.1. Exposing the Prometheus endpoint</a></span><ul><li><span class="section"><a href="#standalone_red_hat_quay">13.1.1. Standalone Red Hat Quay</a></span></li><li><span class="section"><a href="#red_hat_quay_operator">13.1.2. Red Hat Quay Operator</a></span></li><li><span class="section"><a href="#setting-up-prometheus-to-consume-metrics">13.1.3. Setting up Prometheus to consume metrics</a></span></li><li><span class="section"><a href="#dns-configuration-under-kubernetes">13.1.4. DNS configuration under Kubernetes</a></span></li><li><span class="section"><a href="#dns-configuration-for-a-manual-cluster">13.1.5. DNS configuration for a manual cluster</a></span></li></ul></li><li><span class="section"><a href="#metrics-intro">13.2. Introduction to metrics</a></span><ul><li><span class="section"><a href="#metrics-general-registry-stats">13.2.1. General registry statistics</a></span></li><li><span class="section"><a href="#metrics-queue-items">13.2.2. Queue items</a></span></li><li><span class="section"><a href="#metrics-garbage-collection">13.2.3. Garbage collection metrics</a></span></li><li><span class="section"><a href="#metrics-image-push-pull">13.2.4. Image push / pull metrics</a></span></li><li><span class="section"><a href="#metrics-authentication">13.2.5. Authentication metrics</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#red-hat-quay-quota-management-and-enforcement">14. Red Hat Quay quota management and enforcement overview</a></span><ul><li><span class="section"><a href="#quota-management-arch">14.1. Quota management architecture</a></span></li><li><span class="section"><a href="#quota-management-limitations">14.2. Quota management limitations</a></span></li><li><span class="section"><a href="#red-hat-quay-quota-management-39">14.3. Quota management for Red Hat Quay 3.9</a></span><ul><li><span class="section"><a href="#quota-management-configuring-38">14.3.1. Option A: Configuring quota management for Red Hat Quay 3.9 by adjusting the QUOTA_TOTAL_DELAY feature flag</a></span></li><li><span class="section"><a href="#quota-management-configuring-39">14.3.2. Option B: Configuring quota management for Red Hat Quay 3.9 by setting QUOTA_TOTAL_DELAY_SECONDS to 0</a></span></li></ul></li><li><span class="section"><a href="#quota-management-testing-39">14.4. Testing quota management for Red Hat Quay 3.9</a></span></li><li><span class="section"><a href="#default-quota">14.5. Setting default quota</a></span></li><li><span class="section"><a href="#quota-establishment-ui">14.6. Establishing quota in Red Hat Quay UI</a></span></li><li><span class="section"><a href="#quota-establishment-api">14.7. Establishing quota with the Red Hat Quay API</a></span><ul><li><span class="section"><a href="#setting_the_quota">14.7.1. Setting the quota</a></span></li><li><span class="section"><a href="#viewing_the_quota">14.7.2. Viewing the quota</a></span></li><li><span class="section"><a href="#modifying_the_quota">14.7.3. Modifying the quota</a></span></li><li><span class="section"><a href="#pushing_images">14.7.4. Pushing images</a></span></li><li><span class="section"><a href="#rejecting_pushes_using_quota_limits">14.7.5. Rejecting pushes using quota limits</a></span></li></ul></li><li><span class="section"><a href="#quota-management-query-39">14.8. Calculating the total registry size in Red Hat Quay 3.9</a></span></li><li><span class="section"><a href="#deleting-tag-permanently">14.9. Permanently deleting an image tag</a></span><ul><li><span class="section"><a href="#permanently-deleting-image-tag-v2-ui">14.9.1. Permanently deleting an image tag using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#permanently-deleting-image-tag-legacy-ui">14.9.2. Permanently deleting an image tag using the Red Hat Quay legacy UI</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#georepl-intro">15. Geo-replication</a></span><ul><li><span class="section"><a href="#arch-georpl-features">15.1. Geo-replication features</a></span></li><li><span class="section"><a href="#arch-georepl-prereqs">15.2. Geo-replication requirements and constraints</a></span></li><li><span class="section"><a href="#georepl-arch-standalone">15.3. Geo-replication using standalone Red Hat Quay</a></span><ul><li><span class="section"><a href="#enable-storage-replication-standalone">15.3.1. Enable storage replication - standalone Quay</a></span></li><li><span class="section"><a href="#georepl-deploy-standalone">15.3.2. Run Red Hat Quay with storage preferences</a></span></li><li><span class="section"><a href="#standalone-georepl-site-removal">15.3.3. Removing a geo-replicated site from your standalone Red Hat Quay deployment</a></span></li></ul></li><li><span class="section"><a href="#georepl-arch-operator">15.4. Geo-replication using the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#georepl-deploy-operator">15.4.1. Setting up geo-replication on OpenShift Container Platform</a></span></li><li><span class="section"><a href="#operator-georepl-site-removal">15.4.2. Removing a geo-replicated site from your Red Hat Quay Operator deployment</a></span></li></ul></li><li><span class="section"><a href="#georepl-mixed-storage">15.5. Mixed storage for geo-replication</a></span></li></ul></li><li><span class="chapter"><a href="#backing-up-and-restoring-intro">16. Backing up and restoring Red Hat Quay managed by the Red Hat Quay Operator</a></span><ul><li><span class="section"><a href="#backing-up-red-hat-quay-operator">16.1. Backing up Red Hat Quay</a></span><ul><li><span class="section"><a href="#quay-configuration-backup">16.1.1. Red Hat Quay configuration backup</a></span></li><li><span class="section"><a href="#scaling-down-quay-deployment">16.1.2. Scaling down your Red Hat Quay deployment</a></span></li><li><span class="section"><a href="#backing-up-managed-database">16.1.3. Backing up the Red Hat Quay managed database</a></span></li><li><span class="section"><a href="#scaling-up-quay-deployment">16.1.4. Scale the Red Hat Quay deployment back up</a></span></li></ul></li><li><span class="section"><a href="#restoring-up-red-hat-quay">16.2. Restoring Red Hat Quay</a></span><ul><li><span class="section"><a href="#restoring-quay-and-configuration-from-backup">16.2.1. Restoring Red Hat Quay and its configuration from a backup</a></span></li><li><span class="section"><a href="#scale-down-quay-deployment">16.2.2. Scaling down your Red Hat Quay deployment</a></span></li><li><span class="section"><a href="#restoring-quay-database">16.2.3. Restoring your Red Hat Quay database</a></span></li><li><span class="section"><a href="#restoring-quay-object-storage-data">16.2.4. Restore your Red Hat Quay object storage data</a></span></li><li><span class="section"><a href="#scaling-up-quay">16.2.5. Scaling up your Red Hat Quay deployment</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#migrating_a_standalone_quay_deployment_to_a_red_hat_quay_operator_managed_deployment">17. Migrating a standalone Quay deployment to a Red Hat Quay Operator managed deployment</a></span><ul><li><span class="section"><a href="#backing_up_a_standalone_deployment_of_red_hat_quay">17.1. Backing up a standalone deployment of Red Hat Quay</a></span></li><li><span class="section"><a href="#using_backed_up_standalone_content_to_migrate_to_openshift_container_platform">17.2. Using backed up standalone content to migrate to OpenShift Container Platform.</a></span></li></ul></li><li><span class="chapter"><a href="#standalone-deployment-backup-restore">18. Backing up and restoring Red Hat Quay on a standalone deployment</a></span><ul><li><span class="section"><a href="#backing-up-red-hat-quay-standalone">18.1. Backing up Red Hat Quay on standalone deployments</a></span></li><li><span class="section"><a href="#restoring-red-hat-quay-standalone">18.2. Restoring Red Hat Quay on standalone deployments</a></span></li></ul></li><li><span class="chapter"><a href="#supported-oci-media-types">19. Configuring artifact types</a></span><ul><li><span class="section"><a href="#configuring-oci-media-types-proc">19.1. Configuring OCI artifact types</a></span></li><li><span class="section"><a href="#configuring-additional-oci-media-types-proc">19.2. Configuring additional artifact types</a></span></li><li><span class="section"><a href="#configuring-unknown-oci-media-types-proc">19.3. Configuring unknown media types</a></span></li></ul></li><li><span class="chapter"><a href="#garbage-collection">20. Red Hat Quay garbage collection</a></span><ul><li><span class="section"><a href="#garbage-collection-practice">20.1. Red Hat Quay garbage collection in practice</a></span><ul><li><span class="section"><a href="#measuring-storage-reclamation">20.1.1. Measuring storage reclamation</a></span></li></ul></li><li><span class="section"><a href="#garbage-collection-configuration-fields">20.2. Garbage collection configuration fields</a></span></li><li><span class="section"><a href="#disabling-garbage-collection">20.3. Disabling garbage collection</a></span></li><li><span class="section"><a href="#garbage-collection-quota-management">20.4. Garbage collection and quota management</a></span></li><li><span class="section"><a href="#garbage-collection-procedure">20.5. Garbage collection in practice</a></span></li><li><span class="section"><a href="#garbage-collection-metrics">20.6. Red Hat Quay garbage collection metrics</a></span></li></ul></li><li><span class="chapter"><a href="#using-v2-ui">21. Using the Red Hat Quay v2 UI</a></span><ul><li><span class="section"><a href="#reference-miscellaneous-v2-ui">21.1. v2 user interface configuration</a></span><ul><li><span class="section"><a href="#creating-new-organization-v2-ui">21.1.1. Creating a new organization in the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#deleting-organization-v2">21.1.2. Deleting an organization using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#creating-new-repository-v2">21.1.3. Creating a new repository using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#deleting-repository-v2">21.1.4. Deleting a repository using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#pushing-image-v2">21.1.5. Pushing an image to the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#deleting-image-v2">21.1.6. Deleting an image using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#creating-robot-account-v2-ui">21.1.7. Creating a robot account using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#organization-settings-v2-ui">21.1.8. Organization settings for the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#tag-overview-v2-ui">21.1.9. Viewing image tag information using the Red Hat Quay v2 UI</a></span></li><li><span class="section"><a href="#settings-overview-v2-ui">21.1.10. Adjusting repository settings using the Red Hat Quay v2 UI</a></span></li></ul></li><li><span class="section"><a href="#enabling-legacy-ui">21.2. Enabling the Red Hat Quay legacy UI</a></span></li></ul></li><li><span class="chapter"><a href="#health-check-quay">22. Performing health checks on Red Hat Quay deployments</a></span><ul><li><span class="section"><a href="#health-check-endpoints">22.1. Red Hat Quay health check endpoints</a></span></li><li><span class="section"><a href="#instance-endpoint-quay">22.2. Navigating to a Red Hat Quay health check endpoint</a></span></li></ul></li><li><span class="chapter"><a href="#branding-quay-deployment">23. Branding a Red Hat Quay deployment on the legacy UI</a></span></li><li><span class="chapter"><a href="#quay-schema">24. Schema for Red Hat Quay configuration</a></span></li></ul></div><section class="preface" id="idm45621345778704"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div><p>
			Once you have deployed a Red Hat Quay registry, there are many ways you can further configure and manage that deployment. Topics covered here include:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Advanced Red Hat Quay configuration
				</li><li class="listitem">
					Setting notifications to alert you of a new Red Hat Quay release
				</li><li class="listitem">
					Securing connections with SSL/TLS certificates
				</li><li class="listitem">
					Directing action logs storage to Elasticsearch
				</li><li class="listitem">
					Configuring image security scanning with Clair
				</li><li class="listitem">
					Scan pod images with the Container Security Operator
				</li><li class="listitem">
					Integrate Red Hat Quay into OpenShift Container Platform with the Quay Bridge Operator
				</li><li class="listitem">
					Mirroring images with repository mirroring
				</li><li class="listitem">
					Sharing Red Hat Quay images with a BitTorrent service
				</li><li class="listitem">
					Authenticating users with LDAP
				</li><li class="listitem">
					Enabling Quay for Prometheus and Grafana metrics
				</li><li class="listitem">
					Setting up geo-replication
				</li><li class="listitem">
					Troubleshooting Red Hat Quay
				</li></ul></div><p>
			For a complete list of Red Hat Quay configuration fields, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.8/html-single/configure_red_hat_quay/index">Configure Red Hat Quay</a> page.
		</p></section><section class="chapter" id="advanced-quay-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Advanced Red Hat Quay configuration</h1></div></div></div><p>
			You can configure your Red Hat Quay after initial deployment using one of the following interfaces:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The Red Hat Quay Config Tool. With this tool, a web-based interface for configuring the Red Hat Quay cluster is provided when running the <code class="literal">Quay</code> container in <code class="literal">config</code> mode. This method is recommended for configuring the Red Hat Quay service.
				</li><li class="listitem">
					Editing the <code class="literal">config.yaml</code>. The <code class="literal">config.yaml</code> file contains most configuration information for the Red Hat Quay cluster. Editing the <code class="literal">config.yaml</code> file directly is possible, but it is only recommended for advanced tuning and performance features that are not available through the Config Tool.
				</li><li class="listitem">
					Red Hat Quay API. Some Red Hat Quay features can be configured through the API.
				</li></ul></div><p>
			This content in this section describes how to use each of the aforementioned interfaces and how to configure your deployment with advanced features.
		</p><section class="section" id="using-the-config-tool"><div class="titlepage"><div><div><h2 class="title">1.1. Using Red Hat Quay Config Tool to modify Red Hat Quay</h2></div></div></div><p>
				The Red Hat Quay Config Tool is made available by running a <code class="literal">Quay</code> container in <code class="literal">config</code> mode alongside the regular Red Hat Quay service.
			</p><p>
				Use the following sections to run the Config Tool from the Red Hat Quay Operator, or to run the Config Tool on host systems from the command line interface (CLI).
			</p><section class="section" id="running-config-tool-from-quay-operator"><div class="titlepage"><div><div><h3 class="title">1.1.1. Running the Config Tool from the Red Hat Quay Operator</h3></div></div></div><p>
					When running the Red Hat Quay Operator on OpenShift Container Platform, the Config Tool is readily available to use. Use the following procedure to access the Red Hat Quay Config Tool.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							You have deployed the Red Hat Quay Operator on OpenShift Container Platform.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure.</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the OpenShift console, select the Red Hat Quay project, for example, <code class="literal">quay-enterprise</code>.
						</li><li class="listitem"><p class="simpara">
							In the navigation pane, select <span class="strong strong"><strong>Networking</strong></span> → <span class="strong strong"><strong>Routes</strong></span>. You should see routes to both the Red Hat Quay application and Config Tool, as shown in the following image:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/configtoolroute.png" alt="View the route to the Red Hat Quay Config Tool"/></span>
						</p></li><li class="listitem">
							Select the route to the Config Tool, for example, <code class="literal">example-quayecosystem-quay-config</code>. The Config Tool UI should open in your browser.
						</li><li class="listitem"><p class="simpara">
							Select <span class="strong strong"><strong>Modify configuration for this cluster</strong></span> to bring up the Config Tool setup, for example:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/configtoolsetup.png" alt="Modify Red Hat Quay cluster settings from the Config Tool"/></span>
						</p></li><li class="listitem">
							Make the desired changes, and then select <span class="strong strong"><strong>Save Configuration Changes</strong></span>.
						</li><li class="listitem">
							Make any corrections needed by clicking <span class="strong strong"><strong>Continue Editing</strong></span>, or, select <span class="strong strong"><strong>Next</strong></span> to continue.
						</li><li class="listitem">
							When prompted, select <span class="strong strong"><strong>Download Configuration</strong></span>. This will download a tarball of your new <code class="literal">config.yaml</code>, as well as any certificates and keys used with your Red Hat Quay setup. The <code class="literal">config.yaml</code> can be used to make advanced changes to your configuration or use as a future reference.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Go to deployment rollout</strong></span> → <span class="strong strong"><strong>Populate the configuration to deployments</strong></span>. Wait for the Red Hat Quay pods to restart for the changes to take effect.
						</li></ol></div></section><section class="section" id="running-config-tool-from-cli"><div class="titlepage"><div><div><h3 class="title">1.1.2. Running the Config Tool from the command line</h3></div></div></div><p>
					If you are running Red Hat Quay from a host system, you can use the following procedure to make changes to your configuration after the initial deployment.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Prerequisites
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You have installed either <code class="literal">podman</code> or <code class="literal">docker</code>.
								</li></ul></div></li><li class="listitem">
							Start Red Hat Quay in configuration mode.
						</li><li class="listitem"><p class="simpara">
							On the first <code class="literal">Quay</code> node, enter the following command:
						</p><pre class="screen">$ podman run --rm -it --name quay_config -p 8080:8080 \
    -v path/to/config-bundle:/conf/stack \
    registry.redhat.io/quay/quay-rhel8:v3.9.0 config &lt;my_secret_password&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To modify an existing config bundle, you can mount your configuration directory into the <code class="literal">Quay</code> container.
							</p></div></div></li><li class="listitem">
							When the Red Hat Quay configuration tool starts, open your browser and navigate to the URL and port used in your configuration file, for example, <code class="literal">quay-server.example.com:8080</code>.
						</li><li class="listitem">
							Enter your username and password.
						</li><li class="listitem">
							Modify your Red Hat Quay cluster as desired.
						</li></ol></div></section><section class="section" id="deploying-config-tool-using-tls"><div class="titlepage"><div><div><h3 class="title">1.1.3. Deploying the config tool using TLS certificates</h3></div></div></div><p>
					You can deploy the config tool with secured TLS certificates by passing environment variables to the runtime variable. This ensures that sensitive data like credentials for the database and storage backend are protected.
				</p><p>
					The public and private keys must contain valid Subject Alternative Names (SANs) for the route that you deploy the config tool on.
				</p><p>
					The paths can be specified using <code class="literal">CONFIG_TOOL_PRIVATE_KEY</code> and <code class="literal">CONFIG_TOOL_PUBLIC_KEY</code>.
				</p><p>
					If you are running your deployment from a container, the <code class="literal">CONFIG_TOOL_PRIVATE_KEY</code> and <code class="literal">CONFIG_TOOL_PUBLIC_KEY</code> values the locations of the certificates inside of the container. For example:
				</p><pre class="programlisting language-terminal">$ podman run --rm -it --name quay_config -p 7070:8080 \

-v ${PRIVATE_KEY_PATH}:/tls/localhost.key \
-v ${PUBLIC_KEY_PATH}:/tls/localhost.crt \
-e CONFIG_TOOL_PRIVATE_KEY=/tls/localhost.key \
-e CONFIG_TOOL_PUBLIC_KEY=/tls/localhost.crt \
-e DEBUGLOG=true \
-ti config-app:dev</pre></section></section><section class="section" id="overview-advanced-config"><div class="titlepage"><div><div><h2 class="title">1.2. Using the API to modify Red Hat Quay</h2></div></div></div><p>
				See the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/red_hat_quay_api_guide/index">Red Hat Quay API Guide</a> for information on how to access Red Hat Quay API.
			</p></section><section class="section" id="editing-config-file-to-modify-quay"><div class="titlepage"><div><div><h2 class="title">1.3. Editing the <code class="literal">config.yaml</code> file to modify Red Hat Quay</h2></div></div></div><p>
				Some advanced configuration features that are not available through the Config Tool can be implemented by editing the <code class="literal">config.yaml</code> file directly. Available settings are described in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/manage_red_hat_quay/quay-schema">Schema for Red Hat Quay configuration</a>
			</p><p>
				The following examples are settings you can change directly in the <code class="literal">config.yaml</code> file.
			</p><section class="section" id="add-name-and-company-to-quay-sign-in"><div class="titlepage"><div><div><h3 class="title">1.3.1. Add name and company to Red Hat Quay sign-in</h3></div></div></div><p>
					By setting the following field, users are prompted for their name and company when they first sign in. This is an optional field, but can provide your with extra data about your Red Hat Quay users.
				</p><pre class="programlisting language-yaml">---
FEATURE_USER_METADATA: true
---</pre></section><section class="section" id="disable-tls-protocols"><div class="titlepage"><div><div><h3 class="title">1.3.2. Disable TLS Protocols</h3></div></div></div><p>
					You can change the <code class="literal">SSL_PROTOCOLS</code> setting to remove SSL protocols that you do not want to support in your Red Hat Quay instance. For example, to remove TLS v1 support from the default <code class="literal">SSL_PROTOCOLS:['TLSv1','TLSv1.1','TLSv1.2']</code>, change it to the following:
				</p><pre class="programlisting language-yaml">---
SSL_PROTOCOLS : ['TLSv1.1','TLSv1.2']
---</pre></section><section class="section" id="rate-limit-api-calls"><div class="titlepage"><div><div><h3 class="title">1.3.3. Rate limit API calls</h3></div></div></div><p>
					Adding the <code class="literal">FEATURE_RATE_LIMITS</code> parameter to the <code class="literal">config.yaml</code> file causes <code class="literal">nginx</code> to limit certain API calls to 30-per-second. If <code class="literal">FEATURE_RATE_LIMITS</code> is not set, API calls are limited to 300-per-second, effectively making them unlimited.
				</p><p>
					Rate limiting is important when you must ensure that the available resources are not overwhelmed with traffic.
				</p><p>
					Some namespaces might require unlimited access, for example, if they are important to CI/CD and take priority. In that scenario, those namespaces might be placed in a list in the <code class="literal">config.yaml</code> file using the <code class="literal">NON_RATE_LIMITED_NAMESPACES</code>.
				</p></section><section class="section" id="adjust-database-connection-pool"><div class="titlepage"><div><div><h3 class="title">1.3.4. Adjust database connection pooling</h3></div></div></div><p>
					Red Hat Quay is composed of many different processes which all run within the same container. Many of these processes interact with the database.
				</p><p>
					With the <code class="literal">DB_CONNECTION_POOLING</code> parameter, each process that interacts with the database will contain a connection pool These per-process connection pools are configured to maintain a maximum of 20 connections. When under heavy load, it is possible to fill the connection pool for every process within a Red Hat Quay container. Under certain deployments and loads, this might require analysis to ensure that Red Hat Quay does not exceed the database’s configured maximum connection count.
				</p><p>
					Over time, the connection pools will release idle connections. To release all connections immediately, Red Hat Quay must be restarted.
				</p><p>
					Database connection pooling can be toggled by setting the <code class="literal">DB_CONNECTION_POOLING</code> to <code class="literal">true</code> or <code class="literal">false</code>. For example:
				</p><pre class="programlisting language-yaml">---
DB_CONNECTION_POOLING: true
---</pre><p>
					When <code class="literal">DB_CONNECTION_POOLING</code> is enabled, you can change the maximum size of the connection pool with the <code class="literal">DB_CONNECTION_ARGS</code> in your <code class="literal">config.yaml</code>. For example:
				</p><pre class="programlisting language-yaml">---
DB_CONNECTION_ARGS:
  max_connections: 10
---</pre><section class="section" id="database-connection-arguments"><div class="titlepage"><div><div><h4 class="title">1.3.4.1. Database connection arguments</h4></div></div></div><p>
						You can customize your Red Hat Quay database connection settings within the <code class="literal">config.yaml</code> file. These are dependent on your deployment’s database driver, for example, <code class="literal">psycopg2</code> for Postgres and <code class="literal">pymysql</code> for MySQL. You can also pass in argument used by Peewee’s connection pooling mechanism. For example:
					</p><pre class="programlisting language-yaml">---
DB_CONNECTION_ARGS:
  max_connections: n  # Max Connection Pool size. (Connection Pooling only)
  timeout: n  # Time to hold on to connections. (Connection Pooling only)
  stale_timeout: n  # Number of seconds to block when the pool is full. (Connection Pooling only)
---</pre></section><section class="section" id="database-ssl-configuration"><div class="titlepage"><div><div><h4 class="title">1.3.4.2. Database SSL configuration</h4></div></div></div><p>
						Some key-value pairs defined under the <code class="literal">DB_CONNECTION_ARGS</code> field are generic, while others are specific to the database. In particular, SSL configuration depends on the database that you are deploying.
					</p><section class="section" id="postgres-ssl-connection-arguments"><div class="titlepage"><div><div><h5 class="title">1.3.4.2.1. PostgreSQL SSL connection arguments</h5></div></div></div><p>
							The following YAML shows a sample PostgreSQL SSL configuration:
						</p><pre class="programlisting language-yaml">---
DB_CONNECTION_ARGS:
  sslmode: verify-ca
  sslrootcert: /path/to/cacert
---</pre><p>
							The <code class="literal">sslmode</code> parameter determines whether, or with, what priority a secure SSL TCP/IP connection will be negotiated with the server. There are six modes for the <code class="literal">sslmode</code> parameter:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>disabl:</strong></span>: Only try a non-SSL connection.
								</li><li class="listitem">
									<span class="strong strong"><strong>allow</strong></span>: Try a non-SSL connection first. Upon failure, try an SSL connection.
								</li><li class="listitem">
									<span class="strong strong"><strong>prefer</strong></span>: Default. Try an SSL connection first. Upon failure, try a non-SSL connection.
								</li><li class="listitem">
									<span class="strong strong"><strong>require</strong></span>: Only try an SSL connection. If a root CA file is present, verify the connection in the same way as if <code class="literal">verify-ca</code> was specified.
								</li><li class="listitem">
									<span class="strong strong"><strong>verify-ca</strong></span>: Only try an SSL connection, and verify that the server certificate is issued by a trust certificate authority (CA).
								</li><li class="listitem">
									<span class="strong strong"><strong>verify-full</strong></span>: Only try an SSL connection. Verify that the server certificate is issued by a trust CA, and that the requested server host name matches that in the certificate.
								</li></ul></div><p>
							For more information about the valid arguments for PostgreSQL, see <a class="link" href="https://www.postgresql.org/docs/current/libpq-connect.html">Database Connection Control Functions</a>.
						</p></section><section class="section" id="mysql-ssl-connection-arguments"><div class="titlepage"><div><div><h5 class="title">1.3.4.2.2. MySQL SSL connection arguments</h5></div></div></div><p>
							The following YAML shows a sample MySQL SSL configuration:
						</p><pre class="programlisting language-yaml">---
DB_CONNECTION_ARGS:
  ssl:
    ca: /path/to/cacert
---</pre><p>
							For more information about the valid connection arguments for MySQL, see <a class="link" href="https://dev.mysql.com/doc/refman/8.0/en/connecting-using-uri-or-key-value-pairs.html">Connecting to the Server Using URI-Like Strings or Key-Value Pairs</a>.
						</p></section></section><section class="section" id="http-connection-counts"><div class="titlepage"><div><div><h4 class="title">1.3.4.3. HTTP connection counts</h4></div></div></div><p>
						You can specify the quantity of simultaneous HTTP connections using environment variables. The environment variables can be specified as a whole, or for a specific component. The default for each is 50 parallel connections per process. See the following YAML for example environment variables;
					</p><pre class="programlisting language-yaml">---
WORKER_CONNECTION_COUNT_REGISTRY=n
WORKER_CONNECTION_COUNT_WEB=n
WORKER_CONNECTION_COUNT_SECSCAN=n
WORKER_CONNECTION_COUNT=n
---</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Specifying a count for a specific component will override any value set in the <code class="literal">WORKER_CONNECTION_COUNT</code> configuration field.
						</p></div></div></section><section class="section" id="dynamic-process-counts"><div class="titlepage"><div><div><h4 class="title">1.3.4.4. Dynamic process counts</h4></div></div></div><p>
						To estimate the quantity of dynamically sized processes, the following calculation is used by default.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Red Hat Quay queries the available CPU count from the entire machine. Any limits applied using kubernetes or other non-virtualized mechanisms will not affect this behavior. Red Hat Quay makes its calculation based on the total number of processors on the Node. The default values listed are simply targets, but shall not exceed the maximum or be lower than the minimum.
						</p></div></div><p>
						Each of the following process quantities can be overridden using the environment variable specified below:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								registry - Provides HTTP endpoints to handle registry action
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										minimum: 8
									</li><li class="listitem">
										maximum: 64
									</li><li class="listitem">
										default: $CPU_COUNT x 4
									</li><li class="listitem">
										environment variable: WORKER_COUNT_REGISTRY
									</li></ul></div></li><li class="listitem"><p class="simpara">
								web - Provides HTTP endpoints for the web-based interface
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										minimum: 2
									</li><li class="listitem">
										maximum: 32
									</li><li class="listitem">
										default: $CPU_COUNT x 2
									</li><li class="listitem">
										environment_variable: WORKER_COUNT_WEB
									</li></ul></div></li><li class="listitem"><p class="simpara">
								secscan - Interacts with Clair
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										minimum: 2
									</li><li class="listitem">
										maximum: 4
									</li><li class="listitem">
										default: $CPU_COUNT x 2
									</li><li class="listitem">
										environment variable: WORKER_COUNT_SECSCAN
									</li></ul></div></li></ul></div></section><section class="section" id="environment-variables"><div class="titlepage"><div><div><h4 class="title">1.3.4.5. Environment variables</h4></div></div></div><p>
						Red Hat Quay allows overriding default behavior using environment variables. The following table lists and describes each variable and the values they can expect.
					</p><div class="table" id="idm45621345262080"><p class="title"><strong>Table 1.1. Worker count environment variables</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 33%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621349227968" scope="col">Variable</th><th align="left" valign="top" id="idm45621349226880" scope="col">Description</th><th align="left" valign="top" id="idm45621349225792" scope="col">Values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621349227968">
									<p>
										WORKER_COUNT_REGISTRY
									</p>
									</td><td align="left" valign="top" headers="idm45621349226880">
									<p>
										Specifies the number of processes to handle registry requests within the <code class="literal">Quay</code> container.
									</p>
									</td><td align="left" valign="top" headers="idm45621349225792">
									<p>
										Integer between 8 and 64
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45621349227968">
									<p>
										WORKER_COUNT_WEB
									</p>
									</td><td align="left" valign="top" headers="idm45621349226880">
									<p>
										Specifies the number of processes to handle UI/Web requests within the container.
									</p>
									</td><td align="left" valign="top" headers="idm45621349225792">
									<p>
										Integer between 2 and 32
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45621349227968">
									<p>
										WORKER_COUNT_SECSCAN
									</p>
									</td><td align="left" valign="top" headers="idm45621349226880">
									<p>
										Specifies the number of processes to handle Security Scanning (for example, Clair) integration within the container.
									</p>
									</td><td align="left" valign="top" headers="idm45621349225792">
									<p>
										Integer between 2 and 4
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45621349227968">
									<p>
										DB_CONNECTION_POOLING
									</p>
									</td><td align="left" valign="top" headers="idm45621349226880">
									<p>
										Toggle database connection pooling.
									</p>
									</td><td align="left" valign="top" headers="idm45621349225792">
									<p>
										"true" or "false"
									</p>
									</td></tr></tbody></table></div></div></section><section class="section" id="turning-off-connection-pooling"><div class="titlepage"><div><div><h4 class="title">1.3.4.6. Turning off connection pooling</h4></div></div></div><p>
						Red Hat Quay deployments with a large amount of user activity can regularly hit the 2k maximum database connection limit. In these cases, connection pooling, which is enabled by default for Red Hat Quay, can cause database connection count to rise exponentially and require you to turn off connection pooling.
					</p><p>
						If turning off connection pooling is not enough to prevent hitting the 2k database connection limit, you need to take additional steps to deal with the problem. If this happens, you might need to increase the maximum database connections to better suit your workload.
					</p></section></section></section></section><section class="chapter" id="config-using-api"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Using the configuration API</h1></div></div></div><p>
			The configuration tool exposes 4 endpoints that can be used to build, validate, bundle and deploy a configuration. The config-tool API is documented at <a class="link" href="https://github.com/quay/config-tool/blob/master/pkg/lib/editor/API.md">https://github.com/quay/config-tool/blob/master/pkg/lib/editor/API.md</a>. In this section, you will see how to use the API to retrieve the current configuration and how to validate any changes you make.
		</p><section class="section" id="retrieving_the_default_configuration"><div class="titlepage"><div><div><h2 class="title">2.1. Retrieving the default configuration</h2></div></div></div><p>
				If you are running the configuration tool for the first time, and do not have an existing configuration, you can retrieve the default configuration. Start the container in config mode:
			</p><pre class="screen">$ sudo podman run --rm -it --name quay_config \
  -p 8080:8080 \
  registry.redhat.io/quay/quay-rhel8:v3.9.0 config secret</pre><p>
				Use the <code class="literal">config</code> endpoint of the configuration API to get the default:
			</p><pre class="literallayout">$ curl -X GET -u quayconfig:secret http://quay-server:8080/api/v1/config  | jq</pre><p>
				The value returned is the default configuration in JSON format:
			</p><pre class="programlisting language-json">{
  "config.yaml": {
    "AUTHENTICATION_TYPE": "Database",
    "AVATAR_KIND": "local",
    "DB_CONNECTION_ARGS": {
      "autorollback": true,
      "threadlocals": true
    },
    "DEFAULT_TAG_EXPIRATION": "2w",
    "EXTERNAL_TLS_TERMINATION": false,
    "FEATURE_ACTION_LOG_ROTATION": false,
    "FEATURE_ANONYMOUS_ACCESS": true,
    "FEATURE_APP_SPECIFIC_TOKENS": true,
    ....
  }

}</pre></section><section class="section" id="retrieving_the_current_configuration"><div class="titlepage"><div><div><h2 class="title">2.2. Retrieving the current configuration</h2></div></div></div><p>
				If you have already configured and deployed the Quay registry, stop the container and restart it in configuration mode, loading the existing configuration as a volume:
			</p><pre class="screen">$ sudo podman run --rm -it --name quay_config \
  -p 8080:8080 \
  -v $QUAY/config:/conf/stack:Z \
  registry.redhat.io/quay/quay-rhel8:v3.9.0 config secret</pre><p>
				Use the <code class="literal">config</code> endpoint of the API to get the current configuration:
			</p><pre class="literallayout">$ curl -X GET -u quayconfig:secret http://quay-server:8080/api/v1/config  | jq</pre><p>
				The value returned is the current configuration in JSON format, including database and Redis configuration data:
			</p><pre class="programlisting language-json">{
  "config.yaml": {
    ....
    "BROWSER_API_CALLS_XHR_ONLY": false,
    "BUILDLOGS_REDIS": {
      "host": "quay-server",
      "password": "strongpassword",
      "port": 6379
    },
    "DATABASE_SECRET_KEY": "4b1c5663-88c6-47ac-b4a8-bb594660f08b",
    "DB_CONNECTION_ARGS": {
      "autorollback": true,
      "threadlocals": true
    },
    "DB_URI": "postgresql://quayuser:quaypass@quay-server:5432/quay",
    "DEFAULT_TAG_EXPIRATION": "2w",
    ....


  }

}</pre></section><section class="section" id="validating_configuration_using_the_api"><div class="titlepage"><div><div><h2 class="title">2.3. Validating configuration using the API</h2></div></div></div><p>
				You can validate a configuration by posting it to the <code class="literal">config/validate</code> endpoint:
			</p><pre class="literallayout">curl -u quayconfig:secret --header 'Content-Type: application/json' --request POST --data '
{
  "config.yaml": {
    ....
    "BROWSER_API_CALLS_XHR_ONLY": false,
    "BUILDLOGS_REDIS": {
      "host": "quay-server",
      "password": "strongpassword",
      "port": 6379
    },
    "DATABASE_SECRET_KEY": "4b1c5663-88c6-47ac-b4a8-bb594660f08b",
    "DB_CONNECTION_ARGS": {
      "autorollback": true,
      "threadlocals": true
    },
    "DB_URI": "postgresql://quayuser:quaypass@quay-server:5432/quay",
    "DEFAULT_TAG_EXPIRATION": "2w",
    ....

  }

} http://quay-server:8080/api/v1/config/validate | jq</pre><p>
				The returned value is an array containing the errors found in the configuration. If the configuration is valid, an empty array <code class="literal">[]</code> is returned.
			</p></section><section class="section" id="determining_the_required_fields"><div class="titlepage"><div><div><h2 class="title">2.4. Determining the required fields</h2></div></div></div><p>
				You can determine the required fields by posting an empty configuration structure to the <code class="literal">config/validate</code> endpoint:
			</p><pre class="literallayout">curl -u quayconfig:secret --header 'Content-Type: application/json' --request POST --data '
{
  "config.yaml": {
  }

} http://quay-server:8080/api/v1/config/validate | jq</pre><p>
				The value returned is an array indicating which fields are required:
			</p><pre class="programlisting language-yaml">[
  {
    "FieldGroup": "Database",
    "Tags": [
      "DB_URI"
    ],
    "Message": "DB_URI is required."
  },
  {
    "FieldGroup": "DistributedStorage",
    "Tags": [
      "DISTRIBUTED_STORAGE_CONFIG"
    ],
    "Message": "DISTRIBUTED_STORAGE_CONFIG must contain at least one storage location."
  },
  {
    "FieldGroup": "HostSettings",
    "Tags": [
      "SERVER_HOSTNAME"
    ],
    "Message": "SERVER_HOSTNAME is required"
  },
  {
    "FieldGroup": "HostSettings",
    "Tags": [
      "SERVER_HOSTNAME"
    ],
    "Message": "SERVER_HOSTNAME must be of type Hostname"
  },
  {
    "FieldGroup": "Redis",
    "Tags": [
      "BUILDLOGS_REDIS"
    ],
    "Message": "BUILDLOGS_REDIS is required"
  }
]</pre></section></section><section class="chapter" id="release-notifications"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Getting Red Hat Quay release notifications</h1></div></div></div><p>
			To keep up with the latest Red Hat Quay releases and other changes related to Red Hat Quay, you can sign up for update notifications on the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a>. After signing up for notifications, you will receive notifications letting you know when there is new a Red Hat Quay version, updated documentation, or other Red Hat Quay news.
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Log into the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a> with your Red Hat customer account credentials.
				</li><li class="listitem">
					Select your user name (upper-right corner) to see Red Hat Account and Customer Portal selections: 
					<span class="inlinemediaobject"><img src="images/notification-profile.png" alt="View account and portal selections"/></span>
				</li><li class="listitem">
					Select Notifications. Your profile activity page appears.
				</li><li class="listitem">
					Select the Notifications tab.
				</li><li class="listitem">
					Select Manage Notifications.
				</li><li class="listitem">
					Select Follow, then choose Products from the drop-down box.
				</li><li class="listitem">
					From the drop-down box next to the Products, search for and select Red Hat Quay: 
					<span class="inlinemediaobject"><img src="images/notification-follow.png" alt="Select Products from notifications box"/></span>
				</li><li class="listitem">
					Select the SAVE NOTIFICATION button. Going forward, you will receive notifications when there are changes to the Red Hat Quay product, such as a new release.
				</li></ol></div></section><section class="chapter" id="using-ssl-to-protect-quay"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Using SSL to protect connections to Red Hat Quay</h1></div></div></div><section class="section" id="introduction-using-ssl"><div class="titlepage"><div><div><h2 class="title">4.1. Using SSL/TLS</h2></div></div></div><p>
				To configure Red Hat Quay with a <a class="link" href="https://en.wikipedia.org/wiki/Self-signed_certificate">self-signed certificate</a>, you must create a Certificate Authority (CA) and then generate the required key and certificate files.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The following examples assume you have configured the server hostname <code class="literal">quay-server.example.com</code> using DNS or another naming mechanism, such as adding an entry in your <code class="literal">/etc/hosts</code> file:
				</p><pre class="programlisting language-terminal">$ cat /etc/hosts
...
192.168.1.112   quay-server.example.com</pre></div></div></section><section class="section" id="create-a-ca-and-sign-a-certificate"><div class="titlepage"><div><div><h2 class="title">4.2. Creating a certificate authority and signing a certificate</h2></div></div></div><p>
				Use the following procedures to create a certificate file and a primary key file named <code class="literal">ssl.cert</code> and <code class="literal">ssl.key</code>.
			</p><section class="section" id="creating-a-certificate-authority"><div class="titlepage"><div><div><h3 class="title">4.2.1. Creating a certificate authority</h3></div></div></div><p>
					Use the following procedure to create a certificate authority (CA)
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Generate the root CA key by entering the following command:
						</p><pre class="programlisting language-terminal">$ openssl genrsa -out rootCA.key 2048</pre></li><li class="listitem"><p class="simpara">
							Generate the root CA certificate by entering the following command:
						</p><pre class="programlisting language-terminal">$ openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem</pre></li><li class="listitem"><p class="simpara">
							Enter the information that will be incorporated into your certificate request, including the server hostname, for example:
						</p><pre class="programlisting language-terminal">Country Name (2 letter code) [XX]:IE
State or Province Name (full name) []:GALWAY
Locality Name (eg, city) [Default City]:GALWAY
Organization Name (eg, company) [Default Company Ltd]:QUAY
Organizational Unit Name (eg, section) []:DOCS
Common Name (eg, your name or your server's hostname) []:quay-server.example.com</pre></li></ol></div></section><section class="section" id="signing-a-certificate"><div class="titlepage"><div><div><h3 class="title">4.2.2. Signing a certificate</h3></div></div></div><p>
					Use the following procedure to sign a certificate.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Generate the server key by entering the following command:
						</p><pre class="programlisting language-terminal">$ openssl genrsa -out ssl.key 2048</pre></li><li class="listitem"><p class="simpara">
							Generate a signing request by entering the following command:
						</p><pre class="programlisting language-terminal">$ openssl req -new -key ssl.key -out ssl.csr</pre></li><li class="listitem"><p class="simpara">
							Enter the information that will be incorporated into your certificate request, including the server hostname, for example:
						</p><pre class="programlisting language-terminal">Country Name (2 letter code) [XX]:IE
State or Province Name (full name) []:GALWAY
Locality Name (eg, city) [Default City]:GALWAY
Organization Name (eg, company) [Default Company Ltd]:QUAY
Organizational Unit Name (eg, section) []:DOCS
Common Name (eg, your name or your server's hostname) []:quay-server.example.com</pre></li><li class="listitem"><p class="simpara">
							Create a configuration file <code class="literal">openssl.cnf</code>, specifying the server hostname, for example:
						</p><div class="formalpara"><p class="title"><strong>openssl.cnf</strong></p><p>
								
<pre class="programlisting language-terminal">[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = quay-server.example.com
IP.1 = 192.168.1.112</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Use the configuration file to generate the certificate <code class="literal">ssl.cert</code>:
						</p><pre class="programlisting language-terminal">$ openssl x509 -req -in ssl.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out ssl.cert -days 356 -extensions v3_req -extfile openssl.cnf</pre></li></ol></div></section></section><section class="section" id="configuring-ssl-using-cli"><div class="titlepage"><div><div><h2 class="title">4.3. Configuring SSL using the command line interface</h2></div></div></div><p>
				Use the following procedure to configure SSL/TLS using the command line interface.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a certificate authority and signed the certificate.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Copy the certificate file and primary key file to your configuration directory, ensuring they are named <code class="literal">ssl.cert</code> and <code class="literal">ssl.key</code> respectively:
					</p><pre class="programlisting language-terminal">cp ~/ssl.cert ~/ssl.key $QUAY/config</pre></li><li class="listitem"><p class="simpara">
						Change into the <code class="literal">$QUAY/config</code> directory by entering the following command:
					</p><pre class="programlisting language-terminal">$ cd $QUAY/config</pre></li><li class="listitem"><p class="simpara">
						Edit the <code class="literal">config.yaml</code> file and specify that you want Red Hat Quay to handle TLS/SSL:
					</p><div class="formalpara"><p class="title"><strong>config.yaml</strong></p><p>
							
<pre class="programlisting language-yaml">...
SERVER_HOSTNAME: quay-server.example.com
...
PREFERRED_URL_SCHEME: https
...</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Optional: Append the contents of the rootCA.pem file to the end of the ssl.cert file by entering the following command:
					</p><pre class="programlisting language-terminal">$ cat rootCA.pem &gt;&gt; ssl.cert</pre></li><li class="listitem"><p class="simpara">
						Stop the <code class="literal">Quay</code> container by entering the following command:
					</p><pre class="programlisting language-terminal">$ sudo podman stop quay</pre></li><li class="listitem"><p class="simpara">
						Restart the registry by entering the following command:
					</p><pre class="screen">$ sudo podman run -d --rm -p 80:8080 -p 443:8443 \
  --name=quay \
  -v $QUAY/config:/conf/stack:Z \
  -v $QUAY/storage:/datastorage:Z \
  registry.redhat.io/quay/quay-rhel8:v3.9.0</pre></li></ol></div></section><section class="section" id="configuring-ssl-using-ui"><div class="titlepage"><div><div><h2 class="title">4.4. Configuring SSL/TLS using the Red Hat Quay UI</h2></div></div></div><p>
				Use the following procedure to configure SSL/TLS using the Red Hat Quay UI.
			</p><p>
				To configure SSL using the command line interface, see "Configuring SSL/TLS using the command line interface".
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a certificate authority and signed the certificate.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Start the <code class="literal">Quay</code> container in configuration mode:
					</p><pre class="screen">$ sudo podman run --rm -it --name quay_config -p 80:8080 -p 443:8443 registry.redhat.io/quay/quay-rhel8:v3.9.0 config secret</pre></li><li class="listitem">
						In the <span class="strong strong"><strong>Server Configuration</strong></span> section, select <span class="strong strong"><strong>Red Hat Quay handles TLS</strong></span> for SSL/TLS. Upload the certificate file and private key file created earlier, ensuring that the <span class="strong strong"><strong>Server Hostname</strong></span> matches the value used when the certificates were created.
					</li><li class="listitem">
						Validate and download the updated configuration.
					</li><li class="listitem"><p class="simpara">
						Stop the <code class="literal">Quay</code> container and then restart the registry by entering the following command:
					</p><pre class="screen">$ sudo podman rm -f quay
$ sudo podman run -d --rm -p 80:8080 -p 443:8443 \
--name=quay \
-v $QUAY/config:/conf/stack:Z \
-v $QUAY/storage:/datastorage:Z \
registry.redhat.io/quay/quay-rhel8:v3.9.0</pre></li></ol></div></section><section class="section" id="testing_ssl_configuration_using_the_command_line"><div class="titlepage"><div><div><h2 class="title">4.5. Testing SSL configuration using the command line</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Use the <code class="literal">podman login</code> command to attempt to log in to the Quay registry with SSL enabled:
					</p><pre class="screen">$ sudo podman login quay-server.example.com
Username: quayadmin
Password:

Error: error authenticating creds for "quay-server.example.com": error pinging docker registry quay-server.example.com: Get "https://quay-server.example.com/v2/": x509: certificate signed by unknown authority</pre></li><li class="listitem"><p class="simpara">
						Podman does not trust self-signed certificates. As a workaround, use the <code class="literal">--tls-verify</code> option:
					</p><pre class="screen">$ sudo podman login --tls-verify=false quay-server.example.com
Username: quayadmin
Password:

Login Succeeded!</pre></li></ul></div><p>
				Configuring Podman to trust the root Certificate Authority (CA) is covered in a subsequent section.
			</p></section><section class="section" id="testing_ssl_configuration_using_the_browser"><div class="titlepage"><div><div><h2 class="title">4.6. Testing SSL configuration using the browser</h2></div></div></div><p>
				When you attempt to access the Quay registry, in this case, <code class="literal"><a class="link" href="https://quay-server.example.com">https://quay-server.example.com</a></code>, the browser warns of the potential risk:
			</p><p>
				<span class="inlinemediaobject"><img src="images/ssl-connection-not-private.png" alt="Potential risk"/></span>
			</p><p>
				Proceed to the log in screen, and the browser will notify you that the connection is not secure:
			</p><p>
				<span class="inlinemediaobject"><img src="images/ssl-connection-not-secure.png" alt="Connection not secure"/></span>
			</p><p>
				Configuring the system to trust the root Certificate Authority (CA) is covered in the subsequent section.
			</p></section><section class="section" id="configuring_podman_to_trust_the_certificate_authority"><div class="titlepage"><div><div><h2 class="title">4.7. Configuring podman to trust the Certificate Authority</h2></div></div></div><p>
				Podman uses two paths to locate the CA file, namely, <code class="literal">/etc/containers/certs.d/</code> and <code class="literal">/etc/docker/certs.d/</code>.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Copy the root CA file to one of these locations, with the exact path determined by the server hostname, and naming the file <code class="literal">ca.crt</code>:
					</p><pre class="screen">$ sudo cp rootCA.pem /etc/containers/certs.d/quay-server.example.com/ca.crt</pre></li><li class="listitem"><p class="simpara">
						Alternatively, if you are using Docker, you can copy the root CA file to the equivalent Docker directory:
					</p><pre class="screen">$ sudo cp rootCA.pem /etc/docker/certs.d/quay-server.example.com/ca.crt</pre></li></ul></div><p>
				You should no longer need to use the <code class="literal">--tls-verify=false</code> option when logging in to the registry:
			</p><pre class="screen">$ sudo podman login quay-server.example.com

Username: quayadmin
Password:
Login Succeeded!</pre></section><section class="section" id="configuring-system-trust-ca"><div class="titlepage"><div><div><h2 class="title">4.8. Configuring the system to trust the certificate authority</h2></div></div></div><p>
				Use the following procedure to configure your system to trust the certificate authority.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Enter the following command to copy the <code class="literal">rootCA.pem</code> file to the consolidated system-wide trust store:
					</p><pre class="programlisting language-terminal">$ sudo cp rootCA.pem /etc/pki/ca-trust/source/anchors/</pre></li><li class="listitem"><p class="simpara">
						Enter the following command to update the system-wide trust store configuration:
					</p><pre class="programlisting language-terminal">$ sudo update-ca-trust extract</pre></li><li class="listitem"><p class="simpara">
						Optional. You can use the <code class="literal">trust list</code> command to ensure that the <code class="literal">Quay</code> server has been configured:
					</p><pre class="programlisting language-terminal">$ trust list | grep quay
    label: quay-server.example.com</pre><p class="simpara">
						Now, when you browse to the registry at <code class="literal"><a class="link" href="https://quay-server.example.com">https://quay-server.example.com</a></code>, the lock icon shows that the connection is secure:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/ssl-connection-secure.png" alt="Connection not secure"/></span>
					</p></li><li class="listitem"><p class="simpara">
						To remove the <code class="literal">rootCA.pem</code> file from system-wide trust, delete the file and update the configuration:
					</p><pre class="programlisting language-terminal">$ sudo rm /etc/pki/ca-trust/source/anchors/rootCA.pem</pre><pre class="programlisting language-terminal">$ sudo update-ca-trust extract</pre><pre class="programlisting language-terminal">$ trust list | grep quay</pre></li></ol></div><p>
				More information can be found in the RHEL 9 documentation in the chapter <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/securing_networks/index#using-shared-system-certificates_securing-networks">Using shared system certificates</a>.
			</p></section></section><section class="chapter" id="config-custom-ssl-certs-manual"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Adding TLS Certificates to the Red Hat Quay Container</h1></div></div></div><p>
			To add custom TLS certificates to Red Hat Quay, create a new directory named <code class="literal">extra_ca_certs/</code> beneath the Red Hat Quay config directory. Copy any required site-specific TLS certificates to this new directory.
		</p><section class="section" id="add-certificates-to-quay-container"><div class="titlepage"><div><div><h2 class="title">5.1. Add TLS certificates to Red Hat Quay</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View certificate to be added to the container
					</p><pre class="screen">$ cat storage.crt
-----BEGIN CERTIFICATE-----
MIIDTTCCAjWgAwIBAgIJAMVr9ngjJhzbMA0GCSqGSIb3DQEBCwUAMD0xCzAJBgNV
[...]
-----END CERTIFICATE-----</pre></li><li class="listitem"><p class="simpara">
						Create certs directory and copy certificate there
					</p><pre class="screen">$ mkdir -p quay/config/extra_ca_certs
$ cp storage.crt quay/config/extra_ca_certs/
$ tree quay/config/
├── config.yaml
├── extra_ca_certs
│   ├── storage.crt</pre></li><li class="listitem"><p class="simpara">
						Obtain the <code class="literal">Quay</code> container’s <code class="literal">CONTAINER ID</code> with <code class="literal">podman ps</code>:
					</p><pre class="screen">$ sudo podman ps
CONTAINER ID        IMAGE                                COMMAND                  CREATED             STATUS              PORTS
5a3e82c4a75f        &lt;registry&gt;/&lt;repo&gt;/quay:v3.9.0 "/sbin/my_init"          24 hours ago        Up 18 hours         0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 443/tcp   grave_keller</pre></li><li class="listitem"><p class="simpara">
						Restart the container with that ID:
					</p><pre class="screen">$ sudo podman restart 5a3e82c4a75f</pre></li><li class="listitem"><p class="simpara">
						Examine the certificate copied into the container namespace:
					</p><pre class="screen">$ sudo podman exec -it 5a3e82c4a75f cat /etc/ssl/certs/storage.pem
-----BEGIN CERTIFICATE-----
MIIDTTCCAjWgAwIBAgIJAMVr9ngjJhzbMA0GCSqGSIb3DQEBCwUAMD0xCzAJBgNV</pre></li></ol></div></section><section class="section" id="config-custom-ssl-cert-kubernetes"><div class="titlepage"><div><div><h2 class="title">5.2. Adding custom SSL/TLS certificates when Red Hat Quay is deployed on Kubernetes</h2></div></div></div><p>
				When deployed on Kubernetes, Red Hat Quay mounts in a secret as a volume to store config assets. Currently, this breaks the upload certificate function of the superuser panel.
			</p><p>
				As a temporary workaround, <code class="literal">base64</code> encoded certificates can be added to the secret <span class="emphasis"><em>after</em></span> Red Hat Quay has been deployed.
			</p><p>
				Use the following procedure to add custom SSL/TLS certificates when Red Hat Quay is deployed on Kubernetes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Red Hat Quay has been deployed.
					</li><li class="listitem">
						You have a custom <code class="literal">ca.crt</code> file.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Base64 encode the contents of an SSL/TLS certificate by entering the following command:
					</p><pre class="programlisting language-terminal">$ cat ca.crt | base64 -w 0</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">...c1psWGpqeGlPQmNEWkJPMjJ5d0pDemVnR2QNCnRsbW9JdEF4YnFSdVd3PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Enter the following <code class="literal">kubectl</code> command to edit the <code class="literal">quay-enterprise-config-secret</code> file:
					</p><pre class="programlisting language-terminal">$ kubectl --namespace quay-enterprise edit secret/quay-enterprise-config-secret</pre></li><li class="listitem"><p class="simpara">
						Add an entry for the certificate and paste the full <code class="literal">base64</code> encoded stringer under the entry. For example:
					</p><pre class="programlisting language-terminal">  custom-cert.crt:
c1psWGpqeGlPQmNEWkJPMjJ5d0pDemVnR2QNCnRsbW9JdEF4YnFSdVd3PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=</pre></li><li class="listitem"><p class="simpara">
						Use the <code class="literal">kubectl delete</code> command to remove all Red Hat Quay pods. For example:
					</p><pre class="programlisting language-terminal">$ kubectl delete pod quay-operator.v3.7.1-6f9d859bd-p5ftc quayregistry-clair-postgres-7487f5bd86-xnxpr quayregistry-quay-app-upgrade-xq2v6 quayregistry-quay-config-editor-6dfdcfc44f-hlvwm quayregistry-quay-database-859d5445ff-cqthr quayregistry-quay-redis-84f888776f-hhgms</pre><p class="simpara">
						Afterwards, the Red Hat Quay deployment automatically schedules replace pods with the new certificate data.
					</p></li></ol></div></section></section><section class="chapter" id="proc_manage-log-storage"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Configuring action log storage for Elasticsearch and Splunk</h1></div></div></div><p>
			By default, the previous three months of usage logs are stored in the Red Hat Quay database and exposed through the web UI on organization and repository levels. Appropriate administrative privileges are required to see log entries. For deployments with a large amount of logged operations, you can store the usage logs in Elasticsearch and Splunk instead of the Red Hat Quay database backend.
		</p><section class="section" id="proc_manage-log-storage-elasticsearch"><div class="titlepage"><div><div><h2 class="title">6.1. Configuring action log storage for Elasticsearch</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					To configure action log storage for Elasticsearch, you must provide your own Elasticsearch stack, as it is not included with Red Hat Quay as a customizable component.
				</p></div></div><p>
				Enabling Elasticsearch logging can be done during Red Hat Quay deployment or post-deployment using the configuration tool. The resulting configuration is stored in the <code class="literal">config.yaml</code> file. When configured, usage log access continues to be provided through the web UI for repositories and organizations.
			</p><p>
				Use the following procedure to configure action log storage for Elasticsearch:
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Obtain an Elasticsearch account.
					</li><li class="listitem">
						Open the Red Hat Quay Config Tool (either during or after Red Hat Quay deployment).
					</li><li class="listitem"><p class="simpara">
						Scroll to the <span class="strong strong"><strong>Action Log Storage Configuration</strong></span> setting and select <span class="strong strong"><strong>Elasticsearch</strong></span>. The following figure shows the Elasticsearch settings that appear:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/elasticsearch_action_logs.png" alt="Choose Elasticsearch to view settings to store logs"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Fill in the following information for your Elasticsearch instance:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Elasticsearch hostname</strong></span>: The hostname or IP address of the system providing the Elasticsearch service.
							</li><li class="listitem">
								<span class="strong strong"><strong>Elasticsearch port</strong></span>: The port number providing the Elasticsearch service on the host you just entered. Note that the port must be accessible from all systems running the Red Hat Quay registry. The default is TCP port 9200.
							</li><li class="listitem">
								<span class="strong strong"><strong>Elasticsearch access key</strong></span>: The access key needed to gain access to the Elastic search service, if required.
							</li><li class="listitem">
								<span class="strong strong"><strong>Elasticsearch secret key</strong></span>: The secret key needed to gain access to the Elastic search service, if required.
							</li><li class="listitem">
								<span class="strong strong"><strong>AWS region</strong></span>: If you are running on AWS, set the AWS region (otherwise, leave it blank).
							</li><li class="listitem">
								<span class="strong strong"><strong>Index prefix</strong></span>: Choose a prefix to attach to log entries.
							</li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Logs Producer</strong></span>: Choose either Elasticsearch (default) or Kinesis to direct logs to an intermediate Kinesis stream on AWS. You need to set up your own pipeline to send logs from Kinesis to Elasticsearch (for example, Logstash). The following figure shows additional fields you would need to fill in for Kinesis:
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/kinesis_producer.png" alt="On AWS optionally set up an intermediate Kinesis stream"/></span>
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						If you chose Elasticsearch as the Logs Producer, no further configuration is needed. If you chose Kinesis, fill in the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Stream name</strong></span>: The name of the Kinesis stream.
							</li><li class="listitem">
								<span class="strong strong"><strong>AWS access key</strong></span>: The name of the AWS access key needed to gain access to the Kinesis stream, if required.
							</li><li class="listitem">
								<span class="strong strong"><strong>AWS secret key</strong></span>: The name of the AWS secret key needed to gain access to the Kinesis stream, if required.
							</li><li class="listitem">
								<span class="strong strong"><strong>AWS region</strong></span>: The AWS region.
							</li></ul></div></li><li class="listitem">
						When you are done, save the configuration. The configuration tool checks your settings. If there is a problem connecting to the Elasticsearch or Kinesis services, you will see an error and have the opportunity to continue editing. Otherwise, logging will begin to be directed to your Elasticsearch configuration after the cluster restarts with the new configuration.
					</li></ol></div></section><section class="section" id="proc_manage-log-storage-splunk"><div class="titlepage"><div><div><h2 class="title">6.2. Configuring action log storage for Splunk</h2></div></div></div><p>
				<a class="link" href="https://www.splunk.com/">Splunk</a> is an alternative to Elasticsearch that can provide log analyses for your Red Hat Quay data.
			</p><p>
				Enabling Splunk logging can be done during Red Hat Quay deployment or post-deployment using the configuration tool. The resulting configuration is stored in the <code class="literal">config.yaml</code> file. When configured, usage log access continues to be provided through the Splunk web UI for repositories and organizations.
			</p><p>
				Use the following procedures to enable Splunk for your Red Hat Quay deployment.
			</p><section class="section" id="proc_installing-creating-username-splunk"><div class="titlepage"><div><div><h3 class="title">6.2.1. Installing and creating a username for Splunk</h3></div></div></div><p>
					Use the following procedure to install and create Splunk credentials.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a Splunk account by navigating to <a class="link" href="https://www.splunk.com/en_us/sign-up.html">Splunk</a> and entering the required credentials.
						</li><li class="listitem">
							Navigate to the <a class="link" href="https://www.splunk.com/en_us/download/splunk-enterprise.html">Splunk Enterprise</a> <span class="strong strong"><strong>Free Trial</strong></span> page, select your platform and installation package, and then click <span class="strong strong"><strong>Download Now</strong></span>.
						</li><li class="listitem">
							Install the Splunk software on your machine. When prompted, create a username, for example, <code class="literal">splunk_admin</code> and password.
						</li><li class="listitem">
							After creating a username and password, a localhost URL will be provided for your Splunk deployment, for example, <code class="literal"><a class="link" href="http://&lt;sample_url&gt;.remote.csb:8000/">http://&lt;sample_url&gt;.remote.csb:8000/</a></code>. Open the URL in your preferred browser.
						</li><li class="listitem">
							Log in with the username and password you created during installation. You are directed to the Splunk UI.
						</li></ol></div></section><section class="section" id="proc_generating-splunk-token"><div class="titlepage"><div><div><h3 class="title">6.2.2. Generating a Splunk token</h3></div></div></div><p>
					Use one of the following procedures to create a bearer token for Splunk.
				</p><section class="section" id="proc_generating-splunk-token-ui"><div class="titlepage"><div><div><h4 class="title">6.2.2.1. Generating a Splunk token using the Splunk UI</h4></div></div></div><p>
						Use the following procedure to create a bearer token for Splunk using the Splunk UI.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed Splunk and created a username.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								On the Splunk UI, navigate to <span class="strong strong"><strong>Settings</strong></span> → <span class="strong strong"><strong>Tokens</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Enable Token Authentication</strong></span>.
							</li><li class="listitem">
								Ensure that <span class="strong strong"><strong>Token Authentication</strong></span> is enabled by clicking <span class="strong strong"><strong>Token Settings</strong></span> and selecting <span class="strong strong"><strong>Token Authentication</strong></span> if necessary.
							</li><li class="listitem">
								Optional: Set the expiration time for your token. This defaults at 30 days.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Save</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>New Token</strong></span>.
							</li><li class="listitem">
								Enter information for <span class="strong strong"><strong>User</strong></span> and <span class="strong strong"><strong>Audience</strong></span>.
							</li><li class="listitem">
								Optional: Set the <span class="strong strong"><strong>Expiration</strong></span> and <span class="strong strong"><strong>Not Before</strong></span> information.
							</li><li class="listitem"><p class="simpara">
								Click <span class="strong strong"><strong>Create</strong></span>. Your token appears in the <span class="strong strong"><strong>Token</strong></span> box. Copy the token immediately.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									If you close out of the box before copying the token, you must create a new token. The token in its entirety is not available after closing the <span class="strong strong"><strong>New Token</strong></span> window.
								</p></div></div></li></ol></div></section><section class="section" id="proc_generating-splunk-token-cli"><div class="titlepage"><div><div><h4 class="title">6.2.2.2. Generating a Splunk token using the CLI</h4></div></div></div><p>
						Use the following procedure to create a bearer token for Splunk using the CLI.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed Splunk and created a username.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In your CLI, enter the following <code class="literal">CURL</code> command to enable token authentication, passing in your Splunk username and password:
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;username&gt;:&lt;password&gt; -X POST &lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;/services/admin/token-auth/tokens_auth -d disabled=false</pre></li><li class="listitem"><p class="simpara">
								Create a token by entering the following <code class="literal">CURL</code> command, passing in your Splunk username and password.
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;username&gt;:&lt;password&gt; -X POST &lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;/services/authorization/tokens?output_mode=json --data name=&lt;username&gt; --data audience=Users --data-urlencode expires_on=+30d</pre></li><li class="listitem">
								Save the generated bearer token.
							</li></ol></div></section></section><section class="section" id="proc_splunk-config"><div class="titlepage"><div><div><h3 class="title">6.2.3. Configuring Red Hat Quay to use Splunk</h3></div></div></div><p>
					Use the following procedure to configure Red Hat Quay to use Splunk.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed Splunk and created a username.
						</li><li class="listitem">
							You have generated a Splunk bearer token.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open your Red Hat Quay <code class="literal">config.yaml</code> file and add the following configuration fields:
						</p><pre class="programlisting language-yaml">---
LOGS_MODEL: splunk
LOGS_MODEL_CONFIG:
    producer: splunk
    splunk_config:
        host: http://&lt;user_name&gt;.remote.csb <span id="CO1-1"/><span class="callout">1</span>
        port: 8089 <span id="CO1-2"/><span class="callout">2</span>
        bearer_token: &lt;bearer_token&gt; <span id="CO1-3"/><span class="callout">3</span>
        url_scheme: &lt;http/https&gt; <span id="CO1-4"/><span class="callout">4</span>
        verify_ssl: False <span id="CO1-5"/><span class="callout">5</span>
        index_prefix: &lt;splunk_log_index_name&gt; <span id="CO1-6"/><span class="callout">6</span>
        ssl_ca_path: &lt;location_to_ssl-ca-cert.pem&gt; <span id="CO1-7"/><span class="callout">7</span>
---</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									String. The Splunk cluster endpoint.
								</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Integer. The Splunk management cluster endpoint port. Differs from the Splunk GUI hosted port. Can be found on the Splunk UI under <span class="strong strong"><strong>Settings</strong></span> → <span class="strong strong"><strong>Server Settings</strong></span> → <span class="strong strong"><strong>General Settings</strong></span>.
								</div></dd><dt><a href="#CO1-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									String. The generated bearer token for Splunk.
								</div></dd><dt><a href="#CO1-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									String. The URL scheme for access the Splunk service. If Splunk is configured to use TLS/SSL, this must be <code class="literal">https</code>.
								</div></dd><dt><a href="#CO1-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Boolean. Whether to enable TLS/SSL. Defaults to <code class="literal">true</code>.
								</div></dd><dt><a href="#CO1-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									String. The Splunk index prefix. Can be a new, or used, index. Can be created from the Splunk UI.
								</div></dd><dt><a href="#CO1-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									String. The relative container path to a single <code class="literal">.pem</code> file containing a certificate authority (CA) for TLS/SSL validation.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							If you are configuring <code class="literal">ssl_ca_path</code>, you must configure the SSL/TLS certificate so that Red Hat Quay will trust it.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									If you are using a standalone deployment of Red Hat Quay, SSL/TLS certificates can be provided by placing the certificate file inside of the <code class="literal">extra_ca_certs</code> directory, or inside of the relative container path and specified by <code class="literal">ssl_ca_path</code>.
								</li><li class="listitem"><p class="simpara">
									If you are using the Red Hat Quay Operator, create a config bundle secret, including the certificate authority (CA) of the Splunk server. For example:
								</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config_390.yaml --from-file extra_ca_cert_splunkserver.crt=./splunkserver.crt config-bundle-secret</pre><p class="simpara">
									Specify the <code class="literal">conf/stack/extra_ca_certs/splunkserver.crt</code> file in your <code class="literal">config.yaml</code>. For example:
								</p><pre class="programlisting language-yaml">LOGS_MODEL: splunk
LOGS_MODEL_CONFIG:
    producer: splunk
    splunk_config:
        host: ec2-12-345-67-891.us-east-2.compute.amazonaws.com
        port: 8089
        bearer_token: eyJra
        url_scheme: https
        verify_ssl: true
        index_prefix: quay123456
        ssl_ca_path: conf/stack/splunkserver.crt</pre></li></ol></div></li></ol></div></section><section class="section" id="proc_splunk-action-log"><div class="titlepage"><div><div><h3 class="title">6.2.4. Creating an action log</h3></div></div></div><p>
					Use the following procedure to create a user account that can forward action logs to Splunk.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You must use the Splunk UI to view Red Hat Quay action logs. At this time, viewing Splunk action logs on the Red Hat Quay <span class="strong strong"><strong>Usage Logs</strong></span> page is unsupported, and returns the following message: <code class="literal">Method not implemented. Splunk does not support log lookups</code>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed Splunk and created a username.
						</li><li class="listitem">
							You have generated a Splunk bearer token.
						</li><li class="listitem">
							You have configured your Red Hat Quay <code class="literal">config.yaml</code> file to enable Splunk.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your Red Hat Quay deployment.
						</li><li class="listitem">
							Click on the name of the organization that you will use to create an action log for Splunk.
						</li><li class="listitem">
							In the navigation pane, click <span class="strong strong"><strong>Robot Accounts</strong></span> → <span class="strong strong"><strong>Create Robot Account</strong></span>.
						</li><li class="listitem">
							When prompted, enter a name for the robot account, for example <code class="literal">spunkrobotaccount</code>, then click <span class="strong strong"><strong>Create robot account</strong></span>.
						</li><li class="listitem">
							On your browser, open the Splunk UI.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Search and Reporting</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the search bar, enter the name of your index, for example, <code class="literal">&lt;splunk_log_index_name&gt;</code> and press <span class="strong strong"><strong>Enter</strong></span>.
						</p><p class="simpara">
							The search results populate on the Splunk UI, showing information like <code class="literal">host</code>, <code class="literal">sourcetype</code>, etc. By clicking the <code class="literal">&gt;</code> arrow, you can see metadata for the logs, such as the <code class="literal">ip</code>, JSON metadata, and account name.
						</p></li></ol></div></section></section></section><section class="chapter" id="clair-vulnerability-scanner"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Clair for Red Hat Quay</h1></div></div></div><p>
			Clair v4 (Clair) is an open source application that leverages static code analyses for parsing image content and reporting vulnerabilities affecting the content. Clair is packaged with Red Hat Quay and can be used in both standalone and Operator deployments. It can be run in highly scalable configurations, where components can be scaled separately as appropriate for enterprise environments.
		</p><section class="section" id="clair-vulnerability-scanner-hosts"><div class="titlepage"><div><div><h2 class="title">7.1. Clair vulnerability databases</h2></div></div></div><p>
				Clair uses the following vulnerability databases to report for issues in your images:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Ubuntu Oval database
					</li><li class="listitem">
						Debian Security Tracker
					</li><li class="listitem">
						Red Hat Enterprise Linux (RHEL) Oval database
					</li><li class="listitem">
						SUSE Oval database
					</li><li class="listitem">
						Oracle Oval database
					</li><li class="listitem">
						Alpine SecDB database
					</li><li class="listitem">
						VMWare Photon OS database
					</li><li class="listitem">
						Amazon Web Services (AWS) UpdateInfo
					</li><li class="listitem"><p class="simpara">
						<a class="link" href="https://osv.dev/">Open Source Vulnerability (OSV) Database</a>
					</p><p class="simpara">
						Clair reports vulnerability and security information for <code class="literal">golang</code>, <code class="literal">java</code>, and <code class="literal">ruby</code> ecosystems through OSV.
					</p></li></ul></div><p>
				For information about how Clair does security mapping with the different databases, see <a class="link" href="https://quay.github.io/claircore/concepts/severity_mapping.html">ClairCore Severity Mapping</a>.
			</p></section><section class="section" id="clair-standalone-configure"><div class="titlepage"><div><div><h2 class="title">7.2. Setting up Clair on standalone Red Hat Quay deployments</h2></div></div></div><p>
				For standalone Red Hat Quay deployments, you can set up Clair manually.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your Red Hat Quay installation directory, create a new directory for the Clair database data:
					</p><pre class="programlisting language-terminal">$ mkdir /home/&lt;user-name&gt;/quay-poc/postgres-clairv4</pre></li><li class="listitem"><p class="simpara">
						Set the appropriate permissions for the <code class="literal">postgres-clairv4</code> file by entering the following command:
					</p><pre class="programlisting language-terminal">$ setfacl -m u:26:-wx /home/&lt;user-name&gt;/quay-poc/postgres-clairv4</pre></li><li class="listitem"><p class="simpara">
						Deploy a Clair Postgres database by entering the following command:
					</p><pre class="programlisting language-terminal">$ sudo podman run -d --name postgresql-clairv4 \
  -e POSTGRESQL_USER=clairuser \
  -e POSTGRESQL_PASSWORD=clairpass \
  -e POSTGRESQL_DATABASE=clair \
  -e POSTGRESQL_ADMIN_PASSWORD=adminpass \
  -p 5433:5433 \
  -v /home/&lt;user-name&gt;/quay-poc/postgres-clairv4:/var/lib/pgsql/data:Z \
  registry.redhat.io/rhel8/postgresql-13:1-109</pre></li><li class="listitem"><p class="simpara">
						Install the Postgres <code class="literal">uuid-ossp</code> module for your Clair deployment:
					</p><pre class="programlisting language-terminal">$ podman exec -it postgresql-clairv4 /bin/bash -c 'echo "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"" | psql -d clair -U postgres'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">CREATE EXTENSION</pre>
						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Clair requires the <code class="literal">uuid-ossp</code> extension to be added to its Postgres database. For users with proper privileges, creating the extension will automatically be added by Clair. If users do not have the proper privileges, the extension must be added before start Clair.
						</p><p>
							If the extension is not present, the following error will be displayed when Clair attempts to start: <code class="literal">ERROR: Please load the "uuid-ossp" extension. (SQLSTATE 42501)</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Stop the <code class="literal">Quay</code> container if it is running and restart it in configuration mode, loading the existing configuration as a volume:
					</p><pre class="programlisting language-terminal">$ sudo podman run --rm -it --name quay_config \
  -p 80:8080 -p 443:8443 \
  -v $QUAY/config:/conf/stack:Z \
  registry.redhat.io/quay/quay-rhel8:v3.8.2 config secret</pre></li><li class="listitem">
						Log in to the configuration tool and click <span class="strong strong"><strong>Enable Security Scanning</strong></span> in the <span class="strong strong"><strong>Security Scanner</strong></span> section of the UI.
					</li><li class="listitem">
						Set the HTTP endpoint for Clair using a port that is not already in use on the <code class="literal">quay-server</code> system, for example, <code class="literal">8081</code>.
					</li><li class="listitem"><p class="simpara">
						Create a pre-shared key (PSK) using the <span class="strong strong"><strong>Generate PSK</strong></span> button.
					</p><div class="formalpara"><p class="title"><strong>Security Scanner UI</strong></p><p>
							<span class="inlinemediaobject"><img src="images/poc-quay-scanner-config.png" alt="Security Scanner"/></span>
						</p></div></li><li class="listitem">
						Validate and download the <code class="literal">config.yaml</code> file for Red Hat Quay, and then stop the <code class="literal">Quay</code> container that is running the configuration editor.
					</li><li class="listitem"><p class="simpara">
						Extract the new configuration bundle into your Red Hat Quay installation directory, for example:
					</p><pre class="programlisting language-terminal">$ tar xvf quay-config.tar.gz -d /home/&lt;user-name&gt;/quay-poc/</pre></li><li class="listitem"><p class="simpara">
						Create a folder for your Clair configuration file, for example:
					</p><pre class="programlisting language-terminal">$ mkdir /etc/opt/clairv4/config/</pre></li><li class="listitem"><p class="simpara">
						Change into the Clair configuration folder:
					</p><pre class="programlisting language-terminal">$ cd /etc/opt/clairv4/config/</pre></li><li class="listitem"><p class="simpara">
						Create a Clair configuration file, for example:
					</p><pre class="programlisting language-yaml">http_listen_addr: :8081
introspection_addr: :8088
log_level: debug
indexer:
  connstring: host=quay-server.example.com port=5433 dbname=clair user=clairuser password=clairpass sslmode=disable
  scanlock_retry: 10
  layer_scan_concurrency: 5
  migrations: true
matcher:
  connstring: host=quay-server.example.com port=5433 dbname=clair user=clairuser password=clairpass sslmode=disable
  max_conn_pool: 100
  migrations: true
  indexer_addr: clair-indexer
notifier:
  connstring: host=quay-server.example.com port=5433 dbname=clair user=clairuser password=clairpass sslmode=disable
  delivery_interval: 1m
  poll_interval: 5m
  migrations: true
auth:
  psk:
    key: "MTU5YzA4Y2ZkNzJoMQ=="
    iss: ["quay"]
# tracing and metrics
trace:
  name: "jaeger"
  probability: 1
  jaeger:
    agent:
      endpoint: "localhost:6831"
    service_name: "clair"
metrics:
  name: "prometheus"</pre><p class="simpara">
						For more information about Clair’s configuration format, see <a class="link" href="https://quay.github.io/clair/reference/config.html">Clair configuration reference</a>.
					</p></li><li class="listitem"><p class="simpara">
						Start Clair by using the container image, mounting in the configuration from the file you created:
					</p><pre class="screen">$ sudo podman run -d --name clairv4 \
-p 8081:8081 -p 8088:8088 \
-e CLAIR_CONF=/clair/config.yaml \
-e CLAIR_MODE=combo \
-v /etc/opt/clairv4/config:/clair:Z \
registry.redhat.io/quay/clair-rhel8:v3.9.0</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Running multiple Clair containers is also possible, but for deployment scenarios beyond a single container the use of a container orchestrator like Kubernetes or OpenShift Container Platform is strongly recommended.
						</p></div></div></li></ol></div></section><section class="section" id="clair-quay-operator-overview"><div class="titlepage"><div><div><h2 class="title">7.3. Clair on OpenShift Container Platform</h2></div></div></div><p>
				To set up Clair v4 (Clair) on a Red Hat Quay deployment on OpenShift Container Platform, it is recommended to use the Red Hat Quay Operator. By default, the Red Hat Quay Operator will install or upgrade a Clair deployment along with your Red Hat Quay deployment and configure Clair automatically.
			</p></section><section class="section" id="clair-testing"><div class="titlepage"><div><div><h2 class="title">7.4. Testing Clair</h2></div></div></div><p>
				Use the following procedure to test Clair on either a standalone Red Hat Quay deployment, or on an OpenShift Container Platform Operator-based deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have deployed the Clair container image.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Pull a sample image by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman pull ubuntu:20.04</pre></li><li class="listitem"><p class="simpara">
						Tag the image to your registry by entering the following command:
					</p><pre class="programlisting language-terminal">$ sudo podman tag docker.io/library/ubuntu:20.04 &lt;quay-server.example.com&gt;/&lt;user-name&gt;/ubuntu:20.04</pre></li><li class="listitem"><p class="simpara">
						Push the image to your Red Hat Quay registry by entering the following command:
					</p><pre class="programlisting language-terminal">$ sudo podman push --tls-verify=false quay-server.example.com/quayadmin/ubuntu:20.04</pre></li><li class="listitem">
						Log in to your Red Hat Quay deployment through the UI.
					</li><li class="listitem">
						Click the repository name, for example, <span class="strong strong"><strong>quayadmin/ubuntu</strong></span>.
					</li><li class="listitem"><p class="simpara">
						In the navigation pane, click <span class="strong strong"><strong>Tags</strong></span>.
					</p><div class="formalpara"><p class="title"><strong>Report summary</strong></p><p>
							<span class="inlinemediaobject"><img src="images/clair-reposcan.png" alt="Security scan information appears for scanned repository images"/></span>
						</p></div></li><li class="listitem"><p class="simpara">
						Click the image report, for example, <span class="strong strong"><strong>45 medium</strong></span>, to show a more detailed report:
					</p><div class="formalpara"><p class="title"><strong>Report details</strong></p><p>
							<span class="inlinemediaobject"><img src="images/clair-vulnerabilities.png" alt="See all vulnerabilities or only those that are fixable"/></span>
						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							In some cases, Clair shows duplicate reports on images, for example, <code class="literal">ubi8/nodejs-12</code> or <code class="literal">ubi8/nodejs-16</code>. This occurs because vulnerabilities with same name are for different packages. This behavior is expected with Clair vulnerability reporting and will not be addressed as a bug.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="quay-bridge-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Integrating Red Hat Quay into OpenShift Container Platform with the Quay Bridge Operator</h1></div></div></div><p>
			The Quay Bridge Operator duplicates the features of the integrated OpenShift Container Platform registry into the new Red Hat Quay registry. Using the Quay Bridge Operator, you can replace the integrated container registry in OpenShift Container Platform with a Red Hat Quay registry.
		</p><p>
			The features enabled with the Quay Bridge Operator include:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Synchronizing OpenShift Container Platform namespaces as Red Hat Quay organizations.
				</li><li class="listitem">
					Creating robot accounts for each default namespace service account.
				</li><li class="listitem">
					Creating secrets for each created robot account, and associating each robot secret to a service account as <code class="literal">Mountable</code> and <code class="literal">Image Pull Secret</code>.
				</li><li class="listitem">
					Synchronizing OpenShift Container Platform image streams as Red Hat Quay repositories.
				</li><li class="listitem">
					Automatically rewriting new builds making use of image streams to output to Red Hat Quay.
				</li><li class="listitem">
					Automatically importing an image stream tag after a build completes.
				</li></ul></div><p>
			By using the following procedures, you can enable bi-directional communication between your Red Hat Quay and OpenShift Container Platform clusters.
		</p><section class="section" id="setting-up-quay-for-qbo"><div class="titlepage"><div><div><h2 class="title">8.1. Setting up Red Hat Quay for the Quay Bridge Operator</h2></div></div></div><p>
				In this procedure, you will create a dedicated Red Hat Quay organization, and from a new application created within that organization you will generate an OAuth token to be used with the Quay Bridge Operator in OpenShift Container Platform.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to Red Hat Quay through the web UI.
					</li><li class="listitem">
						Select the organization for which the external application will be configured.
					</li><li class="listitem">
						On the navigation pane, select <span class="strong strong"><strong>Applications</strong></span>.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Create New Application</strong></span> and enter a name for the new application, for example, <code class="literal">openshift</code>.
					</li><li class="listitem">
						On the <span class="strong strong"><strong>OAuth Applications</strong></span> page, select your application, for example, <code class="literal">openshift</code>.
					</li><li class="listitem">
						On the navigation pane, select <span class="strong strong"><strong>Generate Token</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Select the following fields:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Administer Organization</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Administer Repositories</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Create Repositories</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>View all visible repositories</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Read/Write to any accessible repositories</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Administer User</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Read User Information</strong></span>
							</li></ul></div></li><li class="listitem">
						Review the assigned permissions.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Authorize Application</strong></span> and then confirm confirm the authorization by selecting <span class="strong strong"><strong>Authorize Application</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Save the generated access token.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Red Hat Quay does not offer token management. You cannot list tokens, delete tokens, or modify tokens. The generated access token is only shown once and cannot be re-obtained after closing the page.
						</p></div></div></li></ol></div></section><section class="section" id="installing-qbo-on-ocp"><div class="titlepage"><div><div><h2 class="title">8.2. Installing the Quay Bridge Operator on OpenShift Container Platform</h2></div></div></div><p>
				In this procedure, you will install the Quay Bridge Operator on OpenShift Container Platform.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequiites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have set up Red Hat Quay and obtained an Access Token.
					</li><li class="listitem">
						An OpenShift Container Platform 4.6 or greater environment for which you have cluster administrator permissions.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console and navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span> on the navigation pane.
					</li><li class="listitem">
						Search for <code class="literal">Quay Bridge Operator</code>, click the <span class="strong strong"><strong>Quay Bridge Operator</strong></span> title, and then click <span class="strong strong"><strong>Install</strong></span>.
					</li><li class="listitem">
						Select the version to install, for example, <span class="strong strong"><strong>stable-3.7</strong></span>, and then click <span class="strong strong"><strong>Install</strong></span>.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>View Operator</strong></span> when the installation finishes to go to the Quay Bridge Operator’s <span class="strong strong"><strong>Details</strong></span> page. Alternatively, you can click <span class="strong strong"><strong>Installed Operators</strong></span> → <span class="strong strong"><strong>Red Hat Quay Bridge Operator</strong></span> to go to the <span class="strong strong"><strong>Details</strong></span> page.
					</li></ol></div></section><section class="section" id="creating-ocp-secret-for-oauth-token"><div class="titlepage"><div><div><h2 class="title">8.3. Creating an OpenShift Container Platform secret for the OAuth token</h2></div></div></div><p>
				In this procedure, you will add the previously obtained access token to communicate with your Red Hat Quay deployment. The access token will be stored within OpenShift Container Platform as a secret.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have set up Red Hat Quay and obtained an access token.
					</li><li class="listitem">
						You have deployed the Quay Bridge Operator on OpenShift Container Platform.
					</li><li class="listitem">
						An OpenShift Container Platform 4.6 or greater environment for which you have cluster administrator permissions.
					</li><li class="listitem">
						You have installed the OpenShift CLI (oc).
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Create a secret that contains the access token in the <code class="literal">openshift-operators</code> namespace:
					</p><pre class="programlisting language-terminal">$ oc create secret -n openshift-operators generic &lt;secret-name&gt; --from-literal=token=&lt;access_token&gt;</pre></li></ul></div></section><section class="section" id="creating-quay-integration-cr"><div class="titlepage"><div><div><h2 class="title">8.4. Creating the QuayIntegration custom resource</h2></div></div></div><p>
				In this procedure, you will create a <code class="literal">QuayIntegration</code> custom resource, which can be completed from either the web console or from the command line.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have set up Red Hat Quay and obtained an access token.
					</li><li class="listitem">
						You have deployed the Quay Bridge Operator on OpenShift Container Platform.
					</li><li class="listitem">
						An OpenShift Container Platform 4.6 or greater environment for which you have cluster administrator permissions.
					</li><li class="listitem">
						Optional: You have installed the OpenShift CLI (oc).
					</li></ul></div><section class="section" id="optional_creating_the_quayintegration_custom_resource_using_the_cli"><div class="titlepage"><div><div><h3 class="title">8.4.1. Optional: Creating the QuayIntegration custom resource using the CLI</h3></div></div></div><p>
					Follow this procedure to create the <code class="literal">QuayIntegration</code> custom resource using the command line.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">quay-integration.yaml</code>:
						</p><pre class="screen">$ touch quay-integration.yaml</pre></li><li class="listitem"><p class="simpara">
							Use the following configuration for a minimal deployment of the <code class="literal">QuayIntegration</code> custom resource:
						</p><pre class="programlisting language-yaml">  apiVersion: quay.redhat.com/v1
  kind: QuayIntegration
  metadata:
    name: example-quayintegration
  spec:
    clusterID: openshift  <span id="CO2-1"/><span class="callout">1</span>
    credentialsSecret:
      namespace: openshift-operators
      name: quay-integration<span id="CO2-2"/><span class="callout">2</span>
    quayHostname: https://&lt;QUAY_URL&gt;   <span id="CO2-3"/><span class="callout">3</span>
    insecureRegistry: false <span id="CO2-4"/><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The clusterID value should be unique across the entire ecosystem. This value is required and defaults to <code class="literal">openshift</code>.
								</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">credentialsSecret</code> property refers to the namespace and name of the secret containing the token that was previously created.
								</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Replace the <code class="literal">QUAY_URL</code> with the hostname of your Red Hat Quay instance.
								</div></dd><dt><a href="#CO2-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									If Red Hat Quay is using self signed certificates, set the property to <code class="literal">insecureRegistry: true</code>.
								</div></dd></dl></div><p class="simpara">
							For a list of all configuration fields, see "QuayIntegration configuration fields".
						</p></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">QuayIntegration</code> custom resource:
						</p><pre class="screen">$ oc create -f quay-integration.yaml</pre></li></ol></div><section class="section" id="optional_creating_the_quayintegration_custom_resource_using_the_web_console"><div class="titlepage"><div><div><h4 class="title">8.4.1.1. Optional: Creating the QuayIntegration custom resource using the web console</h4></div></div></div><p>
						Follow this procedure to create the <code class="literal">QuayIntegration</code> custom resource using the web console.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Open the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console and navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Red Hat Quay Bridge Operator</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Details</strong></span> page of the Quay Bridge Operator, click <span class="strong strong"><strong>Create Instance</strong></span> on the <span class="strong strong"><strong>Quay Integration</strong></span> API card.
							</li><li class="listitem"><p class="simpara">
								On the <span class="strong strong"><strong>Create QuayIntegration</strong></span> page, enter the following required information in either <span class="strong strong"><strong>Form view</strong></span> or <span class="strong strong"><strong>YAML view</strong></span>:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<span class="strong strong"><strong>Name</strong></span>: The name that will refer to the <code class="literal">QuayIntegration</code> custom resource object.
									</li><li class="listitem">
										<span class="strong strong"><strong>Cluster ID</strong></span>: The ID associated with this cluster. This value should be unique across the entire ecosystem. Defaults to <code class="literal">openshift</code> if left unspecified.
									</li><li class="listitem">
										<span class="strong strong"><strong>Credentials secret</strong></span>: Refers to the namespace and name of the secret containing the token that was previously created.
									</li><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong>Quay hostname</strong></span>: The hostname of the Quay registry.
									</p><p class="simpara">
										For a list of all configuration fields, see "QuayIntegration configuration fields".
									</p></li></ul></div></li></ol></div><p>
						After the <code class="literal">QuayIntegration</code> custom resource is created, your OpenShift Container Platform cluster will be linked to your Red Hat Quay instance. Organizations within your Red Hat Quay registry should be created for the related namespace for the OpenShift Container Platform environment.
					</p></section></section></section><section class="section" id="quay-integration-config-fields"><div class="titlepage"><div><div><h2 class="title">8.5. QuayIntegration configuration fields</h2></div></div></div><p>
				The following configuration fields are available for the QuayIntegration custom resource:
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 25%; " class="col_2"/><col style="width: 25%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621344043600" scope="col">Name</th><th align="left" valign="top" id="idm45621344444496" scope="col">Description</th><th align="left" valign="top" id="idm45621344443408" scope="col">Schema</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								allowlistNamespaces<br/> (Optional)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								A list of namespaces to include.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								Array
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								clusterID<br/> (Required)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								The ID associated with this cluster.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								credentialsSecret.key<br/> (Required)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								The secret containing credentials to communicate with the Quay registry.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								Object
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								denylistNamespaces<br/> (Optional)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								A list of namespaces to exclude.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								Array
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								insecureRegistry<br/> (Optional)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								Whether to skip TLS verification to the Quay registry
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								Boolean
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								quayHostname<br/> (Required)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								The hostname of the Quay registry.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344043600">
							<p>
								scheduledImageStreamImport<br/> (Optional)
							</p>
							</td><td align="left" valign="top" headers="idm45621344444496">
							<p>
								Whether to enable image stream importing.
							</p>
							</td><td align="left" valign="top" headers="idm45621344443408">
							<p>
								Boolean
							</p>
							</td></tr></tbody></table></div></section></section><section class="chapter" id="repo-mirroring-in-red-hat-quay"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Repository mirroring</h1></div></div></div><section class="section" id="arch-mirroring-intro"><div class="titlepage"><div><div><h2 class="title">9.1. Repository mirroring</h2></div></div></div><p>
				Red Hat Quay repository mirroring lets you mirror images from external container registries, or another local registry, into your Red Hat Quay cluster. Using repository mirroring, you can synchronize images to Red Hat Quay based on repository names and tags.
			</p><p>
				From your Red Hat Quay cluster with repository mirroring enabled, you can perform the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Choose a repository from an external registry to mirror
					</li><li class="listitem">
						Add credentials to access the external registry
					</li><li class="listitem">
						Identify specific container image repository names and tags to sync
					</li><li class="listitem">
						Set intervals at which a repository is synced
					</li><li class="listitem">
						Check the current state of synchronization
					</li></ul></div><p>
				To use the mirroring functionality, you need to perform the following actions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Enable repository mirroring in the Red Hat Quay configuration file
					</li><li class="listitem">
						Run a repository mirroring worker
					</li><li class="listitem">
						Create mirrored repositories
					</li></ul></div><p>
				All repository mirroring configurations can be performed using the configuration tool UI or by the Red Hat Quay API.
			</p></section><section class="section" id="mirroring-versus-georepl"><div class="titlepage"><div><div><h2 class="title">9.2. Repository mirroring compared to geo-replication</h2></div></div></div><p>
				Red Hat Quay geo-replication mirrors the entire image storage backend data between 2 or more different storage backends while the database is shared, for example, one Red Hat Quay registry with two different blob storage endpoints. The primary use cases for geo-replication include the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Speeding up access to the binary blobs for geographically dispersed setups
					</li><li class="listitem">
						Guaranteeing that the image content is the same across regions
					</li></ul></div><p>
				Repository mirroring synchronizes selected repositories, or subsets of repositories, from one registry to another. The registries are distinct, with each registry having a separate database and separate image storage.
			</p><p>
				The primary use cases for mirroring include the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Independent registry deployments in different data centers or regions, where a certain subset of the overall content is supposed to be shared across the data centers and regions
					</li><li class="listitem">
						Automatic synchronization or mirroring of selected (allowlisted) upstream repositories from external registries into a local Red Hat Quay deployment
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Repository mirroring and geo-replication can be used simultaneously.
				</p></div></div><div class="table" id="idm45621345725152"><p class="title"><strong>Table 9.1. Red Hat Quay Repository mirroring and geo-replication comparison</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 33%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621345718608" scope="col">Feature / Capability</th><th align="left" valign="top" id="idm45621344681648" scope="col">Geo-replication</th><th align="left" valign="top" id="idm45621344680560" scope="col">Repository mirroring</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								What is the feature designed to do?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								A shared, global registry
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								Distinct, different registries
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								What happens if replication or mirroring has not been completed yet?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								The remote copy is used (slower)
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								No image is served
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Is access to all storage backends in both regions required?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								Yes (all Red Hat Quay nodes)
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								No (distinct storage)
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Can users push images from both sites to the same repository?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Is all registry content and configuration identical across all regions (shared database)?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Can users select individual namespaces or repositories to be mirrored?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								Yes
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Can users apply filters to synchronization rules?
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								Yes
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621345718608">
							<p>
								Are individual / different role-base access control configurations allowed in each region
							</p>
							</td><td align="left" valign="top" headers="idm45621344681648">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45621344680560">
							<p>
								Yes
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="arch-mirroring-using"><div class="titlepage"><div><div><h2 class="title">9.3. Using repository mirroring</h2></div></div></div><p>
				The following list shows features and limitations of Red Hat Quay repository mirroring:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						With repository mirroring, you can mirror an entire repository or selectively limit which images are synced. Filters can be based on a comma-separated list of tags, a range of tags, or other means of identifying tags through Unix shell-style wildcards. For more information, see the documentation for <a class="link" href="https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm">wildcards</a>.
					</li><li class="listitem">
						When a repository is set as mirrored, you cannot manually add other images to that repository.
					</li><li class="listitem">
						Because the mirrored repository is based on the repository and tags you set, it will hold only the content represented by the repository and tag pair. For example if you change the tag so that some images in the repository no longer match, those images will be deleted.
					</li><li class="listitem">
						Only the designated robot can push images to a mirrored repository, superseding any role-based access control permissions set on the repository.
					</li><li class="listitem">
						Mirroring can be configured to rollback on failure, <span class="emphasis"><em>or</em></span> to run on a best-effort basis.
					</li><li class="listitem">
						With a mirrored repository, a user with <span class="emphasis"><em>read</em></span> permissions can pull images from the repository but cannot push images to the repository.
					</li><li class="listitem">
						Changing settings on your mirrored repository can be performed in the Red Hat Quay user interface, using the <span class="strong strong"><strong>Repositories</strong></span> → <span class="strong strong"><strong>Mirrors</strong></span> tab for the mirrored repository you create.
					</li><li class="listitem">
						Images are synced at set intervals, but can also be synced on demand.
					</li></ul></div></section><section class="section" id="mirroring_configuration_ui"><div class="titlepage"><div><div><h2 class="title">9.4. Mirroring configuration UI</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Start the <code class="literal">Quay</code> container in configuration mode and select the Enable Repository Mirroring check box. If you want to require HTTPS communications and verify certificates during mirroring, select the HTTPS and cert verification check box.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/repo_mirror_config.png" alt="Enable mirroring and require HTTPS and verified certificates"/></span>
					</p></li><li class="listitem">
						Validate and download the <code class="literal">configuration</code> file, and then restart Quay in registry mode using the updated config file.
					</li></ol></div></section><section class="section" id="config-fields-mirroring"><div class="titlepage"><div><div><h2 class="title">9.5. Mirroring configuration fields</h2></div></div></div><div class="table" id="idm45621344036320"><p class="title"><strong>Table 9.2. Mirroring configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 17%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621344030560" scope="col">Field</th><th align="left" valign="top" id="idm45621344296336" scope="col">Type</th><th align="left" valign="top" id="idm45621344295248" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621344030560">
							<p>
								<span class="strong strong"><strong>FEATURE_REPO_MIRROR</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621344296336">
							<p>
								Boolean
							</p>
							</td><td align="left" valign="top" headers="idm45621344295248">
							<p>
								Enable or disable repository mirroring<br/><br/> <span class="strong strong"><strong>Default:</strong></span> <code class="literal">false</code>
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344030560">
							<p>
								<span class="strong strong"><strong>REPO_MIRROR_INTERVAL</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621344296336">
							<p>
								Number
							</p>
							</td><td align="left" valign="top" headers="idm45621344295248">
							<p>
								The number of seconds between checking for repository mirror candidates<br/><br/><span class="strong strong"><strong>Default:</strong></span> 30
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344030560">
							<p>
								<span class="strong strong"><strong>REPO_MIRROR_SERVER_HOSTNAME</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621344296336">
							<p>
								String
							</p>
							</td><td align="left" valign="top" headers="idm45621344295248">
							<p>
								Replaces the <code class="literal">SERVER_HOSTNAME</code> as the destination for mirroring. <br/><br/><span class="strong strong"><strong>Default:</strong></span> None<br/><br/><span class="strong strong"><strong>Example</strong></span>:<br/><code class="literal">openshift-quay-service</code>
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344030560">
							<p>
								<span class="strong strong"><strong>REPO_MIRROR_TLS_VERIFY</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621344296336">
							<p>
								Boolean
							</p>
							</td><td align="left" valign="top" headers="idm45621344295248">
							<p>
								Require HTTPS and verify certificates of Quay registry during mirror.<br/><br/> <span class="strong strong"><strong>Default:</strong></span> <code class="literal">false</code>
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621344030560">
							<p>
								<span class="strong strong"><strong>REPO_MIRROR_ROLLBACK</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621344296336">
							<p>
								Boolean
							</p>
							</td><td align="left" valign="top" headers="idm45621344295248">
							<p>
								When set to <code class="literal">true</code>, the repository rolls back after a failed mirror attempt.
							</p>
							<p>
								<span class="strong strong"><strong>Default</strong></span>: <code class="literal">false</code>
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="mirroring-worker"><div class="titlepage"><div><div><h2 class="title">9.6. Mirroring worker</h2></div></div></div><p>
				Use the following procedure to start the repository mirroring worker.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						If you have not configured TLS communications using a <code class="literal">/root/ca.crt</code> certificate, enter the following command to start a <code class="literal">Quay</code> pod with the <code class="literal">repomirror</code> option:
					</p><pre class="screen">$ sudo podman run -d --name mirroring-worker \
  -v $QUAY/config:/conf/stack:Z \
  registry.redhat.io/quay/quay-rhel8:v3.9.0 repomirror</pre></li><li class="listitem"><p class="simpara">
						If you have configured TLS communications using a <code class="literal">/root/ca.crt</code> certificate, enter the following command to start the repository mirroring worker:
					</p><pre class="screen">$ sudo podman run -d --name mirroring-worker \
  -v $QUAY/config:/conf/stack:Z \
  -v /root/ca.crt:/etc/pki/ca-trust/source/anchors/ca.crt:Z \
  registry.redhat.io/quay/quay-rhel8:v3.9.0 repomirror</pre></li></ul></div></section><section class="section" id="mirroring-creating-repo"><div class="titlepage"><div><div><h2 class="title">9.7. Creating a mirrored repository</h2></div></div></div><p>
				When mirroring a repository from an external container registry, you must create a new private repository. Typically, the same name is used as the target repository, for example, <code class="literal">quay-rhel8</code>.
			</p><p>
				<span class="inlinemediaobject"><img src="images/repo_quay_rhel8.png" alt="Create new Red Hat Quay repo"/></span>
			</p><section class="section" id="mirroring-repository-mirroring-settings"><div class="titlepage"><div><div><h3 class="title">9.7.1. Repository mirroring settings</h3></div></div></div><p>
					Use the following procedure to adjust the settings of your mirrored repository.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have enabled repository mirroring in your Red Hat Quay configuration file.
						</li><li class="listitem">
							You have deployed a mirroring worker.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the Settings tab, set the Repository State to <code class="literal">Mirror</code>:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/repo_mirror_create.png" alt="Create a new Red Hat Quay repo mirror"/></span>
						</p></li><li class="listitem"><p class="simpara">
							In the Mirror tab, enter the details for connecting to the external registry, along with the tags, scheduling and access information:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/repo-mirror-details-start.png" alt="Repository mirroring"/></span>
						</p></li><li class="listitem"><p class="simpara">
							Enter the details as required in the following fields:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Registry Location:</strong></span> The external repository you want to mirror, for example, <code class="literal">registry.redhat.io/quay/quay-rhel8</code>
								</li><li class="listitem">
									<span class="strong strong"><strong>Tags:</strong></span> This field is required. You may enter a comma-separated list of individual tags or tag patterns. (See <span class="emphasis"><em>Tag Patterns</em></span> section for details.)
								</li><li class="listitem">
									<span class="strong strong"><strong>Start Date:</strong></span> The date on which mirroring begins. The current date and time is used by default.
								</li><li class="listitem">
									<span class="strong strong"><strong>Sync Interval:</strong></span> Defaults to syncing every 24 hours. You can change that based on hours or days.
								</li><li class="listitem">
									<span class="strong strong"><strong>Robot User:</strong></span> Create a new robot account or choose an existing robot account to do the mirroring.
								</li><li class="listitem">
									<span class="strong strong"><strong>Username:</strong></span> The username for accessing the external registry holding the repository you are mirroring.
								</li><li class="listitem">
									<span class="strong strong"><strong>Password:</strong></span> The password associated with the Username. Note that the password cannot include characters that require an escape character (\).
								</li></ul></div></li></ol></div></section><section class="section" id="mirroring-advanced-settings"><div class="titlepage"><div><div><h3 class="title">9.7.2. Advanced settings</h3></div></div></div><p>
					In the <span class="strong strong"><strong>Advanced Settings</strong></span> section, you can configure SSL/TLS and proxy with the following options:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Verify TLS:</strong></span> Select this option if you want to require HTTPS and to verify certificates when communicating with the target remote registry.
						</li><li class="listitem">
							<span class="strong strong"><strong>Accept Unsigned Images:</strong></span> Selecting this option allows unsigned images to be mirrored.
						</li><li class="listitem">
							<span class="strong strong"><strong>HTTP Proxy:</strong></span> Select this option if you want to require HTTPS and to verify certificates when communicating with the target remote registry.
						</li><li class="listitem">
							<span class="strong strong"><strong>HTTPS PROXY:</strong></span> Identify the HTTPS proxy server needed to access the remote site, if a proxy server is needed.
						</li><li class="listitem">
							<span class="strong strong"><strong>No Proxy:</strong></span> List of locations that do not require proxy.
						</li></ul></div></section><section class="section" id="mirroring-synchronize-now"><div class="titlepage"><div><div><h3 class="title">9.7.3. Synchronize now</h3></div></div></div><p>
					Use the following procedure to initiate the mirroring operation.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To perform an immediate mirroring operation, press the Sync Now button on the repository’s Mirroring tab. The logs are available on the Usage Logs tab:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/repo-mirror-usage-logs.png" alt="Usage logs"/></span>
						</p><p class="simpara">
							When the mirroring is complete, the images will appear in the Tags tab:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/repo-mirror-tags.png" alt="Repository mirroring tags"/></span>
						</p><p class="simpara">
							Below is an example of a completed Repository Mirroring screen:
						</p><p class="simpara">
							<span class="inlinemediaobject"><img src="images/repo-mirror-details.png" alt="Repository mirroring details"/></span>
						</p></li></ul></div></section></section><section class="section" id="arch-mirroring-events"><div class="titlepage"><div><div><h2 class="title">9.8. Event notifications for mirroring</h2></div></div></div><p>
				There are three notification events for repository mirroring:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Repository Mirror Started
					</li><li class="listitem">
						Repository Mirror Success
					</li><li class="listitem">
						Repository Mirror Unsuccessful
					</li></ul></div><p>
				The events can be configured inside of the <span class="strong strong"><strong>Settings</strong></span> tab for each repository, and all existing notification methods such as email, Slack, Quay UI, and webhooks are supported.
			</p></section><section class="section" id="mirroring-tag-patterns"><div class="titlepage"><div><div><h2 class="title">9.9. Mirroring tag patterns</h2></div></div></div><p>
				At least one tag must be entered. The following table references possible image tag patterns.
			</p><section class="section" id="pattern_syntax"><div class="titlepage"><div><div><h3 class="title">9.9.1. Pattern syntax</h3></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><tbody><tr><td align="left" valign="top">
								<p>
									Pattern
								</p>
								</td><td align="left" valign="top">
								<p>
									Description
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									*
								</p>
								</td><td align="left" valign="top">
								<p>
									Matches all characters
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									?
								</p>
								</td><td align="left" valign="top">
								<p>
									Matches any single character
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									[seq]
								</p>
								</td><td align="left" valign="top">
								<p>
									Matches any character in <span class="emphasis"><em>seq</em></span>
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									[!seq]
								</p>
								</td><td align="left" valign="top">
								<p>
									Matches any character not in <span class="emphasis"><em>seq</em></span>
								</p>
								</td></tr></tbody></table></div></section><section class="section" id="example_tag_patterns"><div class="titlepage"><div><div><h3 class="title">9.9.2. Example tag patterns</h3></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><tbody><tr><td align="left" valign="top">
								<p>
									Example Pattern
								</p>
								</td><td align="left" valign="top">
								<p>
									Example Matches
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3*
								</p>
								</td><td align="left" valign="top">
								<p>
									v32, v3.1, v3.2, v3.2-4beta, v3.3
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3.*
								</p>
								</td><td align="left" valign="top">
								<p>
									v3.1, v3.2, v3.2-4beta
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3.?
								</p>
								</td><td align="left" valign="top">
								<p>
									v3.1, v3.2, v3.3
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3.[12]
								</p>
								</td><td align="left" valign="top">
								<p>
									v3.1, v3.2
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3.[12]*
								</p>
								</td><td align="left" valign="top">
								<p>
									v3.1, v3.2, v3.2-4beta
								</p>
								</td></tr><tr><td align="left" valign="top">
								<p>
									v3.[!1]*
								</p>
								</td><td align="left" valign="top">
								<p>
									v3.2, v3.2-4beta, v3.3
								</p>
								</td></tr></tbody></table></div></section></section><section class="section" id="mirroring-working-with"><div class="titlepage"><div><div><h2 class="title">9.10. Working with mirrored repositories</h2></div></div></div><p>
				Once you have created a mirrored repository, there are several ways you can work with that repository. Select your mirrored repository from the Repositories page and do any of the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Enable/disable the repository</strong></span>: Select the Mirroring button in the left column, then toggle the Enabled check box to enable or disable the repository temporarily.
					</li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Check mirror logs</strong></span>: To make sure the mirrored repository is working properly, you can check the mirror logs. To do that, select the Usage Logs button in the left column. Here’s an example:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/repo_mirror_logs.png" alt="View logs for your Red Hat Quay repo mirror"/></span>
					</p></li><li class="listitem">
						<span class="strong strong"><strong>Sync mirror now</strong></span>: To immediately sync the images in your repository, select the Sync Now button.
					</li><li class="listitem">
						<span class="strong strong"><strong>Change credentials</strong></span>: To change the username and password, select DELETE from the Credentials line. Then select None and add the username and password needed to log into the external registry when prompted.
					</li><li class="listitem">
						<span class="strong strong"><strong>Cancel mirroring</strong></span>: To stop mirroring, which keeps the current images available but stops new ones from being synced, select the CANCEL button.
					</li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Set robot permissions</strong></span>: Red Hat Quay robot accounts are named tokens that hold credentials for accessing external repositories. By assigning credentials to a robot, that robot can be used across multiple mirrored repositories that need to access the same external registry.
					</p><p class="simpara">
						You can assign an existing robot to a repository by going to Account Settings, then selecting the Robot Accounts icon in the left column. For the robot account, choose the link under the REPOSITORIES column. From the pop-up window, you can:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Check which repositories are assigned to that robot.
							</li><li class="listitem">
								Assign read, write or Admin privileges to that robot from the PERMISSION field shown in this figure: 
								<span class="inlinemediaobject"><img src="images/repo_mirror_robot_assign.png" alt="Assign a robot to mirrored repo"/></span>
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Change robot credentials</strong></span>: Robots can hold credentials such as Kubernetes secrets, Docker login information, and Mesos bundles. To change robot credentials, select the Options gear on the robot’s account line on the Robot Accounts window and choose View Credentials. Add the appropriate credentials for the external repository the robot needs to access.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/repo_mirror_robot_perm.png" alt="Assign permission to a robot"/></span>
					</p></li><li class="listitem">
						<span class="strong strong"><strong>Check and change general setting</strong></span>: Select the Settings button (gear icon) from the left column on the mirrored repository page. On the resulting page, you can change settings associated with the mirrored repository. In particular, you can change User and Robot Permissions, to specify exactly which users and robots can read from or write to the repo.
					</li></ul></div></section><section class="section" id="arch-mirroring-recommend"><div class="titlepage"><div><div><h2 class="title">9.11. Repository mirroring recommendations</h2></div></div></div><p>
				Best practices for repository mirroring include the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Repository mirroring pods can run on any node. This means that you can run mirroring on nodes where Red Hat Quay is already running.
					</li><li class="listitem">
						Repository mirroring is scheduled in the database and runs in batches. As a result, repository workers check each repository mirror configuration file and reads when the next sync needs to be. More mirror workers means more repositories can be mirrored at the same time. For example, running 10 mirror workers means that a user can run 10 mirroring operators in parallel. If a user only has 2 workers with 10 mirror configurations, only 2 operators can be performed.
					</li><li class="listitem"><p class="simpara">
						The optimal number of mirroring pods depends on the following conditions:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								The total number of repositories to be mirrored
							</li><li class="listitem">
								The number of images and tags in the repositories and the frequency of changes
							</li><li class="listitem"><p class="simpara">
								Parallel batching
							</p><p class="simpara">
								For example, if a user is mirroring a repository that has 100 tags, the mirror will be completed by one worker. Users must consider how many repositories one wants to mirror in parallel, and base the number of workers around that.
							</p><p class="simpara">
								Multiple tags in the same repository cannot be mirrored in parallel.
							</p></li></ul></div></li></ul></div></section></section><section class="chapter" id="proc_manage-ipv6-dual-stack"><div class="titlepage"><div><div><h1 class="title">Chapter 10. IPv6 and dual-stack deployments</h1></div></div></div><p>
			Your standalone Red Hat Quay deployment can now be served in locations that only support IPv6, such as Telco and Edge environments. Support is also offered for dual-stack networking so your Red Hat Quay deployment can listen on IPv4 and IPv6 simultaneously.
		</p><p>
			For a list of known limitations, see <a class="link" href="#proc_manage-ipv6-limitations-38" title="10.3. IPv6 and dua-stack limitations">IPv6 limitations</a>
		</p><section class="section" id="proc-manage-enabling-ipv6"><div class="titlepage"><div><div><h2 class="title">10.1. Enabling the IPv6 protocol family</h2></div></div></div><p>
				Use the following procedure to enable IPv6 support on your standalone Red Hat Quay deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have updated Red Hat Quay to 3.8.
					</li><li class="listitem">
						Your host and container software platform (Docker, Podman) must be configured to support IPv6.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">FEATURE_LISTEN_IP_VERSION</code> parameter and set it to <code class="literal">IPv6</code>, for example:
					</p><pre class="programlisting language-yaml">---
FEATURE_GOOGLE_LOGIN: false
FEATURE_INVITE_ONLY_USER_CREATION: false
FEATURE_LISTEN_IP_VERSION: IPv6
FEATURE_MAILING: false
FEATURE_NONSUPERUSER_TEAM_SYNCING_SETUP: false
---</pre></li><li class="listitem">
						Start, or restart, your Red Hat Quay deployment.
					</li><li class="listitem"><p class="simpara">
						Check that your deployment is listening to IPv6 by entering the following command:
					</p><pre class="programlisting language-terminal">$ curl &lt;quay_endpoint&gt;/health/instance
{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</pre></li></ol></div><p>
				After enabling IPv6 in your deployment’s <code class="literal">config.yaml</code>, all Red Hat Quay features can be used as normal, so long as your environment is configured to use IPv6 and is not hindered by the ipv6-limitations[current limitations].
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					If your environment is configured to IPv4, but the <code class="literal">FEATURE_LISTEN_IP_VERSION</code> configuration field is set to <code class="literal">IPv6</code>, Red Hat Quay will fail to deploy.
				</p></div></div></section><section class="section" id="proc-manageenabling-dual-stack"><div class="titlepage"><div><div><h2 class="title">10.2. Enabling the dual-stack protocol family</h2></div></div></div><p>
				Use the following procedure to enable dual-stack (IPv4 and IPv6) support on your standalone Red Hat Quay deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have updated Red Hat Quay to 3.8.
					</li><li class="listitem">
						Your host and container software platform (Docker, Podman) must be configured to support IPv6.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">FEATURE_LISTEN_IP_VERSION</code> parameter and set it to <code class="literal">dual-stack</code>, for example:
					</p><pre class="programlisting language-yaml">---
FEATURE_GOOGLE_LOGIN: false
FEATURE_INVITE_ONLY_USER_CREATION: false
FEATURE_LISTEN_IP_VERSION: dual-stack
FEATURE_MAILING: false
FEATURE_NONSUPERUSER_TEAM_SYNCING_SETUP: false
---</pre></li><li class="listitem">
						Start, or restart, your Red Hat Quay deployment.
					</li><li class="listitem"><p class="simpara">
						Check that your deployment is listening to both channels by entering the following command:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								For IPv4, enter the following command:
							</p><pre class="programlisting language-terminal">$ curl --ipv4 &lt;quay_endpoint&gt;
{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</pre></li><li class="listitem"><p class="simpara">
								For IPv6, enter the following command:
							</p><pre class="programlisting language-terminal">$ curl --ipv6 &lt;quay_endpoint&gt;
{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</pre></li></ol></div></li></ol></div><p>
				After enabling dual-stack in your deployment’s <code class="literal">config.yaml</code>, all Red Hat Quay features can be used as normal, so long as your environment is configured for dual-stack.
			</p></section><section class="section" id="proc_manage-ipv6-limitations-38"><div class="titlepage"><div><div><h2 class="title">10.3. IPv6 and dua-stack limitations</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Currently, attempting to configure your Red Hat Quay deployment with the common Azure Blob Storage configuration will not work on IPv6 single stack environments. Because the endpoint of Azure Blob Storage does not support IPv6, there is no workaround in place for this issue.
					</p><p class="simpara">
						For more information, see <a class="link" href="https://issues.redhat.com/browse/PROJQUAY-4433">PROJQUAY-4433</a>.
					</p></li><li class="listitem"><p class="simpara">
						Currently, attempting to configure your Red Hat Quay deployment with Amazon S3 CloudFront will not work on IPv6 single stack environments. Because the endpoint of Amazon S3 CloudFront does not support IPv6, there is no workaround in place for this issue.
					</p><p class="simpara">
						For more information, see <a class="link" href="https://issues.redhat.com/browse/PROJQUAY-4470">PROJQUAY-4470</a>.
					</p></li></ul></div></section></section><section class="chapter" id="ldap-authentication-setup-for-quay-enterprise"><div class="titlepage"><div><div><h1 class="title">Chapter 11. LDAP Authentication Setup for Red Hat Quay</h1></div></div></div><p>
			Lightweight Directory Access Protocol (LDAP) is an open, vendor-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an Internet Protocol (IP) network. Red Hat Quay supports using LDAP as an identity provider.
		</p><section class="section" id="ldap-considerations"><div class="titlepage"><div><div><h2 class="title">11.1. Considerations when enabling LDAP</h2></div></div></div><p>
				Prior to enabling LDAP for your Red Hat Quay deployment, you should consider the following.
			</p><h4 id="existing-quay-deployments">Existing Red Hat Quay deployments</h4><p>
				Conflicts between usernames can arise when you enable LDAP for an existing Red Hat Quay deployment that already has users configured. For example, one user, <code class="literal">alice</code>, was manually created in Red Hat Quay prior to enabling LDAP. If the username <code class="literal">alice</code> also exists in the LDAP directory, Red Hat Quay automatically creates a new user, <code class="literal">alice-1</code>, when <code class="literal">alice</code> logs in for the first time using LDAP. Red Hat Quay then automatically maps the LDAP credentials to the <code class="literal">alice</code> account. For consistency reasons, this might be erroneous for your Red Hat Quay deployment. It is recommended that you remove any potentially conflicting local account names from Red Hat Quay prior to enabling LDAP.
			</p><h4 id="considerations-for-manual-user-creation">Manual User Creation and LDAP authentication</h4><p>
				When Red Hat Quay is configured for LDAP, LDAP-authenticated users are automatically created in Red Hat Quay’s database on first log in, if the configuration option <code class="literal">FEATURE_USER_CREATION</code> is set to <code class="literal">true</code>. If this option is set to <code class="literal">false</code>, the automatic user creation for LDAP users fails, and the user is not allowed to log in. In this scenario, the superuser needs to create the desired user account first. Conversely, if <code class="literal">FEATURE_USER_CREATION</code> is set to <code class="literal">true</code>, this also means that a user can still create an account from the Red Hat Quay login screen, even if there is an equivalent user in LDAP.
			</p></section><section class="section" id="setup-ldap-configuration"><div class="titlepage"><div><div><h2 class="title">11.2. Configuring LDAP for Red Hat Quay</h2></div></div></div><p>
				Use the following procedure to configure LDAP for your Red Hat Quay deployment.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						You can use the Red Hat Quay config tool to configure LDAP.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Using the Red Hat Quay config tool, locate the <span class="strong strong"><strong>Authentication</strong></span> section. Select <span class="strong strong"><strong>LDAP</strong></span> from the dropdown menu, and update the LDAP configuration fields as required.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap.png" alt="LDAP configuration fields"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Optional. On the <span class="strong strong"><strong>Team synchronization</strong></span> box, and click <span class="strong strong"><strong>Enable Team Syncrhonization Support</strong></span>. With team synchronization enabled, Red Hat Quay administrators who are also superusers can set teams to have their membership synchronized with a backing group in LDAP.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-team-sync-1.png" alt="Team synchronization"/></span>
							</p></li><li class="listitem">
								For <span class="strong strong"><strong>Resynchronization duration</strong></span> enter <span class="strong strong"><strong>60m</strong></span>. This option sets the resynchronization duration at which a team must be re-synchronized. This field must be set similar to the following examples: <code class="literal">30m</code>, <code class="literal">1h</code>, <code class="literal">1d</code>.
							</li><li class="listitem"><p class="simpara">
								Optional. For <span class="strong strong"><strong>Self-service team syncing setup</strong></span>, you can click <span class="strong strong"><strong>Allow non-superusers to enable and manage team syncing</strong></span> to allow superusers the ability to enable and manage team syncing under the organizations that they are administrators for.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-team-sync-2.png" alt="Team synchronization"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Locate the <span class="strong strong"><strong>LDAP URI</strong></span> box and provide a full LDAP URI, including the <span class="emphasis"><em>ldap://</em></span> or <span class="emphasis"><em>ldaps://</em></span> prefix, for example, <code class="literal">ldap://117.17.8.101</code>.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-uri.png" alt="LDAP server URI"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Under <span class="strong strong"><strong>Base DN</strong></span>, provide a name which forms the base path for looking up all LDAP records, for example, <code class="literal">o=&lt;organization_id&gt;</code>,<code class="literal">dc=&lt;example_domain_component&gt;</code>,<code class="literal">dc=com</code>.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-basedn.png" alt="Distinguished Names"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Under <span class="strong strong"><strong>User Relative DN</strong></span>, provide a list of Distinguished Name path(s), which form the secondary base path(s) for looking up all user LDAP records relative to the <span class="strong strong"><strong>Base DN</strong></span> defined above. For example, <code class="literal">uid=&lt;name&gt;</code>,<code class="literal">ou=Users</code>,<code class="literal">o=&lt;organization_id&gt;</code>,<code class="literal">dc=&lt;example_domain_component&gt;</code>,<code class="literal">dc=com</code>. This path, or these paths, is tried if the user is not found through the primary relative DN.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/user-relative-dn.png" alt="User Relative DN"/></span>
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									<span class="strong strong"><strong>User Relative DN</strong></span> is relative to <span class="strong strong"><strong>Base DN</strong></span>, for example, <code class="literal">ou=Users</code> and not <code class="literal">ou=Users,dc=&lt;example_domain_component&gt;,dc=com</code>.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Optional. Provide <span class="strong strong"><strong>Secondary User Relative DNs</strong></span> if there are multiple Organizational Units where user objects are located. You can type in the Organizational Units and click <span class="strong strong"><strong>Add</strong></span> to add multiple RDNs. For example, <code class="literal">ou=Users,ou=NYC and ou=Users,ou=SFO</code>.
							</p><p class="simpara">
								The <span class="strong strong"><strong>User Relative DN</strong></span> searches with subtree scope. For example, if your organization has Organization Units <code class="literal">NYC</code> and <code class="literal">SFO</code> under the Users OU (that is, <code class="literal">ou=SFO,ou=Users</code> and <code class="literal">ou=NYC,ou=Users</code>), Red Hat Quay can authenticate users from both the <code class="literal">NYC</code> and <code class="literal">SFO</code> Organizational Units if the <span class="strong strong"><strong>User Relative DN</strong></span> is set to <code class="literal">Users</code> (<code class="literal">ou=Users</code>).
							</p></li><li class="listitem"><p class="simpara">
								Optional. Fill in the <span class="strong strong"><strong>Additional User Filter Expression</strong></span> field for all user lookup queries if desired. Distinguished Names used in the filter must be full based. The <span class="strong strong"><strong>Base DN</strong></span> is not added automatically added to this field, and you must wrap the text in parentheses, for example, <code class="literal">(memberOf=cn=developers,ou=groups,dc=&lt;example_domain_component&gt;,dc=com)</code>.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-user-filter.png" alt="Additional User Filter"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Fill in the <span class="strong strong"><strong>Administrator DN</strong></span> field for the Red Hat Quay administrator account. This account must be able to login and view the records for all users accounts. For example: <code class="literal">uid=&lt;name&gt;,ou=Users,o=&lt;organization_id&gt;,dc=&lt;example_domain_component&gt;,dc=com</code>.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-admin-dn.png" alt="Administrator DN"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Fill in the <span class="strong strong"><strong>Administrator DN Password</strong></span> field. This is the password for the administrator distinguished name.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									The password for this field is stored in plaintext inside of the <code class="literal">config.yaml</code> file. Setting up a dedicated account of using a password hash is highly recommended.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Optional. Fill in the <span class="strong strong"><strong>UID Attribute</strong></span> field. This is the name of the property field in the LDAP user records that stores your user’s username. Most commonly, <span class="strong strong"><strong>uid</strong></span> is entered for this field. This field can be used to log into your Red Hat Quay deployment.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/uid-attribute-ldap.png" alt="UID Attribute"/></span>
							</p></li><li class="listitem"><p class="simpara">
								Optional. Fill in the <span class="strong strong"><strong>Mail Attribute</strong></span> field. This is the name of the property field in your LDAP user records that stores your user’s e-mail addresses. Most commonly, <span class="strong strong"><strong>mail</strong></span> is entered for this field. This field can be used to log into your Red Hat Quay deployment.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/mail-attribute-ldap.png" alt="Mail Attribute"/></span>
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The username to log in must exist in the <span class="strong strong"><strong>User Relative DN</strong></span>.
										</li><li class="listitem">
											If you are using Microsoft Active Directory to setup your LDAP deployment, you must use <code class="literal">sAMAccountName</code> for your UID attribute.
										</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
								Optional. You can add a custom SSL/TLS certificate by clicking <span class="strong strong"><strong>Choose File</strong></span> under the <span class="strong strong"><strong>Custom TLS Certificate</strong></span> optionl. Additionally, you can enable fallbacks to insecure, non-TLS connections by checking the <span class="strong strong"><strong>Allow fallback to non-TLS connections</strong></span> box.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="images/authentication-ldap-ssl.png" alt="LDAP server SSL"/></span>
							</p><p class="simpara">
								If you upload an SSl/TLS certificate, you must provide an <span class="emphasis"><em>ldaps://</em></span> prefix, for example, <code class="literal">LDAP_URI: ldaps://ldap_provider.example.org</code>.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Alternatively, you can update your <code class="literal">config.yaml</code> file directly to include all relevant information. For example:
					</p><pre class="programlisting language-yaml">---
AUTHENTICATION_TYPE: LDAP
---
LDAP_ADMIN_DN: uid=&lt;name&gt;,ou=Users,o=&lt;organization_id&gt;,dc=&lt;example_domain_component&gt;,dc=com
LDAP_ADMIN_PASSWD: ABC123
LDAP_ALLOW_INSECURE_FALLBACK: false
LDAP_BASE_DN:
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com
LDAP_EMAIL_ATTR: mail
LDAP_UID_ATTR: uid
LDAP_URI: ldap://&lt;example_url&gt;.com
LDAP_USER_FILTER: (memberof=cn=developers,ou=Users,dc=&lt;domain_name&gt;,dc=com)
LDAP_USER_RDN:
    - ou=&lt;example_organization_unit&gt;
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com</pre></li><li class="listitem">
						After you have added all required LDAP fields, click the <span class="strong strong"><strong>Save Configuration Changes</strong></span> button to validate the configuration. All validation must succeed before proceeding. Additional configuration can be performed by selecting the <span class="strong strong"><strong>Continue Editing</strong></span> button.
					</li></ol></div></section><section class="section" id="ldap-restricted-users-enabling"><div class="titlepage"><div><div><h2 class="title">11.3. Enabling the LDAP_RESTRICTED_USER_FILTER configuration field</h2></div></div></div><p>
				The <code class="literal">LDAP_RESTRICTED_USER_FILTER</code> configuration field is a subset of the <code class="literal">LDAP_USER_FILTER</code> configuration field. When configured, this option allows Red Hat Quay administrators the ability to configure LDAP users as restricted users when Red Hat Quay uses LDAP as its authentication provider.
			</p><p>
				Use the following procedure to enable LDAP restricted users on your Red Hat Quay deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Your Red Hat Quay deployment uses LDAP as its authentication provider.
					</li><li class="listitem">
						You have configured the <code class="literal">LDAP_USER_FILTER</code> field in your <code class="literal">config.yaml</code> file.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">LDAP_RESTRICTED_USER_FILTER</code> parameter and specify the group of restricted users, for example, <code class="literal">members</code>:
					</p><pre class="programlisting language-yaml">---
AUTHENTICATION_TYPE: LDAP
---
LDAP_ADMIN_DN: uid=&lt;name&gt;,ou=Users,o=&lt;organization_id&gt;,dc=&lt;example_domain_component&gt;,dc=com
LDAP_ADMIN_PASSWD: ABC123
LDAP_ALLOW_INSECURE_FALLBACK: false
LDAP_BASE_DN:
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com
LDAP_EMAIL_ATTR: mail
LDAP_UID_ATTR: uid
LDAP_URI: ldap://&lt;example_url&gt;.com
LDAP_USER_FILTER: (memberof=cn=developers,ou=Users,o=&lt;example_organization_unit&gt;,dc=&lt;example_domain_component&gt;,dc=com)
LDAP_RESTRICTED_USER_FILTER: (&lt;filterField&gt;=&lt;value&gt;)
LDAP_USER_RDN:
    - ou=&lt;example_organization_unit&gt;
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com</pre></li><li class="listitem">
						Start, or restart, your Red Hat Quay deployment.
					</li></ol></div><p>
				After enabling the <code class="literal">LDAP_RESTRICTED_USER_FILTER</code> feature, your LDAP Red Hat Quay users are restricted from reading and writing content, and creating organizations.
			</p></section><section class="section" id="ldap-super-users-enabling"><div class="titlepage"><div><div><h2 class="title">11.4. Enabling the LDAP_SUPERUSER_FILTER configuration field</h2></div></div></div><p>
				With the <code class="literal">LDAP_SUPERUSER_FILTER</code> field configured, Red Hat Quay administrators can configure Lightweight Directory Access Protocol (LDAP) users as superusers if Red Hat Quay uses LDAP as its authentication provider.
			</p><p>
				Use the following procedure to enable LDAP superusers on your Red Hat Quay deployment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Your Red Hat Quay deployment uses LDAP as its authentication provider.
					</li><li class="listitem">
						You have configured the <code class="literal">LDAP_USER_FILTER</code> field field in your <code class="literal">config.yaml</code> file.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">LDAP_SUPERUSER_FILTER</code> parameter and add the group of users you want configured as super users, for example, <code class="literal">root</code>:
					</p><pre class="programlisting language-yaml">---
AUTHENTICATION_TYPE: LDAP
---
LDAP_ADMIN_DN: uid=&lt;name&gt;,ou=Users,o=&lt;organization_id&gt;,dc=&lt;example_domain_component&gt;,dc=com
LDAP_ADMIN_PASSWD: ABC123
LDAP_ALLOW_INSECURE_FALLBACK: false
LDAP_BASE_DN:
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com
LDAP_EMAIL_ATTR: mail
LDAP_UID_ATTR: uid
LDAP_URI: ldap://&lt;example_url&gt;.com
LDAP_USER_FILTER: (memberof=cn=developers,ou=Users,o=&lt;example_organization_unit&gt;,dc=&lt;example_domain_component&gt;,dc=com)
LDAP_SUPERUSER_FILTER: (&lt;filterField&gt;=&lt;value&gt;)
LDAP_USER_RDN:
    - ou=&lt;example_organization_unit&gt;
    - o=&lt;organization_id&gt;
    - dc=&lt;example_domain_component&gt;
    - dc=com</pre></li><li class="listitem">
						Start, or restart, your Red Hat Quay deployment.
					</li></ol></div><p>
				After enabling the <code class="literal">LDAP_SUPERUSER_FILTER</code> feature, your LDAP Red Hat Quay users have superuser privileges. The following options are available to superusers:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Manage users
					</li><li class="listitem">
						Manage organizations
					</li><li class="listitem">
						Manage service keys
					</li><li class="listitem">
						View the change log
					</li><li class="listitem">
						Query the usage logs
					</li><li class="listitem">
						Create globally visible user messages
					</li></ul></div></section><section class="section" id="common-ldap-configuration-issues"><div class="titlepage"><div><div><h2 class="title">11.5. Common LDAP configuration issues</h2></div></div></div><p>
				The following errors might be returned with an invalid configuration.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Invalid credentials</strong></span>. If you receive this error, the Administrator DN or Administrator DN password values are incorrect. Ensure that you are providing accurate Administrator DN and password values.
					</li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>*Verification of superuser %USERNAME% failed</strong></span>. This error is returned for the following reasons:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								The username has not been found.
							</li><li class="listitem">
								The user does not exist in the remote authentication system.
							</li><li class="listitem">
								LDAP authorization is configured improperly.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Cannot find the current logged in user</strong></span>. When configuring LDAP for Red Hat Quay, there may be situations where the LDAP connection is established successfully using the username and password provided in the <span class="strong strong"><strong>Administrator DN</strong></span> fields. However, if the current logged-in user cannot be found within the specified <span class="strong strong"><strong>User Relative DN</strong></span> path using the <span class="strong strong"><strong>UID Attribute</strong></span> or <span class="strong strong"><strong>Mail Attribute</strong></span> fields, there are typically two potential reasons for this:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								The current logged in user does not exist in the <span class="strong strong"><strong>User Relative DN</strong></span> path.
							</li><li class="listitem"><p class="simpara">
								The <span class="strong strong"><strong>Administrator DN</strong></span> does not have rights to search or read the specified LDAP path.
							</p><p class="simpara">
								To fix this issue, ensure that the logged in user is included in the <span class="strong strong"><strong>User Relative DN</strong></span> path, or provide the correct permissions to the <span class="strong strong"><strong>Administrator DN</strong></span> account.
							</p></li></ul></div></li></ul></div></section><section class="section" id="ldap-configuration-fields-link"><div class="titlepage"><div><div><h2 class="title">11.6. LDAP configuration fields</h2></div></div></div><p>
				For a full list of LDAP configuration fields, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/configure_red_hat_quay/index#config-fields-ldap">LDAP configuration fields</a>
			</p></section></section><section class="chapter" id="configuring-oidc-authentication"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Configuring OIDC for Red Hat Quay</h1></div></div></div><p>
			Configuring OpenID Connect (OIDC) for Red Hat Quay can provide several benefits to your Red Hat Quay deployment. For example, OIDC allows users to authenticate to Red Hat Quay using their existing credentials from an OIDC provider, such as <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.0">Red Hat Single Sign-On</a>, Google, Github, Microsoft, or others. Other benefits of OIDC include centralized user management, enhanced security, and single sign-on (SSO). Overall, OIDC configuration can simplify user authentication and management, enhance security, and provide a seamless user experience for Red Hat Quay users.
		</p><p>
			The following procedures show you how to configure Red Hat Single Sign-On and Azure AD. Collectively, these procedures include configuring OIDC on the Red Hat Quay Operator, and on standalone deployments by using the Red Hat Quay config tool.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				By following these procedures, you will be able to add any OIDC provider to Red Hat Quay, regardless of which identity provider you choose to use.
			</p></div></div><section class="section" id="configuring-red-hat-sso-oidc"><div class="titlepage"><div><div><h2 class="title">12.1. Configuring Red Hat Single Sign-On for Red Hat Quay</h2></div></div></div><p>
				Based on the Keycloak project, Red Hat Single Sign-On (RH-SSO) is an open source identity and access management (IAM) solution provided by Red Hat. RH-SSO allows organizations to manage user identities, secure applications, and enforce access control policies across their systems and applications. It also provides a unified authentication and authorization framework, which allows users to log in one time and gain access to multiple applications and resources without needing to re-authenticate. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.0">Red Hat Single Sign-On</a>.
			</p><p>
				By configuring Red Hat Single Sign-On on Red Hat Quay, you can create a seamless authentication integration between Red Hat Quay and other application platforms like OpenShift Container Platform.
			</p><section class="section" id="configuring-red-hat-sso-using-config-tool"><div class="titlepage"><div><div><h3 class="title">12.1.1. Configuring the Red Hat Single Sign-On Operator for the Red Hat Quay Operator</h3></div></div></div><p>
					Use the following procedure to configure Red Hat Single Sign-On for the Red Hat Quay Operator on OpenShift Container Platform.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured Red Hat Single Sign-On for the Red Hat Quay Operator. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.6/html-single/server_installation_and_configuration_guide/index#operator">Red Hat Single Sign-On Operator</a>.
						</li><li class="listitem">
							You have configured TLS/SSL for your Red Hat Quay deployment <span class="emphasis"><em>and</em></span> for Red Hat Single Sign-On.
						</li><li class="listitem">
							You have generated a single Certificate Authority (CA) and uploaded it to your Red Hat Single Sign-On Operator <span class="emphasis"><em>and</em></span> to your Red Hat Quay configuration.
						</li><li class="listitem">
							You are logged into your OpenShift Container Platform cluster.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Navigate to the Red Hat Single Sign-On <span class="strong strong"><strong>Admin Console</strong></span>.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									On the OpenShift Container Platform <span class="strong strong"><strong>Web Console</strong></span>, navigate to <span class="strong strong"><strong>Network</strong></span> → <span class="strong strong"><strong>Route</strong></span>.
								</li><li class="listitem">
									Select the <span class="strong strong"><strong>Red Hat Single Sign-On</strong></span> project from the drop-down list.
								</li><li class="listitem">
									Find the Red Hat Single Sign-On <span class="strong strong"><strong>Admin Console</strong></span> in the <span class="strong strong"><strong>Routes</strong></span> table.
								</li></ol></div></li><li class="listitem">
							Select the Realm that you will use to configure Red Hat Quay.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Clients</strong></span> under the <span class="strong strong"><strong>Configure</strong></span> section of the navigation panel, and then click the <span class="strong strong"><strong>Create</strong></span> button to add a new OIDC for Red Hat Quay.
						</li><li class="listitem"><p class="simpara">
							Enter the following information.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Client ID:</strong></span> <code class="literal">quay-enterprise</code>
								</li><li class="listitem">
									<span class="strong strong"><strong>Client Protocol:</strong></span> <code class="literal">openid-connect</code>
								</li><li class="listitem">
									<span class="strong strong"><strong>Root URL:</strong></span> <code class="literal"><a class="link" href="https://&lt;quay">https://&lt;quay</a> endpoint&gt;/</code>
								</li></ul></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Save</strong></span>. This results in a redirect to the <span class="strong strong"><strong>Clients</strong></span> setting panel.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Access Type</strong></span> and select <span class="strong strong"><strong>Confidential</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Navigate to <span class="strong strong"><strong>Valid Redirect URIs</strong></span>. You must provide three redirect URIs. The value should be the fully qualified domain name of the Red Hat Quay registry appended with <code class="literal">/oauth2/redhatsso/callback</code>. For example:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal"><a class="link" href="https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback">https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback</a></code>
								</li><li class="listitem">
									<code class="literal"><a class="link" href="https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback/attach">https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback/attach</a></code>
								</li><li class="listitem">
									<code class="literal"><a class="link" href="https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback/cli">https://&lt;quay_endpoint&gt;/oauth2/redhatsso/callback/cli</a></code>
								</li></ul></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Save</strong></span> and navigate to the new <span class="strong strong"><strong>Credentials</strong></span> setting.
						</li><li class="listitem">
							Copy the value of the Secret.
						</li></ol></div></section><section class="section" id="configuring-quay-operator-use-redhat-sso"><div class="titlepage"><div><div><h3 class="title">12.1.2. Configuring the Red Hat Quay Operator to use Red Hat Single Sign-On</h3></div></div></div><p>
					Use the following procedure to configure Red Hat Single Sign-On with the Red Hat Quay Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured the Red Hat Single Sign-On Operator for the Red Hat Quay Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Enter the Red Hat Quay config editor tool by navigating to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>. Click <span class="strong strong"><strong>Red Hat Quay</strong></span> → <span class="strong strong"><strong>Quay Registry</strong></span>. Then, click the name of your Red Hat Quay registry, and the URL listed with <span class="strong strong"><strong>Config Editor Endpoint</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Upload a custom SSL/TLS certificate to your OpenShift Container Platform deployment.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to the Red Hat Quay config tool UI.
								</li><li class="listitem">
									Under <span class="strong strong"><strong>Custom SSL Certificates</strong></span>, click <span class="strong strong"><strong>Select file</strong></span> and upload your custom SSL/TLS certificates.
								</li><li class="listitem">
									Reconfigure your Red Hat Quay deployment.
								</li></ol></div></li><li class="listitem">
							Scroll down to the <span class="strong strong"><strong>External Authorization (OAuth)</strong></span> section.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add OIDC Provider</strong></span>.
						</li><li class="listitem">
							When prompted, enter <code class="literal">redhatsso</code>.
						</li><li class="listitem"><p class="simpara">
							Enter the following information:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>OIDC Server:</strong></span> The fully qualified domain name (FQDN) of the Red Hat Single Sign-On instance, appended with <code class="literal">/auth/realms/</code> and the Realm name. You must include the forward slash at the end, for example, <code class="literal">https://sso-redhat.example.com//auth/realms/&lt;keycloak_realm_name&gt;/</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Client ID:</strong></span> The client ID of the application that is being reistered with the identity provider, for example, <code class="literal">quay-enterprise</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Client Secret:</strong></span> The Secret from the <span class="strong strong"><strong>Credentials</strong></span> tab of the <code class="literal">quay-enterprise</code> OIDC client settings.
								</li><li class="listitem">
									<span class="strong strong"><strong>Service Name:</strong></span> The name that is displayed on the Red Hat Quay login page, for example, <code class="literal">Red hat Single Sign On</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Verified Email Address Claim:</strong></span> The name of the claim that is used to verify the email address of the user.
								</li><li class="listitem">
									<span class="strong strong"><strong>Login Scopes:</strong></span> The scopes to send to the OIDC provider when performing the login flow, for example, <code class="literal">openid</code>. After configuration, you must click <span class="strong strong"><strong>Add</strong></span>.
								</li></ul></div></li><li class="listitem">
							Scroll down and click <span class="strong strong"><strong>Validate Configuration Changes</strong></span>. Then, click <span class="strong strong"><strong>Restart Now</strong></span> to deploy the Red Hat Quay Operator with OIDC enabled.
						</li></ol></div></section></section><section class="section" id="configuring-azuread-oidc"><div class="titlepage"><div><div><h2 class="title">12.2. Configuring Azure AD OIDC for Red Hat Quay</h2></div></div></div><p>
				By integrating Azure AD authentication with Red Hat Quay, your organization can take advantage of the centralized user management and security features offered by Azure AD. Some features include the ability to manage user access to Red Hat Quay repositories based on their Azure AD roles and permissions, and the ability to enable multi-factor authentication and other security features provided by Azure AD.
			</p><p>
				Azure Active Directory (Azure AD) authentication for Red Hat Quay allows users to authenticate and access Red Hat Quay using their Azure AD credentials.
			</p><section class="section" id="configuring-azuread-using-config-tool"><div class="titlepage"><div><div><h3 class="title">12.2.1. Configuring Azure AD by using the Red Hat Quay config tool</h3></div></div></div><p>
					The following procedure configures Azure AD for Red Hat Quay using the config tool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enter the Red Hat Quay config editor tool.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If you are running a standalone Red Hat Quay deployment, you can enter the following command:
								</p><pre class="screen">$ sudo podman run --rm -it --name quay_config -p 80:8080 -p 443:8443 registry.redhat.io/quay/quay-rhel8:v3.9.0 config secret</pre><p class="simpara">
									Use your browser to navigate to the user interface for the configuration tool and log in.
								</p></li><li class="listitem">
									If you are on the Red Hat Quay Operator, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>. Click <span class="strong strong"><strong>Red Hat Quay</strong></span> → <span class="strong strong"><strong>Quay Registry</strong></span>. Then, click the name of your Red Hat Quay registry, and the URL listed with <span class="strong strong"><strong>Config Editor Endpoint</strong></span>.
								</li></ol></div></li><li class="listitem">
							Scroll down to the <span class="strong strong"><strong>External Authorization (OAuth)</strong></span> section.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add OIDC Provider</strong></span>.
						</li><li class="listitem"><p class="simpara">
							When prompted, enter the ID for the ODIC provider.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Your OIDC server must end with <code class="literal">/</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							After the ODIC provider has been added, Red Hat Quay lists three callback URLs that must be registered on Azure. These addresses allow Azure to direct back to Red Hat Quay after authentication is confirmed. For example:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback</code>
								</li><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback/attach</code>
								</li><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback/cli</code>
								</li></ul></div></li><li class="listitem">
							After all required fields have been set, validate your settings by clicking <span class="strong strong"><strong>Validate Configuration Changes</strong></span>. If any errors are reported, continue editing your configuration until the settings are valid and Red Hat Quay can connect to your database and Redis servers.
						</li></ol></div></section><section class="section" id="configuring-azuread-updating-config-yaml"><div class="titlepage"><div><div><h3 class="title">12.2.2. Configuring Azure AD by updating the Red Hat Quay config.yaml file</h3></div></div></div><p>
					Use the following procedure to configure Azure AD by updating the Red Hat Quay <code class="literal">config.yaml</code> file directly.
				</p><div class="admonition note"><div class="admonition_header">Procedure</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Using the following procedure, you can add any ODIC provider to Red Hat Quay, regardless of which identity provider is being added.
							</li><li class="listitem">
								If your system has a firewall in use, or proxy enabled, you must whitelist all Azure API endpoints for each Oauth application that is created. Otherwise, the following error is returned: <code class="literal">x509: certificate signed by unknown authority</code>.
							</li></ul></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add the following information to your Red Hat Quay <code class="literal">config.yaml</code> file:
						</p><pre class="screen">AZURE_LOGIN_CONFIG: <span id="CO3-1"/><span class="callout">1</span>
    CLIENT_ID: &lt;client_id&gt; <span id="CO3-2"/><span class="callout">2</span>
    CLIENT_SECRET: &lt;client_secret&gt; <span id="CO3-3"/><span class="callout">3</span>
    OIDC_SERVER: &lt;oidc_server_address_&gt; <span id="CO3-4"/><span class="callout">4</span>
    SERVICE_NAME: Azure AD <span id="CO3-5"/><span class="callout">5</span>
    VERIFIED_EMAIL_CLAIM_NAME: &lt;verified_email&gt; <span id="CO3-6"/><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The parent key that holds the OIDC configuration settings. In this example, the parent key used is <code class="literal">AZURE_LOGIN_CONFIG</code>, however, the string <code class="literal">AZURE</code> can be replaced with any arbitrary string based on your specific needs, for example <code class="literal">ABC123</code>.However, the following strings are not accepted: <code class="literal">GOOGLE</code>, <code class="literal">GITHUB</code>. These strings are reserved for their respecitve identity platforms and require a specific <code class="literal">config.yaml</code> entry contingent upon when platform you are using.
								</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The client ID of the application that is being reistered with the identity provider.
								</div></dd><dt><a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The client secret of the application that is being registered with the identity provider.
								</div></dd><dt><a href="#CO3-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The address of the OIDC server that is being used for authentication. In this example, you must use <code class="literal">sts.windows.net</code> as the issuer identifier. Using <code class="literal"><a class="link" href="https://login.microsoftonline.com">https://login.microsoftonline.com</a></code> results in the following error: <code class="literal">Could not create provider for AzureAD. Error: oidc: issuer did not match the issuer returned by provider, expected "https://login.microsoftonline.com/73f2e714-xxxx-xxxx-xxxx-dffe1df8a5d5" got "https://sts.windows.net/73f2e714-xxxx-xxxx-xxxx-dffe1df8a5d5/"</code>.
								</div></dd><dt><a href="#CO3-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The name of the service that is being authenticated.
								</div></dd><dt><a href="#CO3-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The name of the claim that is used to verify the email address of the user.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Proper configuration of Azure AD results three redirects with the following format:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback</code>
								</li><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback/attach</code>
								</li><li class="listitem">
									<code class="literal">https://QUAY_HOSTNAME/oauth2/&lt;name_of_service&gt;/callback/cli</code>
								</li></ul></div></li><li class="listitem">
							Restart your Red Hat Quay deployment.
						</li></ol></div></section></section></section><section class="chapter" id="prometheus-metrics-under-quay-enterprise"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Prometheus and Grafana metrics under Red Hat Quay</h1></div></div></div><p>
			Red Hat Quay exports a <a class="link" href="https://prometheus.io/">Prometheus</a>- and Grafana-compatible endpoint on each instance to allow for easy monitoring and alerting.
		</p><section class="section" id="exposing-the-prometheus-endpoint"><div class="titlepage"><div><div><h2 class="title">13.1. Exposing the Prometheus endpoint</h2></div></div></div><section class="section" id="standalone_red_hat_quay"><div class="titlepage"><div><div><h3 class="title">13.1.1. Standalone Red Hat Quay</h3></div></div></div><p>
					When using <code class="literal">podman run</code> to start the <code class="literal">Quay</code> container, expose the metrics port <code class="literal">9091</code>:
				</p><pre class="screen">$ sudo podman run -d --rm -p 80:8080 -p 443:8443  -p 9091:9091\
   --name=quay \
   -v $QUAY/config:/conf/stack:Z \
   -v $QUAY/storage:/datastorage:Z \
   registry.redhat.io/quay/quay-rhel8:v3.9.0</pre><p>
					The metrics will now be available:
				</p><pre class="programlisting language-terminal">$ curl quay.example.com:9091/metrics</pre><p>
					See <a class="link" href="https://access.redhat.com/solutions/3750281">Monitoring Quay with Prometheus and Grafana</a> for details on configuring Prometheus and Grafana to monitor Quay repository counts.
				</p></section><section class="section" id="red_hat_quay_operator"><div class="titlepage"><div><div><h3 class="title">13.1.2. Red Hat Quay Operator</h3></div></div></div><p>
					Determine the cluster IP for the <code class="literal">quay-metrics</code> service:
				</p><pre class="programlisting language-terminal">$ oc get services -n quay-enterprise
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                             AGE
example-registry-clair-app            ClusterIP   172.30.61.161    &lt;none&gt;        80/TCP,8089/TCP                     18h
example-registry-clair-postgres       ClusterIP   172.30.122.136   &lt;none&gt;        5432/TCP                            18h
example-registry-quay-app             ClusterIP   172.30.72.79     &lt;none&gt;        443/TCP,80/TCP,8081/TCP,55443/TCP   18h
example-registry-quay-config-editor   ClusterIP   172.30.185.61    &lt;none&gt;        80/TCP                              18h
example-registry-quay-database        ClusterIP   172.30.114.192   &lt;none&gt;        5432/TCP                            18h
example-registry-quay-metrics         ClusterIP   172.30.37.76     &lt;none&gt;        9091/TCP                            18h
example-registry-quay-redis           ClusterIP   172.30.157.248   &lt;none&gt;        6379/TCP                            18h</pre><p>
					Connect to your cluster and access the metrics using the cluster IP and port for the <code class="literal">quay-metrics</code> service:
				</p><pre class="programlisting language-terminal">$ oc debug node/master-0

sh-4.4# curl 172.30.37.76:9091/metrics

# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 4.0447e-05
go_gc_duration_seconds{quantile="0.25"} 6.2203e-05
...</pre></section><section class="section" id="setting-up-prometheus-to-consume-metrics"><div class="titlepage"><div><div><h3 class="title">13.1.3. Setting up Prometheus to consume metrics</h3></div></div></div><p>
					Prometheus needs a way to access all Red Hat Quay instances running in a cluster. In the typical setup, this is done by listing all the Red Hat Quay instances in a single named DNS entry, which is then given to Prometheus.
				</p></section><section class="section" id="dns-configuration-under-kubernetes"><div class="titlepage"><div><div><h3 class="title">13.1.4. DNS configuration under Kubernetes</h3></div></div></div><p>
					A simple <a class="link" href="http://kubernetes.io/docs/user-guide/services/">Kubernetes service</a> can be configured to provide the DNS entry for Prometheus.
				</p></section><section class="section" id="dns-configuration-for-a-manual-cluster"><div class="titlepage"><div><div><h3 class="title">13.1.5. DNS configuration for a manual cluster</h3></div></div></div><p>
					<a class="link" href="https://github.com/skynetservices/skydns">SkyDNS</a> is a simple solution for managing this DNS record when not using Kubernetes. SkyDNS can run on an <a class="link" href="https://github.com/coreos/etcd">etcd</a> cluster. Entries for each Red Hat Quay instance in the cluster can be added and removed in the etcd store. SkyDNS will regularly read them from there and update the list of Quay instances in the DNS record accordingly.
				</p></section></section><section class="section" id="metrics-intro"><div class="titlepage"><div><div><h2 class="title">13.2. Introduction to metrics</h2></div></div></div><p>
				Red Hat Quay provides metrics to help monitor the registry, including metrics for general registry usage, uploads, downloads, garbage collection, and authentication.
			</p><section class="section" id="metrics-general-registry-stats"><div class="titlepage"><div><div><h3 class="title">13.2.1. General registry statistics</h3></div></div></div><p>
					General registry statistics can indicate how large the registry has grown.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339558304" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339557216" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339558304">
								<p>
									quay_user_rows
								</p>
								</td><td align="left" valign="top" headers="idm45621339557216">
								<p>
									Number of users in the database
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339558304">
								<p>
									quay_robot_rows
								</p>
								</td><td align="left" valign="top" headers="idm45621339557216">
								<p>
									Number of robot accounts in the database
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339558304">
								<p>
									quay_org_rows
								</p>
								</td><td align="left" valign="top" headers="idm45621339557216">
								<p>
									Number of organizations in the database
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339558304">
								<p>
									quay_repository_rows
								</p>
								</td><td align="left" valign="top" headers="idm45621339557216">
								<p>
									Number of repositories in the database
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339558304">
								<p>
									quay_security_scanning_unscanned_images_remaining_total
								</p>
								</td><td align="left" valign="top" headers="idm45621339557216">
								<p>
									Number of images that are not scanned by the latest security scanner
								</p>
								</td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
						
<pre class="programlisting language-terminal"># HELP quay_user_rows number of users in the database
# TYPE quay_user_rows gauge
quay_user_rows{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="65",process_name="globalpromstats.py"} 3

# HELP quay_robot_rows number of robot accounts in the database
# TYPE quay_robot_rows gauge
quay_robot_rows{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="65",process_name="globalpromstats.py"} 2

# HELP quay_org_rows number of organizations in the database
# TYPE quay_org_rows gauge
quay_org_rows{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="65",process_name="globalpromstats.py"} 2

# HELP quay_repository_rows number of repositories in the database
# TYPE quay_repository_rows gauge
quay_repository_rows{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="65",process_name="globalpromstats.py"} 4

# HELP quay_security_scanning_unscanned_images_remaining number of images that are not scanned by the latest security scanner
# TYPE quay_security_scanning_unscanned_images_remaining gauge
quay_security_scanning_unscanned_images_remaining{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 5</pre>
					</p></div></section><section class="section" id="metrics-queue-items"><div class="titlepage"><div><div><h3 class="title">13.2.2. Queue items</h3></div></div></div><p>
					The <span class="emphasis"><em>queue items</em></span> metrics provide information on the multiple queues used by Quay for managing work.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339861856" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339860768" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339861856">
								<p>
									quay_queue_items_available
								</p>
								</td><td align="left" valign="top" headers="idm45621339860768">
								<p>
									Number of items in a specific queue
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339861856">
								<p>
									quay_queue_items_locked
								</p>
								</td><td align="left" valign="top" headers="idm45621339860768">
								<p>
									Number of items that are running
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339861856">
								<p>
									quay_queue_items_available_unlocked
								</p>
								</td><td align="left" valign="top" headers="idm45621339860768">
								<p>
									Number of items that are waiting to be processed
								</p>
								</td></tr></tbody></table></div><div class="itemizedlist"><p class="title"><strong>Metric labels</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>queue_name:</strong></span> The name of the queue. One of:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<span class="strong strong"><strong>exportactionlogs:</strong></span> Queued requests to export action logs. These logs are then processed and put in storage. A link is then sent to the requester via email.
								</li><li class="listitem">
									<span class="strong strong"><strong>namespacegc:</strong></span> Queued namespaces to be garbage collected
								</li><li class="listitem">
									<span class="strong strong"><strong>notification:</strong></span> Queue for repository notifications to be sent out
								</li><li class="listitem">
									<span class="strong strong"><strong>repositorygc:</strong></span> Queued repositories to be garbage collected
								</li><li class="listitem">
									<span class="strong strong"><strong>secscanv4:</strong></span> Notification queue specific for Clair V4
								</li><li class="listitem">
									<span class="strong strong"><strong>dockerfilebuild:</strong></span> Queue for Quay docker builds
								</li><li class="listitem">
									<span class="strong strong"><strong>imagestoragereplication:</strong></span> Queued blob to be replicated across multiple storages
								</li><li class="listitem">
									<span class="strong strong"><strong>chunk_cleanup:</strong></span> Queued blob segments that needs to be deleted. This is only used by some storage implementations, for example, Swift.
								</li></ul></div></li></ul></div><p>
					For example, the queue labelled <span class="strong strong"><strong>repositorygc</strong></span> contains the repositories marked for deletion by the repository garbage collection worker. For metrics with a <span class="strong strong"><strong>queue_name</strong></span> label of <span class="strong strong"><strong>repositorygc</strong></span>:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>quay_queue_items_locked</strong></span> is the number of repositories currently being deleted.
						</li><li class="listitem">
							<span class="strong strong"><strong>quay_queue_items_available_unlocked</strong></span> is the number of repositories waiting to get processed by the worker.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
						
<pre class="programlisting language-terminal"># HELP quay_queue_items_available number of queue items that have not expired
# TYPE quay_queue_items_available gauge
quay_queue_items_available{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="63",process_name="exportactionlogsworker.py",queue_name="exportactionlogs"} 0
...

# HELP quay_queue_items_available_unlocked number of queue items that have not expired and are not locked
# TYPE quay_queue_items_available_unlocked gauge
quay_queue_items_available_unlocked{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="63",process_name="exportactionlogsworker.py",queue_name="exportactionlogs"} 0
...

# HELP quay_queue_items_locked number of queue items that have been acquired
# TYPE quay_queue_items_locked gauge
quay_queue_items_locked{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="63",process_name="exportactionlogsworker.py",queue_name="exportactionlogs"} 0</pre>
					</p></div></section><section class="section" id="metrics-garbage-collection"><div class="titlepage"><div><div><h3 class="title">13.2.3. Garbage collection metrics</h3></div></div></div><p>
					These metrics show you how many resources have been removed from garbage collection (gc). They show many times the gc workers have run and how many namespaces, repositories, and blobs were removed.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339568512" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339567424" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339568512">
								<p>
									quay_gc_iterations_total
								</p>
								</td><td align="left" valign="top" headers="idm45621339567424">
								<p>
									Number of iterations by the GCWorker
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339568512">
								<p>
									quay_gc_namespaces_purged_total
								</p>
								</td><td align="left" valign="top" headers="idm45621339567424">
								<p>
									Number of namespaces purged by the NamespaceGCWorker
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339568512">
								<p>
									quay_gc_repos_purged_total
								</p>
								</td><td align="left" valign="top" headers="idm45621339567424">
								<p>
									Number of repositories purged by the RepositoryGCWorker or NamespaceGCWorker
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm45621339568512">
								<p>
									quay_gc_storage_blobs_deleted_total
								</p>
								</td><td align="left" valign="top" headers="idm45621339567424">
								<p>
									Number of storage blobs deleted
								</p>
								</td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
						
<pre class="programlisting language-terminal"># TYPE quay_gc_iterations_created gauge
quay_gc_iterations_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189714e+09
...

# HELP quay_gc_iterations_total number of iterations by the GCWorker
# TYPE quay_gc_iterations_total counter
quay_gc_iterations_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...

# TYPE quay_gc_namespaces_purged_created gauge
quay_gc_namespaces_purged_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189433e+09
...

# HELP quay_gc_namespaces_purged_total number of namespaces purged by the NamespaceGCWorker
# TYPE quay_gc_namespaces_purged_total counter
quay_gc_namespaces_purged_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
....

# TYPE quay_gc_repos_purged_created gauge
quay_gc_repos_purged_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.631782319018925e+09
...

# HELP quay_gc_repos_purged_total number of repositories purged by the RepositoryGCWorker or NamespaceGCWorker
# TYPE quay_gc_repos_purged_total counter
quay_gc_repos_purged_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...

# TYPE quay_gc_storage_blobs_deleted_created gauge
quay_gc_storage_blobs_deleted_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189059e+09
...

# HELP quay_gc_storage_blobs_deleted_total number of storage blobs deleted
# TYPE quay_gc_storage_blobs_deleted_total counter
quay_gc_storage_blobs_deleted_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...</pre>
					</p></div><section class="section" id="metrics-multipart-uploads"><div class="titlepage"><div><div><h4 class="title">13.2.3.1. Multipart uploads metrics</h4></div></div></div><p>
						The multipart uploads metrics show the number of blobs uploads to storage (S3, Rados, GoogleCloudStorage, RHOCS). These can help identify issues when Quay is unable to correctly upload blobs to storage.
					</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621337373840" scope="col">Metric name</th><th align="left" valign="top" id="idm45621337372752" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621337373840">
									<p>
										quay_multipart_uploads_started_total
									</p>
									</td><td align="left" valign="top" headers="idm45621337372752">
									<p>
										Number of multipart uploads to Quay storage that started
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45621337373840">
									<p>
										quay_multipart_uploads_completed_total
									</p>
									</td><td align="left" valign="top" headers="idm45621337372752">
									<p>
										Number of multipart uploads to Quay storage that completed
									</p>
									</td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
							
<pre class="programlisting language-terminal"># TYPE quay_multipart_uploads_completed_created gauge
quay_multipart_uploads_completed_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823308284895e+09
...

# HELP quay_multipart_uploads_completed_total number of multipart uploads to Quay storage that completed
# TYPE quay_multipart_uploads_completed_total counter
quay_multipart_uploads_completed_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0

# TYPE quay_multipart_uploads_started_created gauge
quay_multipart_uploads_started_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823308284352e+09
...

# HELP quay_multipart_uploads_started_total number of multipart uploads to Quay storage that started
# TYPE quay_multipart_uploads_started_total counter
quay_multipart_uploads_started_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...</pre>
						</p></div></section></section><section class="section" id="metrics-image-push-pull"><div class="titlepage"><div><div><h3 class="title">13.2.4. Image push / pull metrics</h3></div></div></div><p>
					A number of metrics are available related to pushing and pulling images.
				</p><section class="section" id="image_pulls_total"><div class="titlepage"><div><div><h4 class="title">13.2.4.1. Image pulls total</h4></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339053168" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339052080" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339053168">
									<p>
										quay_registry_image_pulls_total
									</p>
									</td><td align="left" valign="top" headers="idm45621339052080">
									<p>
										The number of images downloaded from the registry.
									</p>
									</td></tr></tbody></table></div><div class="itemizedlist"><p class="title"><strong>Metric labels</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>protocol:</strong></span> the registry protocol used (should always be v2)
							</li><li class="listitem">
								<span class="strong strong"><strong>ref:</strong></span> ref used to pull - tag, manifest
							</li><li class="listitem">
								<span class="strong strong"><strong>status:</strong></span> http return code of the request
							</li></ul></div></section><section class="section" id="image_bytes_pulled"><div class="titlepage"><div><div><h4 class="title">13.2.4.2. Image bytes pulled</h4></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339226928" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339225840" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339226928">
									<p>
										quay_registry_image_pulled_estimated_bytes_total
									</p>
									</td><td align="left" valign="top" headers="idm45621339225840">
									<p>
										The number of bytes downloaded from the registry
									</p>
									</td></tr></tbody></table></div><div class="itemizedlist"><p class="title"><strong>Metric labels</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong> protocol:</strong></span> the registry protocol used (should always be v2)
							</li></ul></div></section><section class="section" id="image_pushes_total"><div class="titlepage"><div><div><h4 class="title">13.2.4.3. Image pushes total</h4></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621338843728" scope="col">Metric name</th><th align="left" valign="top" id="idm45621338842640" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621338843728">
									<p>
										quay_registry_image_pushes_total
									</p>
									</td><td align="left" valign="top" headers="idm45621338842640">
									<p>
										The number of images uploaded from the registry.
									</p>
									</td></tr></tbody></table></div><div class="itemizedlist"><p class="title"><strong>Metric labels</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>protocol:</strong></span> the registry protocol used (should always be v2)
							</li><li class="listitem">
								<span class="strong strong"><strong>pstatus:</strong></span> http return code of the request
							</li><li class="listitem">
								<span class="strong strong"><strong>pmedia_type:</strong></span> the uploaded manifest type
							</li></ul></div></section><section class="section" id="image_bytes_pushed"><div class="titlepage"><div><div><h4 class="title">13.2.4.4. Image bytes pushed</h4></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621339247472" scope="col">Metric name</th><th align="left" valign="top" id="idm45621339246384" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621339247472">
									<p>
										quay_registry_image_pushed_bytes_total
									</p>
									</td><td align="left" valign="top" headers="idm45621339246384">
									<p>
										The number of bytes uploaded to the registry
									</p>
									</td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
							
<pre class="programlisting language-terminal"># HELP quay_registry_image_pushed_bytes_total number of bytes pushed to the registry
# TYPE quay_registry_image_pushed_bytes_total counter
quay_registry_image_pushed_bytes_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="221",process_name="registry:application"} 0
...</pre>
						</p></div></section></section><section class="section" id="metrics-authentication"><div class="titlepage"><div><div><h3 class="title">13.2.5. Authentication metrics</h3></div></div></div><p>
					The authentication metrics provide the number of authentication requests, labeled by type and whether it succeeded or not. For example, this metric could be used to monitor failed basic authentication requests.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621336119968" scope="col">Metric name</th><th align="left" valign="top" id="idm45621336118880" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621336119968">
								<p>
									quay_authentication_attempts_total
								</p>
								</td><td align="left" valign="top" headers="idm45621336118880">
								<p>
									Number of authentication attempts across the registry and API
								</p>
								</td></tr></tbody></table></div><div class="itemizedlist"><p class="title"><strong>Metric labels</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>auth_kind:</strong></span> The type of auth used, including:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									basic
								</li><li class="listitem">
									oauth
								</li><li class="listitem">
									credentials
								</li></ul></div></li><li class="listitem">
							<span class="strong strong"><strong>success:</strong></span> true or false
						</li></ul></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
						
<pre class="programlisting language-terminal"># TYPE quay_authentication_attempts_created gauge
quay_authentication_attempts_created{auth_kind="basic",host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="221",process_name="registry:application",success="True"} 1.6317843039374158e+09
...

# HELP quay_authentication_attempts_total number of authentication attempts across the registry and API
# TYPE quay_authentication_attempts_total counter
quay_authentication_attempts_total{auth_kind="basic",host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="221",process_name="registry:application",success="True"} 2
...</pre>
					</p></div></section></section></section><section class="chapter" id="red-hat-quay-quota-management-and-enforcement"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Red Hat Quay quota management and enforcement overview</h1></div></div></div><p>
			With Red Hat Quay, users have the ability to report storage consumption and to contain registry growth by establishing configured storage quota limits. On-premise Red Hat Quay users are now equipped with the following capabilities to manage the capacity limits of their environment:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="strong strong"><strong>Quota reporting:</strong></span> With this feature, a superuser can track the storage consumption of all their organizations. Additionally, users can track the storage consumption of their assigned organization.
				</li><li class="listitem">
					<span class="strong strong"><strong>Quota management:</strong></span> With this feature, a superuser can define soft and hard checks for Red Hat Quay users. Soft checks tell users if the storage consumption of an organization reaches their configured threshold. Hard checks prevent users from pushing to the registry when storage consumption reaches the configured limit.
				</li></ul></div><p>
			Together, these features allow service owners of a Red Hat Quay registry to define service level agreements and support a healthy resource budget.
		</p><section class="section" id="quota-management-arch"><div class="titlepage"><div><div><h2 class="title">14.1. Quota management architecture</h2></div></div></div><p>
				With the quota management feature enabled, individual blob sizes are summed at the repository and namespace level. For example, if two tags in the same repository reference the same blob, the size of that blob is only counted once towards the repository total. Additionally, manifest list totals are counted toward the repository total.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Because manifest list totals are counted toward the repository total, the total quota consumed when upgrading from a previous version of Red Hat Quay might be reportedly differently in Red Hat Quay 3.9. In some cases, the new total might go over a repository’s previously-set limit. Red Hat Quay administrators might have to adjust the allotted quota of a repository to account for these changes.
				</p></div></div><p>
				The quota management feature works by calculating the size of existing repositories and namespace with a backfill worker, and then adding or subtracting from the total for every image that is pushed or garbage collected afterwords. Additionally, the subtraction from the total happens when the manifest is garbage collected.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Because subtraction occurs from the total when the manifest is garbage collected, there is a delay in the size calculation until it is able to be garbage collected. For more information about garbage collection, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/manage_red_hat_quay/index#red_hat_quay_garbage_collection">Red Hat Quay garbage collection</a>.
				</p></div></div><p>
				The following database tables hold the quota repository size, quota namespace size, and quota registry size, in bytes, of a Red Hat Quay repository within an organization:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">QuotaRepositorySize</code>
					</li><li class="listitem">
						<code class="literal">QuotaNameSpaceSize</code>
					</li><li class="listitem">
						<code class="literal">QuotaRegistrySize</code>
					</li></ul></div><p>
				The organization size is calculated by the backfill worker to ensure that it is not duplicated. When an image push is initialized, the user’s organization storage is validated to check if it is beyond the configured quota limits. If an image push exceeds defined quota limitations, a soft or hard check occurs:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For a soft check, users are notified.
					</li><li class="listitem">
						For a hard check, the push is stopped.
					</li></ul></div><p>
				If storage consumption is within configured quota limits, the push is allowed to proceed.
			</p><p>
				Image manifest deletion follows a similar flow, whereby the links between associated image tags and the manifest are deleted. Additionally, after the image manifest is deleted, the repository size is recalculated and updated in the <code class="literal">QuotaRepositorySize</code>, <code class="literal">QuotaNameSpaceSize</code>, and <code class="literal">QuotaRegistrySize</code> tables.
			</p></section><section class="section" id="quota-management-limitations"><div class="titlepage"><div><div><h2 class="title">14.2. Quota management limitations</h2></div></div></div><p>
				Quota management helps organizations to maintain resource consumption. One limitation of quota management is that calculating resource consumption on push results in the calculation becoming part of the push’s critical path. Without this, usage data might drift.
			</p><p>
				The maximum storage quota size is dependent on the selected database:
			</p><div class="table" id="idm45621339494016"><p class="title"><strong>Table 14.1. Worker count environment variables</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621341277872" scope="col">Variable</th><th align="left" valign="top" id="idm45621341276784" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621341277872">
							<p>
								Postgres
							</p>
							</td><td align="left" valign="top" headers="idm45621341276784">
							<p>
								8388608 TB
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621341277872">
							<p>
								MySQL
							</p>
							</td><td align="left" valign="top" headers="idm45621341276784">
							<p>
								8388608 TB
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621341277872">
							<p>
								SQL Server
							</p>
							</td><td align="left" valign="top" headers="idm45621341276784">
							<p>
								16777216 TB
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="red-hat-quay-quota-management-39"><div class="titlepage"><div><div><h2 class="title">14.3. Quota management for Red Hat Quay 3.9</h2></div></div></div><p>
				If you are upgrading to Red Hat Quay 3.9, you must reconfigure the quota management feature. This is because with Red Hat Quay 3.9, calculation is done differently. As a result, totals prior to Red Hat Quay 3.9 are no longer valid. There are two methods for configuring quota management in Red Hat Quay 3.9, which are detailed in the following sections.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							This is a one time calculation that must be done after you have upgraded to Red Hat Quay 3.9.
						</li><li class="listitem">
							Superuser privileges are required to create, update and delete quotas. While quotas can be set for users as well as organizations, you cannot reconfigure the <span class="emphasis"><em>user</em></span> quota using the Red Hat Quay UI and you must use the API instead.
						</li></ul></div></div></div><section class="section" id="quota-management-configuring-38"><div class="titlepage"><div><div><h3 class="title">14.3.1. Option A: Configuring quota management for Red Hat Quay 3.9 by adjusting the QUOTA_TOTAL_DELAY feature flag</h3></div></div></div><p>
					Use the following procedure to recalculate Red Hat Quay 3.9 quota management by adjusting the <code class="literal">QUOTA_TOTAL_DELAY</code> feature flag.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						With this recalculation option, the totals appear as <span class="strong strong"><strong>0.00 KB</strong></span> until the allotted time designated for <code class="literal">QUOTA_TOTAL_DELAY</code>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have upgraded to Red Hat Quay 3.9.
						</li><li class="listitem">
							You are logged into Red Hat Quay 3.9 as a superuser.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Deploy Red Hat Quay 3.9 with the following <code class="literal">config.yaml</code> settings:
						</p><pre class="programlisting language-yaml">FEATURE_QUOTA_MANAGEMENT: true
FEATURE_GARBAGE_COLLECTION: true
PERMANENTLY_DELETE_TAGS: true
QUOTA_TOTAL_DELAY_SECONDS: 1800 <span id="CO4-1"/><span class="callout">1</span>
RESET_CHILD_MANIFEST_EXPIRATION: true</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">QUOTA_TOTAL_DELAY_SECONDS</code> flag defaults to <code class="literal">1800</code> seconds, or 30 minutes. This allows Red Hat Quay 3.9 to successfully deploy before the quota management feature begins calculating storage consumption for every blob that has been pushed. Setting this flag to a lower number might result in miscalculation; it <span class="strong strong"><strong>must</strong></span> be set to a number that is greater than the time it takes your Red Hat Quay deployment to start. <code class="literal">1800</code> is the recommended setting, however larger deployments that take longer than 30 minutes to start might require a longer duration than <code class="literal">1800</code>.
								</div></dd></dl></div></li><li class="listitem">
							Navigate to the Red Hat Quay UI and click the name of your Organization.
						</li><li class="listitem">
							The <span class="strong strong"><strong>Total Quota Consumed</strong></span> should read <span class="strong strong"><strong>0.00 KB</strong></span>. Additionally, the <span class="strong strong"><strong>Backfill Queued</strong></span> indicator should be present.
						</li><li class="listitem">
							After the allotted time, for example, 30 minutes, refresh your Red Hat Quay deployment page and return to your Organization. Now, the <span class="strong strong"><strong>Total Quota Consumed</strong></span> should be present.
						</li></ol></div></section><section class="section" id="quota-management-configuring-39"><div class="titlepage"><div><div><h3 class="title">14.3.2. Option B: Configuring quota management for Red Hat Quay 3.9 by setting QUOTA_TOTAL_DELAY_SECONDS to 0</h3></div></div></div><p>
					Use the following procedure to recalculate Red Hat Quay 3.9 quota management by setting <code class="literal">QUOTA_TOTAL_DELAY_SECONDS</code> to <code class="literal">0</code>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Using this option prevents the possibility of miscalculations, however is more time intensive. Use the following procedure for when your Red Hat Quay deployment swaps the <code class="literal">FEATURE_QUOTA_MANAGEMENT</code> parameter from <code class="literal">false</code> to <code class="literal">true</code>. Most users will find xref:
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have upgraded to Red Hat Quay 3.9.
						</li><li class="listitem">
							You are logged into Red Hat Quay 3.9 as a superuser.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Deploy Red Hat Quay 3.9 with the following <code class="literal">config.yaml</code> settings:
						</p><pre class="programlisting language-yaml">FEATURE_GARBAGE_COLLECTION: true
FEATURE_QUOTA_MANAGEMENT: true
QUOTA_BACKFILL: false
QUOTA_TOTAL_DELAY_SECONDS: 0
PERMANENTLY_DELETE_TAGS: true
RESET_CHILD_MANIFEST_EXPIRATION: true</pre></li><li class="listitem">
							Navigate to the Red Hat Quay UI and click the name of your Organization.
						</li><li class="listitem">
							The <span class="strong strong"><strong>Total Quota Consumed</strong></span> should read <span class="strong strong"><strong>0.00 KB</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Redeploy Red Hat Quay and set the <code class="literal">QUOTA_BACKFILL</code> flag set to <code class="literal">true</code>. For example:
						</p><pre class="programlisting language-yaml">QUOTA_BACKFILL: true</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you choose to disable quota management after it has calculated totals, Red Hat Quay marks those totals as stale. If you re-enable the quota management feature again in the future, those namespaces and repositories are recalculated by the backfill worker.
							</p></div></div></li></ol></div></section></section><section class="section" id="quota-management-testing-39"><div class="titlepage"><div><div><h2 class="title">14.4. Testing quota management for Red Hat Quay 3.9</h2></div></div></div><p>
				With quota management configured for Red Hat Quay 3.9, duplicative images are now only counted once towards the repository total.
			</p><p>
				Use the following procedure to test that a duplicative image is only counted once toward the repository total.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have configured quota management for Red Hat Quay 3.9.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Pull a sample image, for example, <code class="literal">ubuntu:18.04</code>, by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman pull ubuntu:18.04</pre></li><li class="listitem"><p class="simpara">
						Tag the same image twice by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman tag docker.io/library/ubuntu:18.04 quay-server.example.com/quota-test/ubuntu:tag1</pre><pre class="programlisting language-terminal">$ podman tag docker.io/library/ubuntu:18.04 quay-server.example.com/quota-test/ubuntu:tag2</pre></li><li class="listitem"><p class="simpara">
						Push the sample image to your organization by entering the following commands:
					</p><pre class="programlisting language-terminal">$ podman push --tls-verify=false quay-server.example.com/quota-test/ubuntu:tag1</pre><pre class="programlisting language-terminal">$ podman push --tls-verify=false quay-server.example.com/quota-test/ubuntu:tag2</pre></li><li class="listitem"><p class="simpara">
						On the Red Hat Quay UI, navigate to <span class="strong strong"><strong>Organization</strong></span> and click the <span class="strong strong"><strong>Repository Name</strong></span>, for example, <span class="strong strong"><strong>quota-test/ubuntu</strong></span>. Then, click <span class="strong strong"><strong>Tags</strong></span>. There should be two repository tags, <code class="literal">tag1</code> and <code class="literal">tag2</code>, each with the same manifest. For example:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/manifest-example.png" alt="Manifest example"/></span>
					</p><p class="simpara">
						However, by clicking on the <span class="strong strong"><strong>Organization</strong></span> link, we can see that the <span class="strong strong"><strong>Total Quota Consumed</strong></span> is <span class="strong strong"><strong>27.94 MB</strong></span>, meaning that the Ubuntu image has only been accounted for once:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/total-quota-consumed.png" alt="Total quota consumed"/></span>
					</p><p class="simpara">
						If you delete one of the Ubuntu tags, the <span class="strong strong"><strong>Total Quota Consumed</strong></span> remains the same.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you have configured the Red Hat Quay time machine to be longer than <code class="literal">0</code> seconds, subtraction will not happen until those tags pass the time machine window. If you want to expedite permanent deletion, see Permanently deleting an image tag in Red Hat Quay 3.9.
						</p></div></div></li></ol></div></section><section class="section" id="default-quota"><div class="titlepage"><div><div><h2 class="title">14.5. Setting default quota</h2></div></div></div><p>
				To specify a system-wide default storage quota that is applied to every organization and user, you can use the <span class="strong strong"><strong>DEFAULT_SYSTEM_REJECT_QUOTA_BYTES</strong></span> configuration flag.
			</p><p>
				If you configure a specific quota for an organization or user, and then delete that quota, the system-wide default quota will apply if one has been set. Similarly, if you have configured a specific quota for an organization or user, and then modify the system-wide default quota, the updated system-wide default will override any specific settings.
			</p><p>
				For more information about the <code class="literal">DEFAULT_SYSTEM_REJECT_QUOTA_BYTES</code> flag,
			</p><p>
				see link:
			</p></section><section class="section" id="quota-establishment-ui"><div class="titlepage"><div><div><h2 class="title">14.6. Establishing quota in Red Hat Quay UI</h2></div></div></div><p>
				The following procedure describes how you can report storage consumption and establish storage quota limits.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A Red Hat Quay registry.
					</li><li class="listitem">
						A superuser account.
					</li><li class="listitem">
						Enough storage to meet the demands of quota limitations.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a new organization or choose an existing one. Initially, no quota is configured, as can be seen on the <span class="strong strong"><strong>Organization Settings</strong></span> tab:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-none-org-settings.png" alt="No Quota Configured"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Log in to the registry as a superuser and navigate to the <span class="strong strong"><strong>Manage Organizations</strong></span> tab on the <span class="strong strong"><strong>Super User Admin Panel</strong></span>. Click the <span class="strong strong"><strong>Options</strong></span> icon of the organization for which you want to create storage quota limits:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-org-options.png" alt="Organization options"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Configure Quota</strong></span> and enter the initial quota, for example, <span class="strong strong"><strong>10 MB</strong></span>. Then click <span class="strong strong"><strong>Apply</strong></span> and <span class="strong strong"><strong>Close</strong></span>:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-init-10MB.png" alt="Initial quota"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Check that the quota consumed shows <span class="strong strong"><strong>0 of 10 MB</strong></span> on the <span class="strong strong"><strong>Manage Organizations</strong></span> tab of the superuser panel:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-init-consumed.png" alt="Initial consumed quota"/></span>
					</p><p class="simpara">
						The consumed quota information is also available directly on the Organization page:
					</p><div class="formalpara"><p class="title"><strong>Initial consumed quota</strong></p><p>
							<span class="inlinemediaobject"><img src="images/quota-org-init-consumed.png" alt="Initial consumed quota"/></span>
						</p></div></li><li class="listitem"><p class="simpara">
						To increase the quota to 100MB, navigate to the <span class="strong strong"><strong>Manage Organizations</strong></span> tab on the superuser panel. Click the <span class="strong strong"><strong>Options</strong></span> icon and select <span class="strong strong"><strong>Configure Quota</strong></span>, setting the quota to 100 MB. Click <span class="strong strong"><strong>Apply</strong></span> and then <span class="strong strong"><strong>Close</strong></span>:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-increase-100MB.png" alt="Increase quota"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Pull a sample image by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman pull ubuntu:18.04</pre></li><li class="listitem"><p class="simpara">
						Tag the sample image by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman tag docker.io/library/ubuntu:18.04 example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:18.04</pre></li><li class="listitem"><p class="simpara">
						Push the sample image to the organization by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:18.04</pre></li><li class="listitem"><p class="simpara">
						On the superuser panel, the quota consumed per organization is displayed:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-consumed-first.png" alt="Total Quota Consumed for first image"/></span>
					</p></li><li class="listitem"><p class="simpara">
						The Organization page shows the total proportion of the quota used by the image:
					</p><div class="formalpara"><p class="title"><strong>Total Quota Consumed for first image</strong></p><p>
							<span class="inlinemediaobject"><img src="images/quota-org-consumed-first.png" alt="Total Quota Consumed for first image"/></span>
						</p></div></li><li class="listitem"><p class="simpara">
						Pull a second sample image by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman pull nginx</pre></li><li class="listitem"><p class="simpara">
						Tag the second image by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman tag docker.io/library/nginx example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/nginx</pre></li><li class="listitem"><p class="simpara">
						Push the second image to the organization by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/nginx</pre></li><li class="listitem"><p class="simpara">
						The Organization page shows the total proportion of the quota used by each repository in that organization:
					</p><div class="formalpara"><p class="title"><strong>Total Quota Consumed for each repository</strong></p><p>
							<span class="inlinemediaobject"><img src="images/quota-org-consumed-second.png" alt="Total Quota Consumed for each repository"/></span>
						</p></div></li><li class="listitem"><p class="simpara">
						Create <span class="emphasis"><em>reject</em></span> and <span class="emphasis"><em>warning</em></span> limits:
					</p><p class="simpara">
						From the superuser panel, navigate to the <span class="strong strong"><strong>Manage Organizations</strong></span> tab. Click the <span class="strong strong"><strong>Options</strong></span> icon for the organization and select <span class="strong strong"><strong>Configure Quota</strong></span>. In the <span class="strong strong"><strong>Quota Policy</strong></span> section, with the <span class="strong strong"><strong>Action</strong></span> type set to <span class="strong strong"><strong>Reject</strong></span>, set the <span class="strong strong"><strong>Quota Threshold</strong></span> to <span class="strong strong"><strong>80</strong></span> and click <span class="strong strong"><strong>Add Limit</strong></span>:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-reject-80.png" alt="Reject limit"/></span>
					</p></li><li class="listitem"><p class="simpara">
						To create a <span class="emphasis"><em>warning</em></span> limit, select <span class="strong strong"><strong>Warning</strong></span> as the <span class="strong strong"><strong>Action</strong></span> type, set the <span class="strong strong"><strong>Quota Threshold</strong></span> to <span class="strong strong"><strong>70</strong></span> and click <span class="strong strong"><strong>Add Limit</strong></span>:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-su-warning-70.png" alt="Warning limit"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Close</strong></span> on the quota popup. The limits are viewable, but not editable, on the <span class="strong strong"><strong>Settings</strong></span> tab of the <span class="strong strong"><strong>Organization</strong></span> page:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/quota-org-quota-policy.png" alt="Quota policy in organization settings"/></span>
					</p></li><li class="listitem"><p class="simpara">
						Push an image where the reject limit is exceeded:
					</p><p class="simpara">
						Because the reject limit (80%) has been set to below the current repository size (~83%), the next pushed image is rejected automatically.
					</p><div class="formalpara"><p class="title"><strong>Sample image push</strong></p><p>
							
<pre class="programlisting language-terminal">$ podman pull ubuntu:20.04

$ podman tag docker.io/library/ubuntu:20.04 example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:20.04

$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:20.04</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Sample output when quota exceeded</strong></p><p>
							
<pre class="programlisting language-terminal">Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0002] failed, retrying in 1s ... (1/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0005] failed, retrying in 1s ... (2/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0009] failed, retrying in 1s ... (3/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						When limits are exceeded, notifications are displayed in the UI:
					</p><div class="formalpara"><p class="title"><strong>Quota notifications</strong></p><p>
							<span class="inlinemediaobject"><img src="images/quota-notifications.png" alt="Quota notifications"/></span>
						</p></div></li></ol></div></section><section class="section" id="quota-establishment-api"><div class="titlepage"><div><div><h2 class="title">14.7. Establishing quota with the Red Hat Quay API</h2></div></div></div><p>
				When an organization is first created, it does not have a quota applied. Use the <span class="strong strong"><strong>/api/v1/organization/{organization}/quota</strong></span> endpoint:
			</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
					
<pre class="programlisting language-terminal">$ curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota  | jq</pre>
				</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
					
<pre class="programlisting language-terminal">[]</pre>
				</p></div><section class="section" id="setting_the_quota"><div class="titlepage"><div><div><h3 class="title">14.7.1. Setting the quota</h3></div></div></div><p>
					To set a quota for an organization, POST data to the <span class="strong strong"><strong>/api/v1/organization/{orgname}/quota</strong></span> endpoint: .Sample command
				</p><pre class="programlisting language-terminal">$ curl -k -X POST -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' -d '{"limit_bytes": 10485760}'  https://example-registry-quay-quay-enterprise.apps.docs.quayteam.org/api/v1/organization/testorg/quota | jq</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
						
<pre class="programlisting language-terminal">"Created"</pre>
					</p></div></section><section class="section" id="viewing_the_quota"><div class="titlepage"><div><div><h3 class="title">14.7.2. Viewing the quota</h3></div></div></div><p>
					To see the applied quota, <code class="literal">GET</code> data from the <span class="strong strong"><strong>/api/v1/organization/{orgname}/quota</strong></span> endpoint:
				</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
						
<pre class="programlisting language-terminal">$ curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota  | jq</pre>
					</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
						
<pre class="programlisting language-json">[
  {
    "id": 1,
    "limit_bytes": 10485760,
    "default_config": false,
    "limits": [],
    "default_config_exists": false
  }
]</pre>
					</p></div></section><section class="section" id="modifying_the_quota"><div class="titlepage"><div><div><h3 class="title">14.7.3. Modifying the quota</h3></div></div></div><p>
					To change the existing quota, in this instance from 10 MB to 100 MB, PUT data to the <span class="strong strong"><strong>/api/v1/organization/{orgname}/quota/{quota_id}</strong></span> endpoint:
				</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
						
<pre class="programlisting language-terminal">$ curl -k -X PUT -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' -d '{"limit_bytes": 104857600}'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota/1 | jq</pre>
					</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
						
<pre class="programlisting language-json">{
  "id": 1,
  "limit_bytes": 104857600,
  "default_config": false,
  "limits": [],
  "default_config_exists": false
}</pre>
					</p></div></section><section class="section" id="pushing_images"><div class="titlepage"><div><div><h3 class="title">14.7.4. Pushing images</h3></div></div></div><p>
					To see the storage consumed, push various images to the organization.
				</p><section class="section" id="pushing_ubuntu_18_04"><div class="titlepage"><div><div><h4 class="title">14.7.4.1. Pushing ubuntu:18.04</h4></div></div></div><p>
						Push ubuntu:18.04 to the organization from the command line:
					</p><div class="formalpara"><p class="title"><strong>Sample commands</strong></p><p>
							
<pre class="programlisting language-terminal">$ podman pull ubuntu:18.04

$ podman tag docker.io/library/ubuntu:18.04 example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:18.04

$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:18.04</pre>
						</p></div></section><section class="section" id="using_the_api_to_view_quota_usage"><div class="titlepage"><div><div><h4 class="title">14.7.4.2. Using the API to view quota usage</h4></div></div></div><p>
						To view the storage consumed, <code class="literal">GET</code> data from the <span class="strong strong"><strong>/api/v1/repository</strong></span> endpoint:
					</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
							
<pre class="programlisting language-terminal">$ curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' 'https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/repository?last_modified=true&amp;namespace=testorg&amp;popularity=true&amp;public=true&amp;quota=true' | jq</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
							
<pre class="programlisting language-json">{
  "repositories": [
    {
      "namespace": "testorg",
      "name": "ubuntu",
      "description": null,
      "is_public": false,
      "kind": "image",
      "state": "NORMAL",
      "quota_report": {
        "quota_bytes": 27959066,
        "configured_quota": 104857600
      },
      "last_modified": 1651225630,
      "popularity": 0,
      "is_starred": false
    }
  ]
}</pre>
						</p></div></section><section class="section" id="pushing_another_image"><div class="titlepage"><div><div><h4 class="title">14.7.4.3. Pushing another image</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Pull, tag, and push a second image, for example, <code class="literal">nginx</code>:
							</p><div class="formalpara"><p class="title"><strong>Sample commands</strong></p><p>
									
<pre class="programlisting language-terminal">$ podman pull nginx

$ podman tag docker.io/library/nginx example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/nginx

$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/nginx</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								To view the quota report for the repositories in the organization, use the <span class="strong strong"><strong>/api/v1/repository</strong></span> endpoint:
							</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
									
<pre class="programlisting language-terminal">$ curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' 'https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/repository?last_modified=true&amp;namespace=testorg&amp;popularity=true&amp;public=true&amp;quota=true'</pre>
								</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
									
<pre class="programlisting language-json">{
  "repositories": [
    {
      "namespace": "testorg",
      "name": "ubuntu",
      "description": null,
      "is_public": false,
      "kind": "image",
      "state": "NORMAL",
      "quota_report": {
        "quota_bytes": 27959066,
        "configured_quota": 104857600
      },
      "last_modified": 1651225630,
      "popularity": 0,
      "is_starred": false
    },
    {
      "namespace": "testorg",
      "name": "nginx",
      "description": null,
      "is_public": false,
      "kind": "image",
      "state": "NORMAL",
      "quota_report": {
        "quota_bytes": 59231659,
        "configured_quota": 104857600
      },
      "last_modified": 1651229507,
      "popularity": 0,
      "is_starred": false
    }
  ]
}</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								To view the quota information in the organization details, use the <span class="strong strong"><strong>/api/v1/organization/{orgname}</strong></span> endpoint:
							</p><div class="formalpara"><p class="title"><strong>Sample command</strong></p><p>
									
<pre class="programlisting language-terminal">$ curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' 'https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg' | jq</pre>
								</p></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
									
<pre class="programlisting language-json">{
  "name": "testorg",
  ...
  "quotas": [
    {
      "id": 1,
      "limit_bytes": 104857600,
      "limits": []
    }
  ],
  "quota_report": {
    "quota_bytes": 87190725,
    "configured_quota": 104857600
  }
}</pre>
								</p></div></li></ol></div></section></section><section class="section" id="rejecting_pushes_using_quota_limits"><div class="titlepage"><div><div><h3 class="title">14.7.5. Rejecting pushes using quota limits</h3></div></div></div><p>
					If an image push exceeds defined quota limitations, a soft or hard check occurs:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For a soft check, or <span class="emphasis"><em>warning</em></span>, users are notified.
						</li><li class="listitem">
							For a hard check, or <span class="emphasis"><em>reject</em></span>, the push is terminated.
						</li></ul></div><section class="section" id="setting_reject_and_warning_limits"><div class="titlepage"><div><div><h4 class="title">14.7.5.1. Setting reject and warning limits</h4></div></div></div><p>
						To set <span class="emphasis"><em>reject</em></span> and <span class="emphasis"><em>warning</em></span> limits, POST data to the <span class="strong strong"><strong>/api/v1/organization/{orgname}/quota/{quota_id}/limit</strong></span> endpoint:
					</p><div class="formalpara"><p class="title"><strong>Sample reject limit command</strong></p><p>
							
<pre class="programlisting language-terminal">$ curl -k -X POST -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' -d '{"type":"Reject","threshold_percent":80}'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota/1/limit</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Sample warning limit command</strong></p><p>
							
<pre class="programlisting language-terminal">$ curl -k -X POST -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json' -d '{"type":"Warning","threshold_percent":50}'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota/1/limit</pre>
						</p></div></section><section class="section" id="viewing_reject_and_warning_limits"><div class="titlepage"><div><div><h4 class="title">14.7.5.2. Viewing reject and warning limits</h4></div></div></div><p>
						To view the <span class="emphasis"><em>reject</em></span> and <span class="emphasis"><em>warning</em></span> limits, use the <span class="strong strong"><strong>/api/v1/organization/{orgname}/quota</strong></span> endpoint:
					</p><div class="formalpara"><p class="title"><strong>View quota limits</strong></p><p>
							
<pre class="programlisting language-terminal">$  curl -k -X GET -H "Authorization: Bearer &lt;token&gt;" -H 'Content-Type: application/json'  https://example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/api/v1/organization/testorg/quota | jq</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Sample output for quota limits</strong></p><p>
							
<pre class="programlisting language-json">[
  {
    "id": 1,
    "limit_bytes": 104857600,
    "default_config": false,
    "limits": [
      {
        "id": 2,
        "type": "Warning",
        "limit_percent": 50
      },
      {
        "id": 1,
        "type": "Reject",
        "limit_percent": 80
      }
    ],
    "default_config_exists": false
  }
]</pre>
						</p></div></section><section class="section" id="pushing_an_image_when_the_reject_limit_is_exceeded"><div class="titlepage"><div><div><h4 class="title">14.7.5.3. Pushing an image when the reject limit is exceeded</h4></div></div></div><p>
						In this example, the reject limit (80%) has been set to below the current repository size (~83%), so the next push should automatically be rejected.
					</p><p>
						Push a sample image to the organization from the command line:
					</p><div class="formalpara"><p class="title"><strong>Sample image push</strong></p><p>
							
<pre class="programlisting language-terminal">$ podman pull ubuntu:20.04

$ podman tag docker.io/library/ubuntu:20.04 example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:20.04

$ podman push --tls-verify=false example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org/testorg/ubuntu:20.04</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Sample output when quota exceeded</strong></p><p>
							
<pre class="programlisting language-terminal">Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0002] failed, retrying in 1s ... (1/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0005] failed, retrying in 1s ... (2/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
WARN[0009] failed, retrying in 1s ... (3/3). Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace
Getting image source signatures
Copying blob d4dfaa212623 [--------------------------------------] 8.0b / 3.5KiB
Copying blob cba97cc5811c [--------------------------------------] 8.0b / 15.0KiB
Copying blob 0c78fac124da [--------------------------------------] 8.0b / 71.8MiB
Error: Error writing blob: Error initiating layer upload to /v2/testorg/ubuntu/blobs/uploads/ in example-registry-quay-quay-enterprise.apps.docs.gcp.quaydev.org: denied: Quota has been exceeded on namespace</pre>
						</p></div></section><section class="section" id="notifications_for_limits_exceeded"><div class="titlepage"><div><div><h4 class="title">14.7.5.4. Notifications for limits exceeded</h4></div></div></div><p>
						When limits are exceeded, a notification appears:
					</p><div class="formalpara"><p class="title"><strong>Quota notifications</strong></p><p>
							<span class="inlinemediaobject"><img src="images/quota-notifications.png" alt="Quota notifications"/></span>
						</p></div></section></section></section><section class="section" id="quota-management-query-39"><div class="titlepage"><div><div><h2 class="title">14.8. Calculating the total registry size in Red Hat Quay 3.9</h2></div></div></div><p>
				Use the following procedure to queue a registry total calculation.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This feature is done on-demand, and calculating a registry total is database intensive. Use with caution.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have upgraded to Red Hat Quay 3.9.
					</li><li class="listitem">
						You are logged in as a Red Hat Quay superuser.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						On the Red Hat Quay UI, click your username → <span class="strong strong"><strong>Super User Admin Panel</strong></span>.
					</li><li class="listitem">
						In the navigation pane, click <span class="strong strong"><strong>Manage Organizations</strong></span>.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Calculate</strong></span>, next to <span class="strong strong"><strong>Total Registry Size: 0.00 KB, Updated: Never , Calculation required</strong></span>. Then, click <span class="strong strong"><strong>Ok</strong></span>.
					</li><li class="listitem"><p class="simpara">
						After a few minutes, depending on the size of your registry, refresh the page. Now, the Total Registry Size should be calculated. For example:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/total-registry-size.png" alt="Total registry size"/></span>
					</p></li></ol></div></section><section class="section" id="deleting-tag-permanently"><div class="titlepage"><div><div><h2 class="title">14.9. Permanently deleting an image tag</h2></div></div></div><p>
				In some cases, users might want to delete an image tag outside of the time machine window. Use the following procedure to manually delete an image tag permanently.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The results of the following procedure cannot be undone. Use with caution.
				</p></div></div><section class="section" id="permanently-deleting-image-tag-v2-ui"><div class="titlepage"><div><div><h3 class="title">14.9.1. Permanently deleting an image tag using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to permanently delete an image tag using the Red Hat Quay v2 UI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have set <code class="literal">FEATURE_UI_V2</code> to <code class="literal">true</code> in your <code class="literal">config.yaml</code> file.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal">PERMANENTLY_DELETE_TAGS</code> and <code class="literal">RESET_CHILD_MANIFEST_EXPIRATION</code> parameters are set to <code class="literal">true</code> in your <code class="literal">config.yaml</code> file. For example:
						</p><pre class="programlisting language-yaml">PERMANENTLY_DELETE_TAGS: true
RESET_CHILD_MANIFEST_EXPIRATION: true</pre></li><li class="listitem">
							In the navigation pane, click <span class="strong strong"><strong>Repositories</strong></span>.
						</li><li class="listitem">
							Click the name of the repository, for example, <span class="strong strong"><strong>quayadmin/busybox</strong></span>.
						</li><li class="listitem">
							Check the box of the image tag that will be deleted, for example, <span class="strong strong"><strong>test</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Actions</strong></span> → <span class="strong strong"><strong>Permanently Delete</strong></span>.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								This action is permanent and cannot be undone.
							</p></div></div></li></ol></div></section><section class="section" id="permanently-deleting-image-tag-legacy-ui"><div class="titlepage"><div><div><h3 class="title">14.9.2. Permanently deleting an image tag using the Red Hat Quay legacy UI</h3></div></div></div><p>
					Use the following procedure to permanently delete an image tag using the Red Hat Quay legacy UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal">PERMANENTLY_DELETE_TAGS</code> and <code class="literal">RESET_CHILD_MANIFEST_EXPIRATION</code> parameters are set to <code class="literal">true</code> in your <code class="literal">config.yaml</code> file. For example:
						</p><pre class="programlisting language-yaml">PERMANENTLY_DELETE_TAGS: true
RESET_CHILD_MANIFEST_EXPIRATION: true</pre></li><li class="listitem">
							On the Red Hat Quay UI, click <span class="strong strong"><strong>Repositories</strong></span> and the name of the repository that contains the image tag you will delete, for example, <span class="strong strong"><strong>quayadmin/busybox</strong></span>.
						</li><li class="listitem">
							In the navigation pane, click <span class="strong strong"><strong>Tags</strong></span>.
						</li><li class="listitem">
							Check the box of the name of the tag you want to delete, for example, <span class="strong strong"><strong>test</strong></span>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Actions</strong></span> drop down menu and select <span class="strong strong"><strong>Delete Tags</strong></span> → <span class="strong strong"><strong>Delete Tag</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Tag History</strong></span> in the navigation pane.
						</li><li class="listitem"><p class="simpara">
							On the name of the tag that was just deleted, for example, <code class="literal">test</code>, click <span class="strong strong"><strong>Delete test</strong></span> under the <span class="strong strong"><strong>Permanently Delete</strong></span> category. For example:
						</p><div class="formalpara"><p class="title"><strong>Permanently delete image tag</strong></p><p>
								<span class="inlinemediaobject"><img src="images/permanently-delete-image-tag.png" alt="Permanently delete image tag"/></span>
							</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								This action is permanent and cannot be undone.
							</p></div></div></li></ol></div></section></section></section><section class="chapter" id="georepl-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Geo-replication</h1></div></div></div><p>
			Geo-replication allows multiple, geographically distributed Red Hat Quay deployments to work as a single registry from the perspective of a client or user. It significantly improves push and pull performance in a globally-distributed Red Hat Quay setup. Image data is asynchronously replicated in the background with transparent failover and redirect for clients.
		</p><p>
			Deployments of Red Hat Quay with geo-replication is supported on standalone and Operator deployments.
		</p><section class="section" id="arch-georpl-features"><div class="titlepage"><div><div><h2 class="title">15.1. Geo-replication features</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						When geo-replication is configured, container image pushes will be written to the preferred storage engine for that Red Hat Quay instance. This is typically the nearest storage backend within the region.
					</li><li class="listitem">
						After the initial push, image data will be replicated in the background to other storage engines.
					</li><li class="listitem">
						The list of replication locations is configurable and those can be different storage backends.
					</li><li class="listitem">
						An image pull will always use the closest available storage engine, to maximize pull performance.
					</li><li class="listitem">
						If replication has not been completed yet, the pull will use the source storage backend instead.
					</li></ul></div></section><section class="section" id="arch-georepl-prereqs"><div class="titlepage"><div><div><h2 class="title">15.2. Geo-replication requirements and constraints</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						In geo-replicated setups, Red Hat Quay requires that all regions are able to read and write to all other region’s object storage. Object storage must be geographically accessible by all other regions.
					</li><li class="listitem">
						In case of an object storage system failure of one geo-replicating site, that site’s Red Hat Quay deployment must be shut down so that clients are redirected to the remaining site with intact storage systems by a global load balancer. Otherwise, clients will experience pull and push failures.
					</li><li class="listitem">
						Red Hat Quay has no internal awareness of the health or availability of the connected object storage system. If the object storage system of one site becomes unavailable, there will be no automatic redirect to the remaining storage system, or systems, of the remaining site, or sites.
					</li><li class="listitem">
						Geo-replication is asynchronous. The permanent loss of a site incurs the loss of the data that has been saved in that sites' object storage system but has not yet been replicated to the remaining sites at the time of failure.
					</li><li class="listitem"><p class="simpara">
						A single database, and therefore all metadata and Red Hat Quay configuration, is shared across all regions.
					</p><p class="simpara">
						Geo-replication does not replicate the database. In the event of an outage, Red Hat Quay with geo-replication enabled will not failover to another database.
					</p></li><li class="listitem">
						A single Redis cache is shared across the entire Red Hat Quay setup and needs to accessible by all Red Hat Quay pods.
					</li><li class="listitem">
						The exact same configuration should be used across all regions, with exception of the storage backend, which can be configured explicitly using the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environment variable.
					</li><li class="listitem">
						Geo-replication requires object storage in each region. It does not work with local storage.
					</li><li class="listitem">
						Each region must be able to access every storage engine in each region, which requires a network path.
					</li><li class="listitem">
						Alternatively, the storage proxy option can be used.
					</li><li class="listitem">
						The entire storage backend, for example, all blobs, is replicated. Repository mirroring, by contrast, can be limited to a repository, or an image.
					</li><li class="listitem">
						All Red Hat Quay instances must share the same entrypoint, typically through a load balancer.
					</li><li class="listitem">
						All Red Hat Quay instances must have the same set of superusers, as they are defined inside the common configuration file.
					</li><li class="listitem">
						Geo-replication requires your Clair configuration to be set to <code class="literal">unmanaged</code>. An unmanaged Clair database allows the Red Hat Quay Operator to work in a geo-replicated environment, where multiple instances of the Red Hat Quay Operator must communicate with the same database. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#clair-unmanaged">Advanced Clair configuration</a>.
					</li><li class="listitem">
						Geo-Replication requires SSL/TLS certificates and keys. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/index#using_ssl_to_protect_connections_to_red_hat_quay">Using SSL/TLS to protect connections to Red Hat Quay</a>.
					</li></ul></div><p>
				If the above requirements cannot be met, you should instead use two or more distinct Red Hat Quay deployments and take advantage of repository mirroring functions.
			</p></section><section class="section" id="georepl-arch-standalone"><div class="titlepage"><div><div><h2 class="title">15.3. Geo-replication using standalone Red Hat Quay</h2></div></div></div><p>
				In the following image, Red Hat Quay is running standalone in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Red Hat Quay instance, and will then be replicated, in the background, to the other storage engines.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If Clair fails in one cluster, for example, the US cluster, US users would not see vulnerability reports in Red Hat Quay for the second cluster (EU). This is because all Clair instances have the same state. When Clair fails, it is usually because of a problem within the cluster.
				</p></div></div><div class="formalpara"><p class="title"><strong>Geo-replication architecture</strong></p><p>
					<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_georeplication.png" alt="Geo-replication"/></span>
				</p></div><section class="section" id="enable-storage-replication-standalone"><div class="titlepage"><div><div><h3 class="title">15.3.1. Enable storage replication - standalone Quay</h3></div></div></div><p>
					Use the following procedure to enable storage replication on Red Hat Quay.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In your Red Hat Quay config editor, locate the <span class="strong strong"><strong>Registry Storage</strong></span> section.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Enable Storage Replication</strong></span>.
						</li><li class="listitem">
							Add each of the storage engines to which data will be replicated. All storage engines to be used must be listed.
						</li><li class="listitem"><p class="simpara">
							If complete replication of all images to all storage engines is required, click <span class="strong strong"><strong>Replicate to storage engine by default</strong></span> under each storage engine configuration. This ensures that all images are replicated to that storage engine.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To enable per-namespace replication, contact Red Hat Quay support.
							</p></div></div></li><li class="listitem">
							When finished, click <span class="strong strong"><strong>Save Configuration Changes</strong></span>. The configuration changes will take effect after Red Hat Quay restarts.
						</li><li class="listitem"><p class="simpara">
							After adding storage and enabling <span class="strong strong"><strong>Replicate to storage engine by default</strong></span> for geo-replication, you must sync existing image data across all storage. To do this, you must <code class="literal">oc exec</code> (alternatively, <code class="literal">docker exec</code> or <code class="literal">kubectl exec</code>) into the container and enter the following commands:
						</p><pre class="programlisting language-terminal"># scl enable python27 bash
# python -m util.backfillreplication</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This is a one time operation to sync content after adding new storage.
							</p></div></div></li></ol></div></section><section class="section" id="georepl-deploy-standalone"><div class="titlepage"><div><div><h3 class="title">15.3.2. Run Red Hat Quay with storage preferences</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Copy the config.yaml to all machines running Red Hat Quay
						</li><li class="listitem"><p class="simpara">
							For each machine in each region, add a <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environment variable with the preferred storage engine for the region in which the machine is running.
						</p><p class="simpara">
							For example, for a machine running in Europe with the config directory on the host available from <code class="literal">$QUAY/config</code>:
						</p><pre class="screen">$ sudo podman run -d --rm -p 80:8080 -p 443:8443  \
   --name=quay \
   -v $QUAY/config:/conf/stack:Z \
   -e QUAY_DISTRIBUTED_STORAGE_PREFERENCE=europestorage \
   registry.redhat.io/quay/quay-rhel8:v3.9.0</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The value of the environment variable specified must match the name of a Location ID as defined in the config panel.
							</p></div></div></li><li class="listitem">
							Restart all Red Hat Quay containers
						</li></ol></div></section><section class="section" id="standalone-georepl-site-removal"><div class="titlepage"><div><div><h3 class="title">15.3.3. Removing a geo-replicated site from your standalone Red Hat Quay deployment</h3></div></div></div><p>
					By using the following procedure, Red Hat Quay administrators can remove sites in a geo-replicated setup.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured Red Hat Quay geo-replication with at least two sites, for example, <code class="literal">usstorage</code> and <code class="literal">eustorage</code>.
						</li><li class="listitem">
							Each site has its own Organization, Repository, and image tags.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Sync the blobs between all of your defined sites by running the following command:
						</p><pre class="programlisting language-terminal">$ python -m util.backfillreplication</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Prior to removing storage engines from your Red Hat Quay <code class="literal">config.yaml</code> file, you <span class="strong strong"><strong>must</strong></span> ensure that all blobs are synced between all defined sites. Complete this step before proceeding.
							</p></div></div></li><li class="listitem">
							In your Red Hat Quay <code class="literal">config.yaml</code> file for site <code class="literal">usstorage</code>, remove the <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> entry for the <code class="literal">eustorage</code> site.
						</li><li class="listitem"><p class="simpara">
							Enter the following command to obtain a list of running containers:
						</p><pre class="programlisting language-terminal">$ podman ps</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">CONTAINER ID  IMAGE                                                                     COMMAND         CREATED         STATUS             PORTS                                        NAMES
92c5321cde38  registry.redhat.io/rhel8/redis-5:1                                        run-redis       11 days ago     Up 11 days ago     0.0.0.0:6379-&gt;6379/tcp                       redis
4e6d1ecd3811  registry.redhat.io/rhel8/postgresql-13:1-109                              run-postgresql  33 seconds ago  Up 34 seconds ago  0.0.0.0:5432-&gt;5432/tcp                       postgresql-quay
d2eadac74fda  registry-proxy.engineering.redhat.com/rh-osbs/quay-quay-rhel8:v3.9.0-131  registry        4 seconds ago   Up 4 seconds ago   0.0.0.0:80-&gt;8080/tcp, 0.0.0.0:443-&gt;8443/tcp  quay</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Enter the following command to execute a shell inside of the PostgreSQL container:
						</p><pre class="programlisting language-terminal">$ podman exec -it postgresql-quay -- /bin/bash</pre></li><li class="listitem"><p class="simpara">
							Enter psql by running the following command:
						</p><pre class="programlisting language-terminal">bash-4.4$ psql</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to reveal a list of sites in your geo-replicated deployment:
						</p><pre class="programlisting language-terminal">quay=# select * from imagestoragelocation;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal"> id |       name
----+-------------------
  1 | usstorage
  2 | eustorage</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Enter the following command to exit the postgres CLI to re-enter bash-4.4:
						</p><pre class="programlisting language-terminal">\q</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to permanently remove the <code class="literal">eustorage</code> site:
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								The following action cannot be undone. Use with caution.
							</p></div></div><pre class="programlisting language-terminal">bash-4.4$ python -m util.removelocation eustorage</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">WARNING: This is a destructive operation. Are you sure you want to remove eustorage from your storage locations? [y/n] y
Deleted placement 30
Deleted placement 31
Deleted placement 32
Deleted placement 33
Deleted location eustorage</pre>
							</p></div></li></ol></div></section></section><section class="section" id="georepl-arch-operator"><div class="titlepage"><div><div><h2 class="title">15.4. Geo-replication using the Red Hat Quay Operator</h2></div></div></div><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_georeplication_openshift-temp.png" alt="Geo-replication architecture"/></span>
			</p><p>
				In the example shown above, the Red Hat Quay Operator is deployed in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines.
			</p><p>
				Because the Operator now manages the Clair security scanner and its database separately, geo-replication setups can be leveraged so that they do not manage the Clair database. Instead, an external shared database would be used. Red Hat Quay and Clair support several providers and vendors of PostgreSQL, which can be found in the Red Hat Quay 3.x <a class="link" href="https://access.redhat.com/articles/4067991">test matrix</a>. Additionally, the Operator also supports custom Clair configurations that can be injected into the deployment, which allows users to configure Clair with the connection credentials for the external database.
			</p><section class="section" id="georepl-deploy-operator"><div class="titlepage"><div><div><h3 class="title">15.4.1. Setting up geo-replication on OpenShift Container Platform</h3></div></div></div><p>
					Use the following procedure to set up geo-replication on OpenShift Container Platform.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Deploy a postgres instance for Red Hat Quay.
						</li><li class="listitem"><p class="simpara">
							Login to the database by entering the following command:
						</p><pre class="programlisting language-terminal">psql -U &lt;username&gt; -h &lt;hostname&gt; -p &lt;port&gt; -d &lt;database_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Create a database for Red Hat Quay named <code class="literal">quay</code>. For example:
						</p><pre class="programlisting language-terminal">CREATE DATABASE quay;</pre></li><li class="listitem"><p class="simpara">
							Enable pg_trm extension inside the database
						</p><pre class="programlisting language-terminal">\c quay;
CREATE EXTENSION IF NOT EXISTS pg_trgm;</pre></li><li class="listitem"><p class="simpara">
							Deploy a Redis instance:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Deploying a Redis instance might be unnecessary if your cloud provider has its own service.
									</li><li class="listitem">
										Deploying a Redis instance is required if you are leveraging Builders.
									</li></ul></div></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Deploy a VM for Redis
								</li><li class="listitem">
									Verify that it is accessible from the clusters where Red Hat Quay is running
								</li><li class="listitem">
									Port 6379/TCP must be open
								</li><li class="listitem"><p class="simpara">
									Run Redis inside the instance
								</p><pre class="programlisting language-terminal">sudo dnf install -y podman
podman run -d --name redis -p 6379:6379 redis</pre></li></ol></div></li><li class="listitem">
							Create two object storage backends, one for each cluster. Ideally, one object storage bucket will be close to the first, or primary, cluster, and the other will run closer to the second, or secondary, cluster.
						</li><li class="listitem">
							Deploy the clusters with the same config bundle, using environment variable overrides to select the appropriate storage backend for an individual cluster.
						</li><li class="listitem">
							Configure a load balancer to provide a single entry point to the clusters.
						</li></ol></div><section class="section" id="configuring-geo-repl"><div class="titlepage"><div><div><h4 class="title">15.4.1.1. Configuring geo-replication for the Red Hat Quay Operator on OpenShift Container Platform</h4></div></div></div><p>
						Use the following procedure to configure geo-replication for the Red Hat Quay Operator.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">config.yaml</code> file that is shared between clusters. This <code class="literal">config.yaml</code> file contains the details for the common PostgreSQL, Redis and storage backends:
							</p><div class="formalpara"><p class="title"><strong>Geo-replication <code class="literal">config.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">SERVER_HOSTNAME: &lt;georep.quayteam.org or any other name&gt; <span id="CO5-1"/><span class="callout">1</span>
DB_CONNECTION_ARGS:
  autorollback: true
  threadlocals: true
DB_URI: postgresql://postgres:password@10.19.0.1:5432/quay <span id="CO5-2"/><span class="callout">2</span>
BUILDLOGS_REDIS:
  host: 10.19.0.2
  port: 6379
USER_EVENTS_REDIS:
  host: 10.19.0.2
  port: 6379
DISTRIBUTED_STORAGE_CONFIG:
  usstorage:
    - GoogleCloudStorage
    - access_key: GOOGQGPGVMASAAMQABCDEFG
      bucket_name: georep-test-bucket-0
      secret_key: AYWfEaxX/u84XRA2vUX5C987654321
      storage_path: /quaygcp
  eustorage:
    - GoogleCloudStorage
    - access_key: GOOGQGPGVMASAAMQWERTYUIOP
      bucket_name: georep-test-bucket-1
      secret_key: AYWfEaxX/u84XRA2vUX5Cuj12345678
      storage_path: /quaygcp
DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS:
  - usstorage
  - eustorage
DISTRIBUTED_STORAGE_PREFERENCE:
  - usstorage
  - eustorage
FEATURE_STORAGE_REPLICATION: true</pre>
								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										A proper <code class="literal">SERVER_HOSTNAME</code> must be used for the route and must match the hostname of the global load balancer.
									</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										To retrieve the configuration file for a Clair instance deployed using the OpenShift Container Platform Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/quay_operator_features#clair-openshift-config">Retrieving the Clair config</a>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">configBundleSecret</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create secret generic --from-file config.yaml=./config.yaml georep-config-bundle</pre></li><li class="listitem"><p class="simpara">
								In each of the clusters, set the <code class="literal">configBundleSecret</code> and use the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environmental variable override to configure the appropriate storage for that cluster. For example:
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">config.yaml</code> file between both deployments must match. If making a change to one cluster, it must also be changed in the other.
								</p></div></div><div class="formalpara"><p class="title"><strong>US cluster <code class="literal">QuayRegistry</code> example</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: georep-config-bundle
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: postgres
      managed: false
    - kind: clairpostgres
      managed: false
    - kind: redis
      managed: false
    - kind: quay
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: usstorage
    - kind: mirror
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: usstorage</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Because SSL/TLS is unmanaged, and the route is managed, you must supply the certificates with either with the config tool or directly in the config bundle. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-preconfigure#operator-preconfig-tls-routes">Configuring TLS and routes</a>.
								</p></div></div><div class="formalpara"><p class="title"><strong>European cluster</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: example-registry
  namespace: quay-enterprise
spec:
  configBundleSecret: georep-config-bundle
  components:
    - kind: objectstorage
      managed: false
    - kind: route
      managed: true
    - kind: tls
      managed: false
    - kind: postgres
      managed: false
    - kind: clairpostgres
      managed: false
    - kind: redis
      managed: false
    - kind: quay
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: eustorage
    - kind: mirror
      managed: true
      overrides:
        env:
        - name: QUAY_DISTRIBUTED_STORAGE_PREFERENCE
          value: eustorage</pre>
								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Because SSL/TLS is unmanaged, and the route is managed, you must supply the certificates with either with the config tool or directly in the config bundle. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/operator-preconfigure#operator-preconfig-tls-routes">Configuring TLS and routes</a>.
								</p></div></div></li></ol></div></section></section><section class="section" id="operator-georepl-site-removal"><div class="titlepage"><div><div><h3 class="title">15.4.2. Removing a geo-replicated site from your Red Hat Quay Operator deployment</h3></div></div></div><p>
					By using the following procedure, Red Hat Quay administrators can remove sites in a geo-replicated setup.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged into OpenShift Container Platform.
						</li><li class="listitem">
							You have configured Red Hat Quay geo-replication with at least two sites, for example, <code class="literal">usstorage</code> and <code class="literal">eustorage</code>.
						</li><li class="listitem">
							Each site has its own Organization, Repository, and image tags.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Sync the blobs between all of your defined sites by running the following command:
						</p><pre class="programlisting language-terminal">$ python -m util.backfillreplication</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Prior to removing storage engines from your Red Hat Quay <code class="literal">config.yaml</code> file, you <span class="strong strong"><strong>must</strong></span> ensure that all blobs are synced between all defined sites. Complete this step before proceeding.
							</p></div></div></li><li class="listitem">
							In your Red Hat Quay <code class="literal">config.yaml</code> file for site <code class="literal">usstorage</code>, remove the <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> entry for the <code class="literal">eustorage</code> site.
						</li><li class="listitem"><p class="simpara">
							Enter the following command to identify your <code class="literal">Quay</code> application pods:
						</p><pre class="programlisting language-terminal">$ oc get pod -n &lt;quay_namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">quay390usstorage-quay-app-5779ddc886-2drh2
quay390eustorage-quay-app-66969cd859-n2ssm</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Enter the following command to open an interactive shell session in the <code class="literal">usstorage</code> pod:
						</p><pre class="programlisting language-terminal">$ oc rsh quay390usstorage-quay-app-5779ddc886-2drh2</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to permanently remove the <code class="literal">eustorage</code> site:
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								The following action cannot be undone. Use with caution.
							</p></div></div><pre class="programlisting language-terminal">sh-4.4$ python -m util.removelocation eustorage</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">WARNING: This is a destructive operation. Are you sure you want to remove eustorage from your storage locations? [y/n] y
Deleted placement 30
Deleted placement 31
Deleted placement 32
Deleted placement 33
Deleted location eustorage</pre>
							</p></div></li></ol></div></section></section><section class="section" id="georepl-mixed-storage"><div class="titlepage"><div><div><h2 class="title">15.5. Mixed storage for geo-replication</h2></div></div></div><p>
				Red Hat Quay geo-replication supports the use of different and multiple replication targets, for example, using AWS S3 storage on public cloud and using Ceph storage on premise. This complicates the key requirement of granting access to all storage backends from all Red Hat Quay pods and cluster nodes. As a result, it is recommended that you use the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						A VPN to prevent visibility of the internal storage, <span class="emphasis"><em>or</em></span>
					</li><li class="listitem">
						A token pair that only allows access to the specified bucket used by Red Hat Quay
					</li></ul></div><p>
				This results in the public cloud instance of Red Hat Quay having access to on-premise storage, but the network will be encrypted, protected, and will use ACLs, thereby meeting security requirements.
			</p><p>
				If you cannot implement these security measures, it might be preferable to deploy two distinct Red Hat Quay registries and to use repository mirroring as an alternative to geo-replication.
			</p></section></section><section class="chapter" id="backing-up-and-restoring-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Backing up and restoring Red Hat Quay managed by the Red Hat Quay Operator</h1></div></div></div><p>
			Use the content within this section to back up and restore Red Hat Quay when managed by the Red Hat Quay Operator on OpenShift Container Platform
		</p><section class="section" id="backing-up-red-hat-quay-operator"><div class="titlepage"><div><div><h2 class="title">16.1. Backing up Red Hat Quay</h2></div></div></div><p>
				This procedure describes how to create a backup of Red Hat Quay deployed on OpenShift Container Platform using the Red Hat Quay Operator
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A healthy Red Hat Quay deployment on OpenShift Container Platform using the Red Hat Quay Operator. The status condition <code class="literal">Available</code> is set to <code class="literal">true</code>.
					</li><li class="listitem">
						The components <code class="literal">quay</code>, <code class="literal">postgres</code> and <code class="literal">objectstorage</code> are set to <code class="literal">managed: true</code>
					</li><li class="listitem">
						If the component <code class="literal">clair</code> is set to <code class="literal">managed: true</code> the component <code class="literal">clairpostgres</code> is also set to <code class="literal">managed: true</code> (starting with Red Hat Quay Operator v3.7 or later)
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If your deployment contains partially unmanaged database or storage components and you are using external services for PostgreSQL or S3-compatible object storage to run your Red Hat Quay deployment, you must refer to the service provider or vendor documentation to create a backup of the data. You can refer to the tools described in this guide as a starting point on how to backup your external PostgreSQL database or object storage.
				</p></div></div><section class="section" id="quay-configuration-backup"><div class="titlepage"><div><div><h3 class="title">16.1.1. Red Hat Quay configuration backup</h3></div></div></div><p>
					Use the following procedure to back up your Red Hat Quay configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To back the <code class="literal">QuayRegistry</code> custom resource by exporting it, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get quayregistry &lt;quay_registry_name&gt; -n &lt;quay_namespace&gt; -o yaml &gt; quay-registry.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the resulting <code class="literal">quayregistry.yaml</code> and remove the status section and the following metadata fields:
						</p><pre class="programlisting language-yaml">  metadata.creationTimestamp
  metadata.finalizers
  metadata.generation
  metadata.resourceVersion
  metadata.uid</pre></li><li class="listitem"><p class="simpara">
							Backup the managed keys secret by entering the following command:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you are running a version older than Red Hat Quay 3.7.0, this step can be skipped. Some secrets are automatically generated while deploying Red Hat Quay for the first time. These are stored in a secret called <code class="literal">&lt;quay_registry_name&gt;-quay_registry_managed_secret_keys</code> in the namespace of the <code class="literal">QuayRegistry</code> resource.
							</p></div></div><pre class="programlisting language-terminal">$ oc get secret -n &lt;quay_namespace&gt; &lt;quay_registry_name&gt;_quay_registry_managed_secret_keys -o yaml &gt; managed_secret_keys.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the resulting <code class="literal">managed_secret_keys.yaml</code> file and remove the entry <code class="literal">metadata.ownerReferences</code>. Your <code class="literal">managed_secret_keys.yaml</code> file should look similar to the following:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: &lt;quayname&gt;_quay_registry_managed_secret_keys
  namespace: &lt;quay_namespace&gt;
data:
  CONFIG_EDITOR_PW: &lt;redacted&gt;
  DATABASE_SECRET_KEY: &lt;redacted&gt;
  DB_ROOT_PW: &lt;redacted&gt;
  DB_URI: &lt;redacted&gt;
  SECRET_KEY: &lt;redacted&gt;
  SECURITY_SCANNER_V4_PSK: &lt;redacted&gt;</pre><p class="simpara">
							All information under the <code class="literal">data</code> property should remain the same.
						</p></li><li class="listitem"><p class="simpara">
							Redirect the current <code class="literal">Quay</code> configuration file by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret -n &lt;quay-namespace&gt;  $(oc get quayregistry &lt;quay_registry_name&gt; -n &lt;quay_namespace&gt;  -o jsonpath='{.spec.configBundleSecret}') -o yaml &gt; config-bundle.yaml</pre></li><li class="listitem"><p class="simpara">
							Backup the <code class="literal">/conf/stack/config.yaml</code> file mounted inside of the <code class="literal">Quay</code> pods:
						</p><pre class="programlisting language-terminal">$ oc exec -it quay_pod_name -- cat /conf/stack/config.yaml &gt; quay_config.yaml</pre></li></ol></div></section><section class="section" id="scaling-down-quay-deployment"><div class="titlepage"><div><div><h3 class="title">16.1.2. Scaling down your Red Hat Quay deployment</h3></div></div></div><p>
					Use the following procedure to scale down your Red Hat Quay deployment.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This step is needed to create a consistent backup of the state of your Red Hat Quay deployment. Do not omit this step, including in setups where PostgreSQL databases and/or S3-compatible object storage are provided by external services (unmanaged by the Red Hat Quay Operator).
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale down your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale down the Red Hat Quay deployment by disabling auto scaling and overriding the replica count for Red Hat Quay, mirror workers, and Clair (if managed). Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <span id="CO6-1"/><span class="callout">1</span>
    - kind: quay
      managed: true
      overrides: <span id="CO6-2"/><span class="callout">2</span>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Disable auto scaling of Quay, Clair and Mirroring workers
										</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Set the replica count to 0 for components accessing the database and objectstorage
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier</strong></span>: Scale down the Red Hat Quay deployment by scaling down the Red Hat Quay Operator first and then the managed Red Hat Quay resources:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt;|awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-app/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-mirror/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/clair-app/ {print $1}') -n &lt;quay-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Wait for the <code class="literal">registry-quay-app</code>, <code class="literal">registry-quay-mirror</code> and <code class="literal">registry-clair-app</code> pods (depending on which components you set to be managed by the Red Hat Quay Operator) to disappear. You can check their status by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;quay_namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">quay-operator.v3.7.1-6f9d859bd-p5ftc               1/1     Running     0             12m
quayregistry-clair-postgres-7487f5bd86-xnxpr       1/1     Running     1 (12m ago)   12m
quayregistry-quay-app-upgrade-xq2v6                0/1     Completed   0             12m
quayregistry-quay-config-editor-6dfdcfc44f-hlvwm   1/1     Running     0             73s
quayregistry-quay-database-859d5445ff-cqthr        1/1     Running     0             12m
quayregistry-quay-redis-84f888776f-hhgms           1/1     Running     0             12m</pre>
							</p></div></li></ol></div></section><section class="section" id="backing-up-managed-database"><div class="titlepage"><div><div><h3 class="title">16.1.3. Backing up the Red Hat Quay managed database</h3></div></div></div><p>
					Use the following procedure to back up the Red Hat Quay managed database.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your Red Hat Quay deployment is configured with external, or unmanged, PostgreSQL database(s), refer to your vendor’s documentation on how to create a consistent backup of these databases.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Identify the Quay PostgreSQL pod name:
						</p><pre class="programlisting language-terminal">$ oc get pod -l quay-component=postgres -n &lt;quay_namespace&gt; -o jsonpath='{.items[0].metadata.name}'</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">quayregistry-quay-database-59f54bb7-58xs7</pre></li><li class="listitem"><p class="simpara">
							Obtain the Quay database name:
						</p><pre class="programlisting language-terminal">$ oc -n &lt;quay_namespace&gt; rsh $(oc get pod -l app=quay -o NAME -n &lt;quay_namespace&gt; |head -n 1) cat /conf/stack/config.yaml|awk -F"/" '/^DB_URI/ {print $4}'
quayregistry-quay-database</pre></li><li class="listitem"><p class="simpara">
							Download a backup database:
						</p><pre class="programlisting language-terminal">$ oc exec quayregistry-quay-database-59f54bb7-58xs7 -- /usr/bin/pg_dump -C quayregistry-quay-database  &gt; backup.sql</pre></li></ol></div><section class="section" id="backing-up-managed-object-storage"><div class="titlepage"><div><div><h4 class="title">16.1.3.1. Backing up the Red Hat Quay managed object storage</h4></div></div></div><p>
						Use the following procedure to back up the Red Hat Quay managed object storage. The instructions in this section apply to the following configurations:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Standalone, multi-cloud object gateway configurations
							</li><li class="listitem">
								OpenShift Data Foundations storage requires that the Red Hat Quay Operator provisioned an S3 object storage bucket from, through the ObjectStorageBucketClaim API
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If your Red Hat Quay deployment is configured with external (unmanged) object storage, refer to your vendor’s documentation on how to create a copy of the content of Quay’s storage bucket.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Decode and export the <code class="literal">AWS_ACCESS_KEY_ID</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt;  -o jsonpath='{.items[0].data.AWS_ACCESS_KEY_ID}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
								Decode and export the <code class="literal">AWS_SECRET_ACCESS_KEY_ID</code> by entering the following command:
							</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.AWS_SECRET_ACCESS_KEY}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
								Create a new directory:
							</p><pre class="programlisting language-terminal">$ mkdir blobs</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can also use <a class="link" href="https://rclone.org/">rclone</a> or <a class="link" href="https://s3tools.org/s3cmd">sc3md</a> instead of the AWS command line utility.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Copy all blobs to the directory by entering the following command:
							</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint https://$(oc get route s3 -n openshift-storage  -o jsonpath='{.spec.host}')  s3://$(oc get cm -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.BUCKET_NAME}') ./blobs</pre></li></ol></div></section></section><section class="section" id="scaling-up-quay-deployment"><div class="titlepage"><div><div><h3 class="title">16.1.4. Scale the Red Hat Quay deployment back up</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale up your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale up the Red Hat Quay deployment by re-enabling auto scaling, if desired, and removing the replica overrides for Quay, mirror workers and Clair as applicable. Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: true <span id="CO7-1"/><span class="callout">1</span>
    - kind: quay <span id="CO7-2"/><span class="callout">2</span>
      managed: true
    - kind: clair
      managed: true
    - kind: mirror
      managed: true
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Re-enables auto scaling of Quay, Clair and Mirroring workers again (if desired)
										</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replica overrides are removed again to scale the Quay components back up
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale up the Red Hat Quay deployment by scaling up the Red Hat Quay Operator again:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=1 deployment $(oc get deployment -n &lt;quay_operator_namespace&gt; | awk '/^quay-operator/ {print $1}') -n &lt;quay_operator_namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay_namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: registry
  namespace: &lt;quay-namespace&gt;
  ...
spec:
  ...
status:
  - lastTransitionTime: '2022-06-20T05:31:17Z'
    lastUpdateTime: '2022-06-20T17:31:13Z'
    message: All components reporting as healthy
    reason: HealthChecksPassing
    status: 'True'
    type: Available</pre></li></ol></div></section></section><section class="section" id="restoring-up-red-hat-quay"><div class="titlepage"><div><div><h2 class="title">16.2. Restoring Red Hat Quay</h2></div></div></div><p>
				Use the following procedures to restore Red Hat Quay when the Red Hat Quay Operator manages the database. It should be performed after a backup of your Red Hat Quay registry has been performed. See <a class="link" href="#backing-up-red-hat-quay-operator" title="16.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> for more information.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Red Hat Quay is deployed on OpenShift Container Platform using the Red Hat Quay Operator.
					</li><li class="listitem">
						A backup of the Red Hat Quay configuration managed by the Red Hat Quay Operator has been created following the instructions in the <a class="link" href="#backing-up-red-hat-quay-operator" title="16.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> section
					</li><li class="listitem">
						Your Red Hat Quay database has been backed up.
					</li><li class="listitem">
						The object storage bucket used by Red Hat Quay has been backed up.
					</li><li class="listitem">
						The components <code class="literal">quay</code>, <code class="literal">postgres</code> and <code class="literal">objectstorage</code> are set to <code class="literal">managed: true</code>
					</li><li class="listitem">
						If the component <code class="literal">clair</code> is set to <code class="literal">managed: true</code>, the component <code class="literal">clairpostgres</code> is also set to <code class="literal">managed: true</code> (starting with Red Hat Quay Operator v3.7 or later)
					</li><li class="listitem">
						There is no running Red Hat Quay deployment managed by the Red Hat Quay Operator in the target namespace on your OpenShift Container Platform cluster
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If your deployment contains partially unmanaged database or storage components and you are using external services for PostgreSQL or S3-compatible object storage to run your Red Hat Quay deployment, you must refer to the service provider or vendor documentation to restore their data from a backup prior to restore Red Hat Quay
				</p></div></div><section class="section" id="restoring-quay-and-configuration-from-backup"><div class="titlepage"><div><div><h3 class="title">16.2.1. Restoring Red Hat Quay and its configuration from a backup</h3></div></div></div><p>
					Use the following procedure to restore Red Hat Quay and its configuration files from a backup.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						These instructions assume you have followed the process in the <a class="link" href="#backing-up-red-hat-quay-operator" title="16.1. Backing up Red Hat Quay">Backing up Red Hat Quay</a> guide and create the backup files with the same names.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Restore the backed up Red Hat Quay configuration by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f ./config-bundle.yaml</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you receive the error <code class="literal">Error from server (AlreadyExists): error when creating "./config-bundle.yaml": secrets "config-bundle-secret" already exists</code>, you must delete your existing resource with <code class="literal">$ oc delete Secret config-bundle-secret -n &lt;quay-namespace&gt;</code> and recreate it with <code class="literal">$ oc create -f ./config-bundle.yaml</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Restore the generated keys from the backup by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f ./managed-secret-keys.yaml</pre></li><li class="listitem"><p class="simpara">
							Restore the <code class="literal">QuayRegistry</code> custom resource:
						</p><pre class="programlisting language-terminal">$ oc create -f ./quay-registry.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment and wait for it to be available:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay-namespace&gt;</pre></li></ol></div></section><section class="section" id="scale-down-quay-deployment"><div class="titlepage"><div><div><h3 class="title">16.2.2. Scaling down your Red Hat Quay deployment</h3></div></div></div><p>
					Use the following procedure to scale down your Red Hat Quay deployment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale down your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale down the Red Hat Quay deployment by disabling auto scaling and overriding the replica count for Quay, mirror workers and Clair (if managed). Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <span id="CO8-1"/><span class="callout">1</span>
    - kind: quay
      managed: true
      overrides: <span id="CO8-2"/><span class="callout">2</span>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Disable auto scaling of Quay, Clair and Mirroring workers
										</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Set the replica count to 0 for components accessing the database and objectstorage
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale down the Red Hat Quay deployment by scaling down the Red Hat Quay Operator first and then the managed Red Hat Quay resources:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt;|awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-app/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/quay-mirror/ {print $1}') -n &lt;quay-namespace&gt;</pre><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment $(oc get deployment -n &lt;quay-namespace&gt;|awk '/clair-app/ {print $1}') -n &lt;quay-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Wait for the <code class="literal">registry-quay-app</code>, <code class="literal">registry-quay-mirror</code> and <code class="literal">registry-clair-app</code> pods (depending on which components you set to be managed by Red Hat Quay Operator) to disappear. You can check their status by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-terminal">registry-quay-config-editor-77847fc4f5-nsbbv   1/1     Running            0          9m1s
registry-quay-database-66969cd859-n2ssm        1/1     Running            0          6d1h
registry-quay-redis-7cc5f6c977-956g8           1/1     Running            0          5d21h</pre></li></ol></div></section><section class="section" id="restoring-quay-database"><div class="titlepage"><div><div><h3 class="title">16.2.3. Restoring your Red Hat Quay database</h3></div></div></div><p>
					Use the following procedure to restore your Red Hat Quay database.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Identify your <code class="literal">Quay</code> database pod by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod -l quay-component=postgres -n  &lt;quay-namespace&gt; -o jsonpath='{.items[0].metadata.name}'</pre><p class="simpara">
							Example output:
						</p><pre class="screen">quayregistry-quay-database-59f54bb7-58xs7</pre></li><li class="listitem"><p class="simpara">
							Upload the backup by copying it from the local environment and into the pod:
						</p><pre class="screen">$ oc cp ./backup.sql -n &lt;quay-namespace&gt; registry-quay-database-66969cd859-n2ssm:/tmp/backup.sql</pre></li><li class="listitem"><p class="simpara">
							Open a remote terminal to the database by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc rsh -n &lt;quay-namespace&gt; registry-quay-database-66969cd859-n2ssm</pre></li><li class="listitem"><p class="simpara">
							Enter psql by running the following command:
						</p><pre class="programlisting language-terminal">bash-4.4$ psql</pre></li><li class="listitem"><p class="simpara">
							You can list the database by running the following command:
						</p><pre class="screen">postgres=# \l</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">                                                  List of databases
           Name            |           Owner            | Encoding |  Collate   |   Ctype    |   Access privileges
----------------------------+----------------------------+----------+------------+------------+-----------------------
postgres                   | postgres                   | UTF8     | en_US.utf8 | en_US.utf8 |
quayregistry-quay-database | quayregistry-quay-database | UTF8     | en_US.utf8 | en_US.utf8 |</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Drop the database by entering the following command:
						</p><pre class="programlisting language-terminal">postgres=# DROP DATABASE "quayregistry-quay-database";</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">DROP DATABASE</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Exit the postgres CLI to re-enter bash-4.4:
						</p><pre class="programlisting language-terminal">\q</pre></li><li class="listitem"><p class="simpara">
							Redirect your PostgreSQL database to your backup database:
						</p><pre class="programlisting language-terminal">sh-4.4$ psql &lt; /tmp/backup.sql</pre></li><li class="listitem"><p class="simpara">
							Exit bash by entering the following command:
						</p><pre class="programlisting language-terminal">sh-4.4$ exit</pre></li></ol></div></section><section class="section" id="restoring-quay-object-storage-data"><div class="titlepage"><div><div><h3 class="title">16.2.4. Restore your Red Hat Quay object storage data</h3></div></div></div><p>
					Use the following procedure to restore your Red Hat Quay object storage data.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Export the <code class="literal">AWS_ACCESS_KEY_ID</code> by entering the following command:
						</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt;  -o jsonpath='{.items[0].data.AWS_ACCESS_KEY_ID}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Export the <code class="literal">AWS_SECRET_ACCESS_KEY</code> by entering the following command:
						</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=$(oc get secret -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.AWS_SECRET_ACCESS_KEY}' |base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Upload all blobs to the bucket by running the following command:
						</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint https://$(oc get route s3 -n openshift-storage  -o jsonpath='{.spec.host}') ./blobs  s3://$(oc get cm -l app=noobaa -n &lt;quay-namespace&gt; -o jsonpath='{.items[0].data.BUCKET_NAME}')</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use <a class="link" href="https://rclone.org/">rclone</a> or <a class="link" href="https://s3tools.org/s3cmd">sc3md</a> instead of the AWS command line utility.
					</p></div></div></section><section class="section" id="scaling-up-quay"><div class="titlepage"><div><div><h3 class="title">16.2.5. Scaling up your Red Hat Quay deployment</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Depending on the version of your Red Hat Quay deployment, scale up your deployment using one of the following options.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.7 and newer:</strong></span> Scale up the Red Hat Quay deployment by re-enabling auto scaling, if desired, and removing the replica overrides for Quay, mirror workers and Clair as applicable. Your <code class="literal">QuayRegistry</code> resource should look similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: true <span id="CO9-1"/><span class="callout">1</span>
    - kind: quay <span id="CO9-2"/><span class="callout">2</span>
      managed: true
    - kind: clair
      managed: true
    - kind: mirror
      managed: true
    …</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Re-enables auto scaling of Red Hat Quay, Clair and mirroring workers again (if desired)
										</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replica overrides are removed again to scale the Red Hat Quay components back up
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>For Operator version 3.6 and earlier:</strong></span> Scale up the Red Hat Quay deployment by scaling up the Red Hat Quay Operator again:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=1 deployment $(oc get deployment -n &lt;quay-operator-namespace&gt; | awk '/^quay-operator/ {print $1}') -n &lt;quay-operator-namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check the status of the Red Hat Quay deployment:
						</p><pre class="programlisting language-terminal">$ oc wait quayregistry registry --for=condition=Available=true -n &lt;quay-namespace&gt;</pre><p class="simpara">
							Example output:
						</p><pre class="programlisting language-yaml">apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: registry
  namespace: &lt;quay-namespace&gt;
  ...
spec:
  ...
status:
  - lastTransitionTime: '2022-06-20T05:31:17Z'
    lastUpdateTime: '2022-06-20T17:31:13Z'
    message: All components reporting as healthy
    reason: HealthChecksPassing
    status: 'True'
    type: Available</pre></li></ol></div></section></section></section><section class="chapter" id="migrating_a_standalone_quay_deployment_to_a_red_hat_quay_operator_managed_deployment"><div class="titlepage"><div><div><h1 class="title">Chapter 17. Migrating a standalone Quay deployment to a Red Hat Quay Operator managed deployment</h1></div></div></div><p>
			The following procedures allow you to back up a standalone Red Hat Quay deployment and migrate it to the Red Hat Quay Operator on OpenShift Container Platform.
		</p><section class="section" id="backing_up_a_standalone_deployment_of_red_hat_quay"><div class="titlepage"><div><div><h2 class="title">17.1. Backing up a standalone deployment of Red Hat Quay</h2></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Back up the Quay <code class="literal">config.yaml</code> of your standalone deployment:
					</p><pre class="programlisting language-terminal">$ mkdir /tmp/quay-backup
$ cp /path/to/Quay/config/directory/config.yaml /tmp/quay-backup</pre></li><li class="listitem"><p class="simpara">
						Create a backup of the database that your standalone Quay deployment is using:
					</p><pre class="programlisting language-terminal">$ pg_dump -h DB_HOST -p 5432 -d QUAY_DATABASE_NAME -U QUAY_DATABASE_USER -W -O &gt; /tmp/quay-backup/quay-database-backup.sql</pre></li><li class="listitem">
						Install the <a class="link" href="https://docs.aws.amazon.com/cli/v1/userguide/install-linux.html#install-linux-bundled-sudo">AWS CLI</a> if you do not have it already.
					</li><li class="listitem"><p class="simpara">
						Create an <code class="literal">~/.aws/</code> directory:
					</p><pre class="programlisting language-terminal">$ mkdir ~/.aws/</pre></li><li class="listitem"><p class="simpara">
						Obtain the <code class="literal">access_key</code> and <code class="literal">secret_key</code> from the Quay <code class="literal">config.yaml</code> of your standalone deployment:
					</p><pre class="programlisting language-terminal">$ grep -i DISTRIBUTED_STORAGE_CONFIG -A10 /tmp/quay-backup/config.yaml</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
    minio-1:
        - RadosGWStorage
        - access_key: ##########
          bucket_name: quay
          hostname: 172.24.10.50
          is_secure: false
          port: "9000"
          secret_key: ##########
          storage_path: /datastorage/registry</pre></li><li class="listitem"><p class="simpara">
						Store the <code class="literal">access_key</code> and <code class="literal">secret_key</code> from the Quay <code class="literal">config.yaml</code> file in your <code class="literal">~/.aws</code> directory:
					</p><pre class="programlisting language-terminal">$ touch ~/.aws/credentials</pre></li><li class="listitem"><p class="simpara">
						Optional: Check that your <code class="literal">access_key</code> and <code class="literal">secret_key</code> are stored:
					</p><pre class="programlisting language-terminal">$ cat &gt; ~/.aws/credentials &lt;&lt; EOF
[default]
aws_access_key_id = ACCESS_KEY_FROM_QUAY_CONFIG
aws_secret_access_key = SECRET_KEY_FROM_QUAY_CONFIG
EOF</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">aws_access_key_id = ACCESS_KEY_FROM_QUAY_CONFIG
aws_secret_access_key = SECRET_KEY_FROM_QUAY_CONFIG</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the <code class="literal">aws cli</code> does not automatically collect the <code class="literal">access_key</code> and <code class="literal">secret_key</code> from the <code class="literal">`~/.aws/credentials file</code>, you can, you can configure these by running <code class="literal">aws configure</code> and manually inputting the credentials.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In your <code class="literal">quay-backup</code> directory, create a <code class="literal">bucket_backup</code> directory:
					</p><pre class="programlisting language-terminal">$ mkdir /tmp/quay-backup/bucket-backup</pre></li><li class="listitem"><p class="simpara">
						Backup all blobs from the S3 storage:
					</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint-url https://PUBLIC_S3_ENDPOINT:PORT s3://QUAY_BUCKET/ /tmp/quay-backup/bucket-backup/</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">PUBLIC_S3_ENDPOINT</code> can be read from the Quay <code class="literal">config.yaml</code> file under <code class="literal">hostname</code> in the <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code>. If the endpoint is insecure, use <code class="literal">http</code> instead of <code class="literal">https</code> in the endpoint URL.
						</p></div></div></li></ol></div><p>
				Up to this point, you should have a complete backup of all Quay data, blobs, the database, and the <code class="literal">config.yaml</code> file stored locally. In the following section, you will migrate the standalone deployment backup to Red Hat Quay on OpenShift Container Platform.
			</p></section><section class="section" id="using_backed_up_standalone_content_to_migrate_to_openshift_container_platform"><div class="titlepage"><div><div><h2 class="title">17.2. Using backed up standalone content to migrate to OpenShift Container Platform.</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Your standalone Red Hat Quay data, blobs, database, and <code class="literal">config.yaml</code> have been backed up.
					</li><li class="listitem">
						Red Hat Quay is deployed on OpenShift Container Platform using the Quay Operator.
					</li><li class="listitem">
						A <code class="literal">QuayRegistry</code> with all components set to <code class="literal">managed</code>.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Procedure</div><div><p>
					The procedure in this documents uses the following namespace: <code class="literal">quay-enterprise</code>.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Scale down the Red Hat Quay Operator:
					</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment quay-operator.v3.6.2 -n openshift-operators</pre></li><li class="listitem"><p class="simpara">
						Scale down the application and mirror deployments:
					</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 deployment QUAY_MAIN_APP_DEPLOYMENT QUAY_MIRROR_DEPLOYMENT</pre></li><li class="listitem"><p class="simpara">
						Copy the database SQL backup to the Quay PostgreSQL database instance:
					</p><pre class="programlisting language-terminal">$ oc cp /tmp/user/quay-backup/quay-database-backup.sql quay-enterprise/quayregistry-quay-database-54956cdd54-p7b2w:/var/lib/pgsql/data/userdata</pre></li><li class="listitem"><p class="simpara">
						Obtain the database password from the Operator-created <code class="literal">config.yaml</code> file:
					</p><pre class="programlisting language-terminal">$ oc get deployment quay-quay-app -o json | jq '.spec.template.spec.volumes[].projected.sources' | grep -i config-secret</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-yaml">      "name": "QUAY_CONFIG_SECRET_NAME"</pre><pre class="programlisting language-terminal">$ oc get secret quay-quay-config-secret-9t77hb84tb -o json | jq '.data."config.yaml"' | cut -d '"' -f2 | base64 -d -w0 &gt; /tmp/quay-backup/operator-quay-config-yaml-backup.yaml</pre><pre class="programlisting language-terminal">cat /tmp/quay-backup/operator-quay-config-yaml-backup.yaml | grep -i DB_URI</pre><p class="simpara">
						Example output:
					</p><pre class="screen">postgresql://QUAY_DATABASE_OWNER:PASSWORD@DATABASE_HOST/QUAY_DATABASE_NAME</pre></li><li class="listitem"><p class="simpara">
						Execute a shell inside of the database pod:
					</p><pre class="programlisting language-terminal"># oc exec -it quay-postgresql-database-pod -- /bin/bash</pre></li><li class="listitem"><p class="simpara">
						Enter psql:
					</p><pre class="programlisting language-terminal">bash-4.4$ psql</pre></li><li class="listitem"><p class="simpara">
						Drop the database:
					</p><pre class="programlisting language-terminal">postgres=# DROP DATABASE "example-restore-registry-quay-database";</pre><p class="simpara">
						Example output:
					</p><pre class="screen">DROP DATABASE</pre></li><li class="listitem"><p class="simpara">
						Create a new database and set the owner as the same name:
					</p><pre class="programlisting language-terminal">postgres=# CREATE DATABASE "example-restore-registry-quay-database" OWNER "example-restore-registry-quay-database";</pre><p class="simpara">
						Example output:
					</p><pre class="screen">CREATE DATABASE</pre></li><li class="listitem"><p class="simpara">
						Connect to the database:
					</p><pre class="programlisting language-terminal">postgres=# \c "example-restore-registry-quay-database";</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">You are now connected to database "example-restore-registry-quay-database" as user "postgres".</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">pg_trmg</code> extension of your Quay database:
					</p><pre class="programlisting language-terminal">example-restore-registry-quay-database=# create extension pg_trgm ;</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">CREATE EXTENSION</pre></li><li class="listitem"><p class="simpara">
						Exit the postgres CLI to re-enter bash-4.4:
					</p><pre class="programlisting language-terminal">\q</pre></li><li class="listitem"><p class="simpara">
						Set the password for your PostgreSQL deployment:
					</p><pre class="programlisting language-terminal">bash-4.4$ psql -h localhost -d "QUAY_DATABASE_NAME" -U QUAY_DATABASE_OWNER -W &lt; /var/lib/pgsql/data/userdata/quay-database-backup.sql</pre><p class="simpara">
						Example output:
					</p><pre class="screen">SET
SET
SET
SET
SET</pre></li><li class="listitem"><p class="simpara">
						Exit bash mode:
					</p><pre class="programlisting language-terminal">bash-4.4$ exit</pre></li><li class="listitem"><p class="simpara">
						Create a new configuration bundle for the Red Hat Quay Operator.
					</p><pre class="programlisting language-terminal">$ touch config-bundle.yaml</pre></li><li class="listitem"><p class="simpara">
						In your new <code class="literal">config-bundle.yaml</code>, include all of the information that the registry requires, such as LDAP configuration, keys, and other modifications that your old registry had. Run the following command to move the <code class="literal">secret_key</code> to your <code class="literal">config-bundle.yaml</code>:
					</p><pre class="programlisting language-terminal">$ cat /tmp/quay-backup/config.yaml | grep SECRET_KEY &gt; /tmp/quay-backup/config-bundle.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must manually copy all the LDAP, OIDC and other information and add it to the /tmp/quay-backup/config-bundle.yaml file.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Create a configuration bundle secret inside of your OpenShift cluster:
					</p><pre class="programlisting language-terminal">$ oc create secret generic new-custom-config-bundle --from-file=config.yaml=/tmp/quay-backup/config-bundle.yaml</pre></li><li class="listitem"><p class="simpara">
						Scale up the Quay pods:
					</p><pre class="screen">$ oc scale --replicas=1 deployment quayregistry-quay-app
deployment.apps/quayregistry-quay-app scaled</pre></li><li class="listitem"><p class="simpara">
						Scale up the mirror pods:
					</p><pre class="screen">$ oc scale --replicas=1  deployment quayregistry-quay-mirror
deployment.apps/quayregistry-quay-mirror scaled</pre></li><li class="listitem"><p class="simpara">
						Patch the <code class="literal">QuayRegistry</code> CRD so that it contains the reference to the new custom configuration bundle:
					</p><pre class="screen">$ oc patch quayregistry QUAY_REGISTRY_NAME --type=merge -p '{"spec":{"configBundleSecret":"new-custom-config-bundle"}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If Quay returns a <code class="literal">500</code> internal server error, you might have to update the <code class="literal">location</code> of your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> to <code class="literal">default</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Create a new AWS <code class="literal">credentials.yaml</code> in your <code class="literal">/.aws/</code> directory and include the <code class="literal">access_key</code> and <code class="literal">secret_key</code> from the Operator-created <code class="literal">config.yaml</code> file:
					</p><pre class="programlisting language-terminal">$ touch credentials.yaml</pre><pre class="programlisting language-terminal">$ grep -i DISTRIBUTED_STORAGE_CONFIG -A10 /tmp/quay-backup/operator-quay-config-yaml-backup.yaml</pre><pre class="programlisting language-terminal">$ cat &gt; ~/.aws/credentials &lt;&lt; EOF
[default]
aws_access_key_id = ACCESS_KEY_FROM_QUAY_CONFIG
aws_secret_access_key = SECRET_KEY_FROM_QUAY_CONFIG
EOF</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the <code class="literal">aws cli</code> does not automatically collect the <code class="literal">access_key</code> and <code class="literal">secret_key</code> from the <code class="literal">`~/.aws/credentials file</code>, you can configure these by running <code class="literal">aws configure</code> and manually inputting the credentials.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Record the NooBaa’s publicly available endpoint:
					</p><pre class="programlisting language-terminal">$ oc get route s3 -n openshift-storage -o yaml -o jsonpath="{.spec.host}{'\n'}"</pre></li><li class="listitem"><p class="simpara">
						Sync the backup data to the NooBaa backend storage:
					</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl --endpoint-url https://NOOBAA_PUBLIC_S3_ROUTE /tmp/quay-backup/bucket-backup/* s3://QUAY_DATASTORE_BUCKET_NAME</pre></li><li class="listitem"><p class="simpara">
						Scale the Operator back up to 1 pod:
					</p><pre class="programlisting language-terminal">$ oc scale –replicas=1 deployment quay-operator.v3.6.4 -n openshift-operators</pre></li></ol></div><p>
				The Operator will use the custom configuration bundle provided and will reconcile all secrets and deployments. Your new Quay deployment on OpenShift Container Platform should contain all of the information that the old deployment had. All images should be pull-able.
			</p></section></section><section class="chapter" id="standalone-deployment-backup-restore"><div class="titlepage"><div><div><h1 class="title">Chapter 18. Backing up and restoring Red Hat Quay on a standalone deployment</h1></div></div></div><p>
			Use the content within this section to back up and restore Red Hat Quay in standalone deployments.
		</p><section class="section" id="backing-up-red-hat-quay-standalone"><div class="titlepage"><div><div><h2 class="title">18.1. Backing up Red Hat Quay on standalone deployments</h2></div></div></div><p>
				This procedure describes how to create a backup of Red Hat Quay on standalone deployments.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a temporary backup directory, for example, <code class="literal">quay-backup</code>:
					</p><pre class="programlisting language-terminal">$ mkdir /tmp/quay-backup</pre></li><li class="listitem"><p class="simpara">
						The following example command denotes the local directory that the Red Hat Quay was started in, for example, <code class="literal">/opt/quay-install</code>:
					</p><pre class="screen">$ podman run --name quay-app \
   -v /opt/quay-install/config:/conf/stack:Z \
   -v /opt/quay-install/storage:/datastorage:Z \
   registry.redhat.io/quay/quay-rhel8:v3.9.0</pre><p class="simpara">
						Change into the directory that bind-mounts to <code class="literal">/conf/stack</code> inside of the container, for example, <code class="literal">/opt/quay-install</code>, by running the following command:
					</p><pre class="programlisting language-terminal">$ cd /opt/quay-install</pre></li><li class="listitem"><p class="simpara">
						Compress the contents of your Red Hat Quay deployment into an archive in the <code class="literal">quay-backup</code> directory by entering the following command:
					</p><pre class="programlisting language-terminal">$ tar cvf /tmp/quay-backup/quay-backup.tar.gz *</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">config.yaml
config.yaml.bak
extra_ca_certs/
extra_ca_certs/ca.crt
ssl.cert
ssl.key</pre></li><li class="listitem"><p class="simpara">
						Back up the Quay container service by entering the following command:
					</p><pre class="screen">$ podman inspect quay-app | jq -r '.[0].Config.CreateCommand | .[]' | paste -s -d ' ' -

  /usr/bin/podman run --name quay-app \
  -v /opt/quay-install/config:/conf/stack:Z \
  -v /opt/quay-install/storage:/datastorage:Z \
  registry.redhat.io/quay/quay-rhel8:v3.9.0</pre></li><li class="listitem"><p class="simpara">
						Redirect the contents of your <code class="literal">conf/stack/config.yaml</code> file to your temporary <code class="literal">quay-config.yaml</code> file by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman exec -it quay cat /conf/stack/config.yaml &gt; /tmp/quay-backup/quay-config.yaml</pre></li><li class="listitem"><p class="simpara">
						Obtain the <code class="literal">DB_URI</code> located in your temporary <code class="literal">quay-config.yaml</code> by entering the following command:
					</p><pre class="programlisting language-terminal">$ grep DB_URI /tmp/quay-backup/quay-config.yaml</pre><p class="simpara">
						Example output:
					</p><pre class="screen">$ postgresql://&lt;username&gt;:test123@172.24.10.50/quay</pre></li><li class="listitem"><p class="simpara">
						Extract the PostgreSQL contents to your temporary backup directory in a backup .sql file by entering the following command:
					</p><pre class="programlisting language-terminal">$ pg_dump -h 172.24.10.50  -p 5432 -d quay  -U  &lt;username&gt;   -W -O &gt; /tmp/quay-backup/quay-backup.sql</pre></li><li class="listitem"><p class="simpara">
						Print the contents of your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> by entering the following command:
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
   default:
    - S3Storage
    - s3_bucket: &lt;bucket_name&gt;
      storage_path: /registry
      s3_access_key: &lt;s3_access_key&gt;
      s3_secret_key: &lt;s3_secret_key&gt;
      host: &lt;host_name&gt;</pre></li><li class="listitem"><p class="simpara">
						Export the <code class="literal">AWS_ACCESS_KEY_ID</code> by using the <code class="literal">access_key</code> credential obtained in Step 7:
					</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=&lt;access_key&gt;</pre></li><li class="listitem"><p class="simpara">
						Export the <code class="literal">AWS_SECRET_ACCESS_KEY</code> by using the <code class="literal">secret_key</code> obtained in Step 7:
					</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=&lt;secret_key&gt;</pre></li><li class="listitem"><p class="simpara">
						Sync the <code class="literal">quay</code> bucket to the <code class="literal">/tmp/quay-backup/blob-backup/</code> directory from the <code class="literal">hostname</code> of your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code>:
					</p><pre class="programlisting language-terminal">$ aws s3 sync s3://&lt;bucket_name&gt;  /tmp/quay-backup/blob-backup/ --source-region us-east-2</pre><p class="simpara">
						Example output:
					</p><pre class="screen">download: s3://&lt;user_name&gt;/registry/sha256/9c/9c3181779a868e09698b567a3c42f3744584ddb1398efe2c4ba569a99b823f7a to registry/sha256/9c/9c3181779a868e09698b567a3c42f3744584ddb1398efe2c4ba569a99b823f7a
download: s3://&lt;user_name&gt;/registry/sha256/e9/e9c5463f15f0fd62df3898b36ace8d15386a6813ffb470f332698ecb34af5b0d to registry/sha256/e9/e9c5463f15f0fd62df3898b36ace8d15386a6813ffb470f332698ecb34af5b0d</pre></li></ol></div><div class="informalexample"><p>
				It is recommended that you delete the <code class="literal">quay-config.yaml</code> file after syncing the <code class="literal">quay</code> bucket because it contains sensitive information. The <code class="literal">quay-config.yaml</code> file will not be lost because it is backed up in the <code class="literal">quay-backup.tar.gz</code> file.
			</p></div></section><section class="section" id="restoring-red-hat-quay-standalone"><div class="titlepage"><div><div><h2 class="title">18.2. Restoring Red Hat Quay on standalone deployments</h2></div></div></div><p>
				This procedure describes how to restore Red Hat Quay on standalone deployments.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have backed up your Red Hat Quay deployment.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a new directory that will bind-mount to <code class="literal">/conf/stack</code> inside of the Red Hat Quay container:
					</p><pre class="programlisting language-terminal">$ mkdir /opt/new-quay-install</pre></li><li class="listitem"><p class="simpara">
						Copy the contents of your temporary backup directory created in <a class="link" href="#backing-up-red-hat-quay-standalone" title="18.1. Backing up Red Hat Quay on standalone deployments">Backing up Red Hat Quay on standalone deployments</a> to the <code class="literal">new-quay-install1</code> directory created in Step 1:
					</p><pre class="programlisting language-terminal">$ cp /tmp/quay-backup/quay-backup.tar.gz /opt/new-quay-install/</pre></li><li class="listitem"><p class="simpara">
						Change into the <code class="literal">new-quay-install</code> directory by entering the following command:
					</p><pre class="programlisting language-terminal">$ cd /opt/new-quay-install/</pre></li><li class="listitem"><p class="simpara">
						Extract the contents of your Red Hat Quay directory:
					</p><pre class="programlisting language-terminal">$ tar xvf /tmp/quay-backup/quay-backup.tar.gz *</pre><p class="simpara">
						Example output:
					</p><pre class="screen">config.yaml
config.yaml.bak
extra_ca_certs/
extra_ca_certs/ca.crt
ssl.cert
ssl.key</pre></li><li class="listitem"><p class="simpara">
						Recall the <code class="literal">DB_URI</code> from your backed-up <code class="literal">config.yaml</code> file by entering the following command:
					</p><pre class="programlisting language-terminal">$ grep DB_URI config.yaml</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-yaml">postgresql://&lt;username&gt;:test123@172.24.10.50/quay</pre></li><li class="listitem"><p class="simpara">
						Run the following command to enter the PostgreSQL database server:
					</p><pre class="programlisting language-terminal">$ sudo postgres</pre></li><li class="listitem"><p class="simpara">
						Enter psql and create a new database in 172.24.10.50 to restore the quay databases, for example, <code class="literal">example_restore_registry_quay_database</code>, by entering the following command:
					</p><pre class="programlisting language-terminal">$ psql "host=172.24.10.50  port=5432 dbname=postgres user=&lt;username&gt;  password=test123"
postgres=&gt; CREATE DATABASE example_restore_registry_quay_database;</pre><p class="simpara">
						Example output:
					</p><pre class="screen">CREATE DATABASE</pre></li><li class="listitem"><p class="simpara">
						Connect to the database by running the following command:
					</p><pre class="programlisting language-terminal">postgres=# \c "example-restore-registry-quay-database";</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">You are now connected to database "example-restore-registry-quay-database" as user "postgres".</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">pg_trmg</code> extension of your Quay database by running the following command:
					</p><pre class="programlisting language-terminal">example_restore_registry_quay_database=&gt; CREATE EXTENSION IF NOT EXISTS pg_trgm;</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">CREATE EXTENSION</pre></li><li class="listitem"><p class="simpara">
						Exit the postgres CLI by entering the following command:
					</p><pre class="programlisting language-terminal">\q</pre></li><li class="listitem"><p class="simpara">
						Import the database backup to your new database by running the following command:
					</p><pre class="programlisting language-terminal">$ psql "host=172.24.10.50 port=5432 dbname=example_restore_registry_quay_database user=&lt;username&gt; password=test123"  -W &lt;  /tmp/quay-backup/quay-backup.sql</pre><p class="simpara">
						Example output:
					</p><pre class="screen">SET
SET
SET
SET
SET</pre><p class="simpara">
						Update the value of <code class="literal">DB_URI</code> in your <code class="literal">config.yaml</code> from <code class="literal">postgresql://&lt;username&gt;:test123@172.24.10.50/quay</code> to <code class="literal">postgresql://&lt;username&gt;:test123@172.24.10.50/example-restore-registry-quay-database</code> before restarting the Red Hat Quay deployment.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The DB_URI format is <code class="literal">DB_URI postgresql://&lt;login_user_name&gt;:&lt;login_user_password&gt;@&lt;postgresql_host&gt;/&lt;quay_database&gt;</code>. If you are moving from one PostgreSQL server to another PostgreSQL server, update the value of <code class="literal">&lt;login_user_name&gt;</code>, <code class="literal">&lt;login_user_password&gt;</code> and <code class="literal">&lt;postgresql_host&gt;</code> at the same time.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In the <code class="literal">/opt/new-quay-install</code> directory, print the contents of your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> bundle:
					</p><pre class="programlisting language-terminal">$ cat config.yaml | grep DISTRIBUTED_STORAGE_CONFIG -A10</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
   default:
DISTRIBUTED_STORAGE_CONFIG:
   default:
    - S3Storage
    - s3_bucket: &lt;bucket_name&gt;
      storage_path: /registry
      s3_access_key: &lt;s3_access_key&gt;
      s3_secret_key: &lt;s3_secret_key&gt;
      host: &lt;host_name&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Your <code class="literal">DISTRIBUTED_STORAGE_CONFIG</code> in <code class="literal">/opt/new-quay-install</code> must be updated before restarting your Red Hat Quay deployment.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Export the <code class="literal">AWS_ACCESS_KEY_ID</code> by using the <code class="literal">access_key</code> credential obtained in Step 13:
					</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=&lt;access_key&gt;</pre></li><li class="listitem"><p class="simpara">
						Export the <code class="literal">AWS_SECRET_ACCESS_KEY</code> by using the <code class="literal">secret_key</code> obtained in Step 13:
					</p><pre class="programlisting language-terminal">$ export AWS_SECRET_ACCESS_KEY=&lt;secret_key&gt;</pre></li><li class="listitem"><p class="simpara">
						Create a new s3 bucket by entering the following command:
					</p><pre class="programlisting language-terminal">$ aws s3 mb s3://&lt;new_bucket_name&gt;  --region us-east-2</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">$ make_bucket: quay</pre></li><li class="listitem"><p class="simpara">
						Upload all blobs to the new s3 bucket by entering the following command:
					</p><pre class="programlisting language-terminal">$ aws s3 sync --no-verify-ssl \
--endpoint-url &lt;example_endpoint_url&gt; <span id="CO10-1"/><span class="callout">1</span>
/tmp/quay-backup/blob-backup/. s3://quay/</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The Red Hat Quay registry endpoint must be the same before backup and after restore.
							</div></dd></dl></div><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">upload: ../../tmp/quay-backup/blob-backup/datastorage/registry/sha256/50/505edb46ea5d32b5cbe275eb766d960842a52ee77ac225e4dc8abb12f409a30d to s3://quay/datastorage/registry/sha256/50/505edb46ea5d32b5cbe275eb766d960842a52ee77ac225e4dc8abb12f409a30d
upload: ../../tmp/quay-backup/blob-backup/datastorage/registry/sha256/27/27930dc06c2ee27ac6f543ba0e93640dd21eea458eac47355e8e5989dea087d0 to s3://quay/datastorage/registry/sha256/27/27930dc06c2ee27ac6f543ba0e93640dd21eea458eac47355e8e5989dea087d0
upload: ../../tmp/quay-backup/blob-backup/datastorage/registry/sha256/8c/8c7daf5e20eee45ffe4b36761c4bb6729fb3ee60d4f588f712989939323110ec to s3://quay/datastorage/registry/sha256/8c/8c7daf5e20eee45ffe4b36761c4bb6729fb3ee60d4f588f712989939323110ec
...</pre></li><li class="listitem"><p class="simpara">
						Before restarting your Red Hat Quay deployment, update the storage settings in your config.yaml:
					</p><pre class="programlisting language-yaml">DISTRIBUTED_STORAGE_CONFIG:
   default:
DISTRIBUTED_STORAGE_CONFIG:
   default:
    - S3Storage
    - s3_bucket: &lt;new_bucket_name&gt;
      storage_path: /registry
      s3_access_key: &lt;s3_access_key&gt;
      s3_secret_key: &lt;s3_secret_key&gt;
      host: &lt;host_name&gt;</pre></li></ol></div></section></section><section class="chapter" id="supported-oci-media-types"><div class="titlepage"><div><div><h1 class="title">Chapter 19. Configuring artifact types</h1></div></div></div><p>
			As a Red Hat Quay administrator, you can configure Open Container Initiative (OCI) artifact types and other experimental artifact types through the <code class="literal">FEATURE_GENERAL_OCI_SUPPORT</code>, <code class="literal">ALLOWED_OCI_ARTIFACT_TYPES</code>, and <code class="literal">IGNORE_UNKNOWN_MEDIATYPES</code> configuration fields.
		</p><p>
			The following Open Container Initiative (OCI) artifact types are built into Red Hat Quay by default and are enabled through the <span class="strong strong"><strong>FEATURE_GENERAL_OCI_SUPPORT</strong></span> configuration field:
		</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 17%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621342816816" scope="col">Field</th><th align="left" valign="top" id="idm45621342815728" scope="col">Media Type</th><th align="left" valign="top" id="idm45621342814640" scope="col">Supported content types</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>Helm</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.cncf.helm.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/tar+gzip</code>, <code class="literal">application/vnd.cncf.helm.chart.content.v1.tar+gzip</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>Cosign</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.oci.image.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/vnd.dev.cosign.simplesigning.v1+json</code>, <code class="literal">application/vnd.dsse.envelope.v1+json</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>SPDX</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.oci.image.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">text/spdx</code>, <code class="literal">text/spdx+xml</code>, <code class="literal">text/spdx+json</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>Syft</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.oci.image.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/vnd.syft+json</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>CycloneDX</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.oci.image.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/vnd.cyclonedx</code>, <code class="literal">application/vnd.cyclonedx+xml</code>, <code class="literal">application/vnd.cyclonedx+json</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>In-toto</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.oci.image.config.v1+json</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/vnd.in-toto+json</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45621342816816">
						<p>
							<span class="strong strong"><strong>Unknown</strong></span>
						</p>
						</td><td align="left" valign="top" headers="idm45621342815728">
						<p>
							<code class="literal">application/vnd.cncf.openpolicyagent.policy.layer.v1+rego</code>
						</p>
						</td><td align="left" valign="top" headers="idm45621342814640">
						<p>
							<code class="literal">application/vnd.cncf.openpolicyagent.policy.layer.v1+rego</code>, <code class="literal">application/vnd.cncf.openpolicyagent.data.layer.v1+json</code>
						</p>
						</td></tr></tbody></table></div><p>
			Additionally, Red Hat Quay uses the <span class="emphasis"><em>ZStandard</em></span>, or <span class="emphasis"><em>zstd</em></span>, to reduce the size of container images or other related artifacts. Zstd helps optimize storage and improve transfer speeds when working with container images.
		</p><p>
			Use the following procedures to configure support for the default and experimental OCI media types.
		</p><section class="section" id="configuring-oci-media-types-proc"><div class="titlepage"><div><div><h2 class="title">19.1. Configuring OCI artifact types</h2></div></div></div><p>
				Use the following procedure to configure artifact types that are embedded in Red Hat Quay by default.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have Red Hat Quay administrator privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						In your Red Hat Quay <code class="literal">config.yaml</code> file, enable support for general OCI support by setting the <code class="literal">FEATURE_GENERAL_OCI_SUPPORT</code> field to <code class="literal">true</code>. For example:
					</p><pre class="programlisting language-yaml">FEATURE_GENERAL_OCI_SUPPORT: true</pre><p class="simpara">
						With <code class="literal">FEATURE_GENERAL_OCI_SUPPORT</code> set to true, Red Hat Quay users can now push and pull charts of the default artifact types to their Red Hat Quay deployment.
					</p></li></ul></div></section><section class="section" id="configuring-additional-oci-media-types-proc"><div class="titlepage"><div><div><h2 class="title">19.2. Configuring additional artifact types</h2></div></div></div><p>
				Use the following procedure to configure additional, and specific, artifact types for your Red Hat Quay deployment.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Using the <code class="literal">ALLOWED_OCI_ARTIFACT_TYPES</code> configuration field, you can restrict which artifact types are accepted by your Red Hat Quay registry. If you want your Red Hat Quay deployment to accept all artifact types, see "Configuring unknown media types".
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequistes</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have Red Hat Quay administrator privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Add the <code class="literal">ALLOWED_OCI_ARTIFACT_TYPES</code> configuration field, along with the configuration and layer types:
					</p><pre class="programlisting language-yaml">FEATURE_GENERAL_OCI_SUPPORT: true
ALLOWED_OCI_ARTIFACT_TYPES:
  &lt;oci config type 1&gt;:
  - &lt;oci layer type 1&gt;
  - &lt;oci layer type 2&gt;

  &lt;oci config type 2&gt;:
  - &lt;oci layer type 3&gt;
  - &lt;oci layer type 4&gt;</pre><p class="simpara">
						For example, you can add Singularity Image Format (SIF) support by adding the following to your <code class="literal">config.yaml</code> file:
					</p><pre class="programlisting language-yaml">ALLOWED_OCI_ARTIFACT_TYPES:
  application/vnd.oci.image.config.v1+json:
  - application/vnd.dev.cosign.simplesigning.v1+json
  application/vnd.cncf.helm.config.v1+json:
  - application/tar+gzip
  application/vnd.sylabs.sif.config.v1+json:
  - application/vnd.sylabs.sif.layer.v1+tar</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When adding OCI artifact types that are not configured by default, Red Hat Quay administrators will also need to manually add support for Cosign and Helm if desired.
						</p></div></div><p class="simpara">
						Now, users can tag SIF images for their Red Hat Quay registry.
					</p></li></ul></div></section><section class="section" id="configuring-unknown-oci-media-types-proc"><div class="titlepage"><div><div><h2 class="title">19.3. Configuring unknown media types</h2></div></div></div><p>
				Use the following procedure to enable all artifact types for your Red Hat Quay deployment.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					With this field enabled, your Red Hat Quay deployment accepts all artifact types.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequistes</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have Red Hat Quay administrator privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Add the <code class="literal">IGNORE_UNKNOWN_MEDIATYPES</code> configuration field to your Red Hat Quay <code class="literal">config.yaml</code> file:
					</p><pre class="programlisting language-yaml">IGNORE_UNKNOWN_MEDIATYPES: true</pre><p class="simpara">
						With this field enabled, your Red Hat Quay deployment accepts unknown and unrecognized artifact types.
					</p></li></ol></div></section></section><section class="chapter" id="garbage-collection"><div class="titlepage"><div><div><h1 class="title">Chapter 20. Red Hat Quay garbage collection</h1></div></div></div><p>
			Red Hat Quay includes automatic and continuous image garbage collection. Garbage collection ensures efficient use of resources for active objects by removing objects that occupy sizeable amounts of disk space, such as dangling or untagged images, repositories, and blobs, including layers and manifests. Garbage collection performed by Red Hat Quay can reduce downtime in your organization’s environment.
		</p><section class="section" id="garbage-collection-practice"><div class="titlepage"><div><div><h2 class="title">20.1. Red Hat Quay garbage collection in practice</h2></div></div></div><p>
				Currently, all garbage collection happens discreetly, and there are no commands to manually run garbage collection. Red Hat Quay provides metrics that track the status of the different garbage collection workers.
			</p><p>
				For namespace and repository garbage collection, the progress is tracked based on the size of their respective queues. Namespace and repository garbage collection workers require a global lock to work. As a result, and for performance reasons, only one worker runs at a time.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat Quay shares blobs between namespaces and repositories in order to conserve disk space. For example, if the same image is pushed 10 times, only one copy of that image will be stored.
				</p><p>
					It is possible that tags can share their layers with different images already stored somewhere in Red Hat Quay. In that case, blobs will stay in storage, because deleting shared blobs would make other images unusable.
				</p><p>
					Blob expiration is independent of the time machine. If you push a tag to Red Hat Quay and the time machine is set to 0 seconds, and then you delete a tag immediately, garbage collection deletes the tag and everything related to that tag, but will not delete the blob storage until the blob expiration time is reached.
				</p></div></div><p>
				Garbage collecting tagged images works differently than garbage collection on namespaces or repositories. Rather than having a queue of items to work with, the garbage collection workers for tagged images actively search for a repository with inactive or expired tags to clean up. Each instance of garbage collection workers will grab a repository lock, which results in one worker per repository.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							In Red Hat Quay, inactive or expired tags are manifests without tags because the last tag was deleted or it expired. The manifest stores information about how the image is composed and stored in the database for each individual tag. When a tag is deleted and the allotted time from <span class="strong strong"><strong>Time Machine</strong></span> has been met, Red Hat Quay garbage collects the blobs that are not connected to any other manifests in the registry. If a particular blob is connected to a manifest, then it is preserved in storage and only its connection to the manifest that is being deleted is removed.
						</li><li class="listitem">
							Expired images will disappear after the allotted time, but are still stored in Red Hat Quay. The time in which an image is completely deleted, or collected, depends on the <span class="strong strong"><strong>Time Machine</strong></span> setting of your organization. The default time for garbage collection is 14 days unless otherwise specified. Until that time, tags can be pointed to an expired or deleted images.
						</li></ul></div></div></div><p>
				For each type of garbage collection, Red Hat Quay provides metrics for the number of rows per table deleted by each garbage collection worker. The following image shows an example of how Red Hat Quay monitors garbage collection with the same metrics:
			</p><p>
				<span class="inlinemediaobject"><img src="images/garbage-collection-metrics.png" alt="Garbage collection metrics"/></span>
			</p><section class="section" id="measuring-storage-reclamation"><div class="titlepage"><div><div><h3 class="title">20.1.1. Measuring storage reclamation</h3></div></div></div><p>
					Red Hat Quay does not have a way to track how much space is freed up by garbage collection. Currently, the best indicator of this is by checking how many blobs have been deleted in the provided metrics.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">UploadedBlob</code> table in the Red Hat Quay metrics tracks the various blobs that are associated with a repository. When a blob is uploaded, it will not be garbage collected before the time designated by the <code class="literal">PUSH_TEMP_TAG_EXPIRATION_SEC</code> parameter. This is to avoid prematurely deleting blobs that are part of an ongoing push. For example, if garbage collection is set to run often, and a tag is deleted in the span of less than one hour, then it is possible that the associated blobs will not get cleaned up immediately. Instead, and assuming that the time designated by the <code class="literal">PUSH_TEMP_TAG_EXPIRATION_SEC</code> parameter has passed, the associated blobs will be removed the next time garbage collection runs on that same repository.
					</p></div></div></section></section><section class="section" id="garbage-collection-configuration-fields"><div class="titlepage"><div><div><h2 class="title">20.2. Garbage collection configuration fields</h2></div></div></div><p>
				The following configuration fields are available to customize what is garbage collected, and the frequency at which garbage collection occurs:
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 17%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621338145856" scope="col">Name</th><th align="left" valign="top" id="idm45621338144768" scope="col">Description</th><th align="left" valign="top" id="idm45621338143680" scope="col">Schema</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>FEATURE_GARBAGE_COLLECTION</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								Whether garbage collection is enabled for image tags. Defaults to <code class="literal">true</code>.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								Boolean
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>FEATURE_NAMESPACE_GARBAGE_COLLECTION</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								Whether garbage collection is enabled for namespaces. Defaults to <code class="literal">true</code>.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								Boolean
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>FEATURE_REPOSITORY_GARBAGE_COLLECTION</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								Whether garbage collection is enabled for repositories. Defaults to <code class="literal">true</code>.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								Boolean
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>GARBAGE_COLLECTION_FREQUENCY</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								The frequency, in seconds, at which the garbage collection worker runs. Affects only garbage collection workers. Defaults to 30 seconds.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>PUSH_TEMP_TAG_EXPIRATION_SEC</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								The number of seconds that blobs will not be garbage collected after being uploaded. This feature prevents garbage collection from cleaning up blobs that are not referenced yet, but still used as part of an ongoing push.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>TAG_EXPIRATION_OPTIONS</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								List of valid tag expiration values.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>DEFAULT_TAG_EXPIRATION</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								Tag expiration time for time machine.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								String
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621338145856">
							<p>
								<span class="strong strong"><strong>CLEAN_BLOB_UPLOAD_FOLDER</strong></span>
							</p>
							</td><td align="left" valign="top" headers="idm45621338144768">
							<p>
								Automatically cleans stale blobs left over from an S3 multipart upload. By default, blob files older than two days are cleaned up every hour.
							</p>
							</td><td align="left" valign="top" headers="idm45621338143680">
							<p>
								Boolean
							</p>
							<p>
								+ <span class="strong strong"><strong>Default:</strong></span> <code class="literal">true</code>
							</p>
							</td></tr></tbody></table></div></section><section class="section" id="disabling-garbage-collection"><div class="titlepage"><div><div><h2 class="title">20.3. Disabling garbage collection</h2></div></div></div><p>
				The garbage collection features for image tags, namespaces, and repositories are stored in the <code class="literal">config.yaml</code> file. These features default to <code class="literal">true</code>.
			</p><p>
				In rare cases, you might want to disable garbage collection, for example, to control when garbage collection is performed. You can disable garbage collection by setting the <code class="literal">GARBAGE_COLLECTION</code> features to <code class="literal">false</code>. When disabled, dangling or untagged images, repositories, namespaces, layers, and manifests are not removed. This might increase the downtime of your environment.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					There is no command to manually run garbage collection. Instead, you would disable, and then re-enable, the garbage collection feature.
				</p></div></div></section><section class="section" id="garbage-collection-quota-management"><div class="titlepage"><div><div><h2 class="title">20.4. Garbage collection and quota management</h2></div></div></div><p>
				Red Hat Quay introduced quota management in 3.7. With quota management, users have the ability to report storage consumption and to contain registry growth by establishing configured storage quota limits.
			</p><p>
				As of Red Hat Quay 3.7, garbage collection reclaims memory that was allocated to images, repositories, and blobs after deletion. Because the garbage collection feature reclaims memory after deletion, there is a discrepancy between what is stored in an environment’s disk space and what quota management is reporting as the total consumption. There is currently no workaround for this issue.
			</p></section><section class="section" id="garbage-collection-procedure"><div class="titlepage"><div><div><h2 class="title">20.5. Garbage collection in practice</h2></div></div></div><p>
				Use the following procedure to check your Red Hat Quay logs to ensure that garbage collection is working.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Enter the following command to ensure that garbage collection is properly working:
					</p><pre class="programlisting language-terminal">$ sudo podman logs &lt;container_id&gt;</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">gcworker stdout | 2022-11-14 18:46:52,458 [63] [INFO] [apscheduler.executors.default] Job "GarbageCollectionWorker._garbage_collection_repos (trigger: interval[0:00:30], next run at: 2022-11-14 18:47:22 UTC)" executed successfully</pre></li><li class="listitem">
						Delete an image tag.
					</li><li class="listitem"><p class="simpara">
						Enter the following command to ensure that the tag was deleted:
					</p><pre class="programlisting language-terminal">$ podman logs quay-app</pre><p class="simpara">
						Example output:
					</p><pre class="programlisting language-terminal">gunicorn-web stdout | 2022-11-14 19:23:44,574 [233] [INFO] [gunicorn.access] 192.168.0.38 - - [14/Nov/2022:19:23:44 +0000] "DELETE /api/v1/repository/quayadmin/busybox/tag/test HTTP/1.0" 204 0 "http://quay-server.example.com/repository/quayadmin/busybox?tab=tags" "Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0"</pre></li></ol></div></section><section class="section" id="garbage-collection-metrics"><div class="titlepage"><div><div><h2 class="title">20.6. Red Hat Quay garbage collection metrics</h2></div></div></div><p>
				The following metrics show how many resources have been removed by garbage collection. These metrics show how many times the garbage collection workers have run and how many namespaces, repositories, and blobs were removed.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621341294576" scope="col">Metric name</th><th align="left" valign="top" id="idm45621341293488" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621341294576">
							<p>
								quay_gc_iterations_total
							</p>
							</td><td align="left" valign="top" headers="idm45621341293488">
							<p>
								Number of iterations by the GCWorker
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621341294576">
							<p>
								quay_gc_namespaces_purged_total
							</p>
							</td><td align="left" valign="top" headers="idm45621341293488">
							<p>
								Number of namespaces purged by the NamespaceGCWorker
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621341294576">
							<p>
								quay_gc_repos_purged_total
							</p>
							</td><td align="left" valign="top" headers="idm45621341293488">
							<p>
								Number of repositories purged by the RepositoryGCWorker or NamespaceGCWorker
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621341294576">
							<p>
								quay_gc_storage_blobs_deleted_total
							</p>
							</td><td align="left" valign="top" headers="idm45621341293488">
							<p>
								Number of storage blobs deleted
							</p>
							</td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Sample metrics output</strong></p><p>
					
<pre class="programlisting language-terminal"># TYPE quay_gc_iterations_created gauge
quay_gc_iterations_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189714e+09
...

# HELP quay_gc_iterations_total number of iterations by the GCWorker
# TYPE quay_gc_iterations_total counter
quay_gc_iterations_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...

# TYPE quay_gc_namespaces_purged_created gauge
quay_gc_namespaces_purged_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189433e+09
...

# HELP quay_gc_namespaces_purged_total number of namespaces purged by the NamespaceGCWorker
# TYPE quay_gc_namespaces_purged_total counter
quay_gc_namespaces_purged_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
....

# TYPE quay_gc_repos_purged_created gauge
quay_gc_repos_purged_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.631782319018925e+09
...

# HELP quay_gc_repos_purged_total number of repositories purged by the RepositoryGCWorker or NamespaceGCWorker
# TYPE quay_gc_repos_purged_total counter
quay_gc_repos_purged_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...

# TYPE quay_gc_storage_blobs_deleted_created gauge
quay_gc_storage_blobs_deleted_created{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 1.6317823190189059e+09
...

# HELP quay_gc_storage_blobs_deleted_total number of storage blobs deleted
# TYPE quay_gc_storage_blobs_deleted_total counter
quay_gc_storage_blobs_deleted_total{host="example-registry-quay-app-6df87f7b66-9tfn6",instance="",job="quay",pid="208",process_name="secscan:application"} 0
...</pre>
				</p></div></section></section><section class="chapter" id="using-v2-ui"><div class="titlepage"><div><div><h1 class="title">Chapter 21. Using the Red Hat Quay v2 UI</h1></div></div></div><p>
			Use the following procedures to configure, and use, the Red Hat Quay v2 UI.
		</p><section class="section" id="reference-miscellaneous-v2-ui"><div class="titlepage"><div><div><h2 class="title">21.1. v2 user interface configuration</h2></div></div></div><p>
				With <code class="literal">FEATURE_UI_V2</code> enabled, you can toggle between the current version of the user interface and the new version of the user interface.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							This UI is currently in beta and subject to change. In its current state, users can only create, view, and delete organizations, repositories, and image tags.
						</li><li class="listitem">
							When running Red Hat Quay in the old UI, timed-out sessions would require that the user input their password again in the pop-up window. With the new UI, users are returned to the main page and required to input their username and password credentials. This is a known issue and will be fixed in a future version of the new UI.
						</li><li class="listitem">
							There is a discrepancy in how image manifest sizes are reported between the legacy UI and the new UI. In the legacy UI, image manifests were reported in mebibytes. In the new UI, Red Hat Quay uses the standard definition of megabyte (MB) to report image manifest sizes.
						</li></ul></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In your deployment’s <code class="literal">config.yaml</code> file, add the <code class="literal">FEATURE_UI_V2</code> parameter and set it to <code class="literal">true</code>, for example:
					</p><pre class="programlisting language-yaml">---
FEATURE_TEAM_SYNCING: false
FEATURE_UI_V2: true
FEATURE_USER_CREATION: true
---</pre></li><li class="listitem">
						Log in to your Red Hat Quay deployment.
					</li><li class="listitem"><p class="simpara">
						In the navigation pane of your Red Hat Quay deployment, you are given the option to toggle between <span class="strong strong"><strong>Current UI</strong></span> and <span class="strong strong"><strong>New UI</strong></span>. Click the toggle button to set it to new UI, and then click <span class="strong strong"><strong>Use Beta Environment</strong></span>, for example:
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/38-ui-toggle.png" alt="Red Hat Quay v2 UI toggle"/></span>
					</p></li></ol></div><section class="section" id="creating-new-organization-v2-ui"><div class="titlepage"><div><div><h3 class="title">21.1.1. Creating a new organization in the Red Hat Quay v2 UI</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have toggled your Red Hat Quay deployment to use the v2 UI.
						</li></ul></div><p>
					Use the following procedure to create an organization using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Click <span class="strong strong"><strong>Organization</strong></span> in the navigation pane.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create Organization</strong></span>.
						</li><li class="listitem">
							Enter an <span class="strong strong"><strong>Organization Name</strong></span>, for example, <code class="literal">testorg</code>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create</strong></span>.
						</li></ol></div><p>
					Now, your example organization should populate under the <span class="strong strong"><strong>Organizations</strong></span> page.
				</p></section><section class="section" id="deleting-organization-v2"><div class="titlepage"><div><div><h3 class="title">21.1.2. Deleting an organization using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to delete an organization using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>Organizations</strong></span> page, select the name of the organization you want to delete, for example, <code class="literal">testorg</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>More Actions</strong></span> drop down menu.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Delete</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								On the <span class="strong strong"><strong>Delete</strong></span> page, there is a <span class="strong strong"><strong>Search</strong></span> input box. With this box, users can search for specific organizations to ensure that they are properly scheduled for deletion. For example, if a user is deleting 10 organizations and they want to ensure that a specific organization was deleted, they can use the <span class="strong strong"><strong>Search</strong></span> input box to confirm said organization is marked for deletion.
							</p></div></div></li><li class="listitem">
							Confirm that you want to permanently delete the organization by typing <span class="strong strong"><strong>confirm</strong></span> in the box.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Delete</strong></span>.
						</p><p class="simpara">
							After deletion, you are returned to the <span class="strong strong"><strong>Organizations</strong></span> page.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can delete more than one organization at a time by selecting multiple organizations, and then clicking <span class="strong strong"><strong>More Actions</strong></span> → <span class="strong strong"><strong>Delete</strong></span>.
							</p></div></div></li></ol></div></section><section class="section" id="creating-new-repository-v2"><div class="titlepage"><div><div><h3 class="title">21.1.3. Creating a new repository using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to create a repository using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Click <span class="strong strong"><strong>Repositories</strong></span> on the navigation pane.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create Repository</strong></span>.
						</li><li class="listitem">
							Select a namespace, for example, <span class="strong strong"><strong>quayadmin</strong></span>, and then enter a <span class="strong strong"><strong>Repository name</strong></span>, for example, <code class="literal">testrepo</code>.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Create</strong></span>.
						</p><p class="simpara">
							Now, your example repository should populate under the <span class="strong strong"><strong>Repositories</strong></span> page.
						</p></li></ol></div></section><section class="section" id="deleting-repository-v2"><div class="titlepage"><div><div><h3 class="title">21.1.4. Deleting a repository using the Red Hat Quay v2 UI</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created a repository.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>Repositories</strong></span> page of the Red Hat Quay v2 UI, click the name of the image you want to delete, for example, <code class="literal">quay/admin/busybox</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>More Actions</strong></span> drop-down menu.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Delete</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If desired, you could click <span class="strong strong"><strong>Make Public</strong></span> or <span class="strong strong"><strong>Make Private</strong></span>.
							</p></div></div></li><li class="listitem">
							Type <span class="strong strong"><strong>confirm</strong></span> in the box, and then click <span class="strong strong"><strong>Delete</strong></span>.
						</li><li class="listitem">
							After deletion, you are returned to the <span class="strong strong"><strong>Repositories</strong></span> page.
						</li></ol></div></section><section class="section" id="pushing-image-v2"><div class="titlepage"><div><div><h3 class="title">21.1.5. Pushing an image to the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to push an image to the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Pull a sample image from an external registry:
						</p><pre class="programlisting language-terminal">$ podman pull busybox</pre></li><li class="listitem"><p class="simpara">
							Tag the image:
						</p><pre class="programlisting language-terminal">$ podman tag docker.io/library/busybox quay-server.example.com/quayadmin/busybox:test</pre></li><li class="listitem"><p class="simpara">
							Push the image to your Red Hat Quay registry:
						</p><pre class="programlisting language-terminal">$ podman push quay-server.example.com/quayadmin/busybox:test</pre></li><li class="listitem">
							Navigate to the <span class="strong strong"><strong>Repositories</strong></span> page on the Red Hat Quay UI and ensure that your image has been properly pushed.
						</li><li class="listitem">
							You can check the security details by selecting your image tag, and then navigating to the <span class="strong strong"><strong>Security Report</strong></span> page.
						</li></ol></div></section><section class="section" id="deleting-image-v2"><div class="titlepage"><div><div><h3 class="title">21.1.6. Deleting an image using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to delete an image using theRed Hat Quay v2 UI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have pushed an image to your Red Hat Quay registry.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>Repositories</strong></span> page of the Red Hat Quay v2 UI, click the name of the image you want to delete, for example, <code class="literal">quay/admin/busybox</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>More Actions</strong></span> drop-down menu.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Delete</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If desired, you could click <span class="strong strong"><strong>Make Public</strong></span> or <span class="strong strong"><strong>Make Private</strong></span>.
							</p></div></div></li><li class="listitem">
							Type <span class="strong strong"><strong>confirm</strong></span> in the box, and then click <span class="strong strong"><strong>Delete</strong></span>.
						</li><li class="listitem">
							After deletion, you are returned to the <span class="strong strong"><strong>Repositories</strong></span> page.
						</li></ol></div></section><section class="section" id="creating-robot-account-v2-ui"><div class="titlepage"><div><div><h3 class="title">21.1.7. Creating a robot account using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to create a robot account using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the Red Hat Quay v2 UI, click <span class="strong strong"><strong>Organizations</strong></span>.
						</li><li class="listitem">
							Click the name of the organization that you will create the robot account for, for example, <code class="literal">test-org</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Robot accounts</strong></span> tab → <span class="strong strong"><strong>Create robot account</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Provide a name for your robot account</strong></span> box, enter a name, for example, <code class="literal">robot1</code>.
						</li><li class="listitem"><p class="simpara">
							Optional. The following options are available if desired:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Add the robot to a team.
								</li><li class="listitem">
									Add the robot to a repository.
								</li><li class="listitem">
									Adjust the robot’s permissions.
								</li></ol></div></li><li class="listitem">
							On the <span class="strong strong"><strong>Review and finish</strong></span> page, review the information you have provided, then click <span class="strong strong"><strong>Review and finish</strong></span>.
						</li><li class="listitem">
							Optional. You can click <span class="strong strong"><strong>Expand</strong></span> or <span class="strong strong"><strong>Collapse</strong></span> to reveal descriptive information about the robot account.
						</li><li class="listitem">
							Optional. You can change permissions of the robot account by clicking the kebab menu → <span class="strong strong"><strong>Set repository permissions</strong></span>.
						</li><li class="listitem">
							Optional. To delete your robot account, check the box of the robot account and click the trash can icon. A popup box appears. Type <span class="strong strong"><strong>confirm</strong></span> in the text box, then, click <span class="strong strong"><strong>Delete</strong></span>. Alternatively, you can click the kebab menu → <span class="strong strong"><strong>Delete</strong></span>.
						</li></ol></div></section><section class="section" id="organization-settings-v2-ui"><div class="titlepage"><div><div><h3 class="title">21.1.8. Organization settings for the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to alter your organization settings using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the Red Hat Quay v2 UI, click <span class="strong strong"><strong>Organizations</strong></span>.
						</li><li class="listitem">
							Click the name of the organization that you will create the robot account for, for example, <code class="literal">test-org</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Settings</strong></span> tab.
						</li><li class="listitem">
							Optional. Enter the email address associated with the organization.
						</li><li class="listitem"><p class="simpara">
							Optional. Set the allotted time for the <span class="strong strong"><strong>Time Machine</strong></span> feature to one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>1 week</strong></span>
								</li><li class="listitem">
									<span class="strong strong"><strong>1 month</strong></span>
								</li><li class="listitem">
									<span class="strong strong"><strong>1 year</strong></span>
								</li><li class="listitem">
									<span class="strong strong"><strong>Never</strong></span>
								</li></ul></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Save</strong></span>.
						</li></ol></div></section><section class="section" id="tag-overview-v2-ui"><div class="titlepage"><div><div><h3 class="title">21.1.9. Viewing image tag information using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to view image tag information using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the Red Hat Quay v2 UI, click <span class="strong strong"><strong>Repositories</strong></span>.
						</li><li class="listitem">
							Click the name of a repository, for example, <code class="literal">quayadmin/busybox</code>.
						</li><li class="listitem"><p class="simpara">
							Click the name of the tag, for example, <code class="literal">test</code>. You are taken to the <span class="strong strong"><strong>Details</strong></span> page of the tag. The page reveals the following information:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Name
								</li><li class="listitem">
									Repository
								</li><li class="listitem">
									Digest
								</li><li class="listitem">
									Vulnerabilities
								</li><li class="listitem">
									Creation
								</li><li class="listitem">
									Modified
								</li><li class="listitem">
									Size
								</li><li class="listitem">
									Labels
								</li><li class="listitem">
									How to fetch the image tag
								</li></ul></div></li><li class="listitem">
							Optional. Click <span class="strong strong"><strong>Security Report</strong></span> to view the tag’s vulnerabilities. You can expand an advisory column to open up CVE data.
						</li><li class="listitem">
							Optional. Click <span class="strong strong"><strong>Packages</strong></span> to view the tag’s packages.
						</li><li class="listitem">
							Click the name of the repository, for example, <code class="literal">busybox</code>, to return to the <span class="strong strong"><strong>Tags</strong></span> page.
						</li><li class="listitem">
							Optional. Hover over the <span class="strong strong"><strong>Pull</strong></span> icon to reveal the ways to fetch the tag.
						</li><li class="listitem">
							Check the box of the tag, or multiple tags, click the <span class="strong strong"><strong>Actions</strong></span> drop down menu, and then <span class="strong strong"><strong>Delete</strong></span> to delete the tag. Confirm deletion by clicking <span class="strong strong"><strong>Delete</strong></span> in the popup box.
						</li></ol></div></section><section class="section" id="settings-overview-v2-ui"><div class="titlepage"><div><div><h3 class="title">21.1.10. Adjusting repository settings using the Red Hat Quay v2 UI</h3></div></div></div><p>
					Use the following procedure to adjust various settings for a repository using the Red Hat Quay v2 UI.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the Red Hat Quay v2 UI, click <span class="strong strong"><strong>Repositories</strong></span>.
						</li><li class="listitem">
							Click the name of a repository, for example, <code class="literal">quayadmin/busybox</code>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Settings</strong></span> tab.
						</li><li class="listitem">
							Optional. Click <span class="strong strong"><strong>User and robot permissions</strong></span>. You can adjust the settings for a user or robot account by clicking the dropdown menu option under <span class="strong strong"><strong>Permissions</strong></span>. You can change the settings to <span class="strong strong"><strong>Read</strong></span>, <span class="strong strong"><strong>Write</strong></span>, or <span class="strong strong"><strong>Admin</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Optional. Click <span class="strong strong"><strong>Events and notifications</strong></span>. You can create an event and notification by clicking <span class="strong strong"><strong>Create Notification</strong></span>. The following event options are available:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Push to Repository
								</li><li class="listitem">
									Package Vulnerability Found
								</li><li class="listitem">
									Image build failed
								</li><li class="listitem">
									Image build queued
								</li><li class="listitem">
									Image build started
								</li><li class="listitem">
									Image build success
								</li><li class="listitem"><p class="simpara">
									Image build cancelled
								</p><p class="simpara">
									Then, issue a notification. The following options are available:
								</p></li><li class="listitem">
									Email Notification
								</li><li class="listitem">
									Flowdock Team Notification
								</li><li class="listitem">
									HipChat Room Notification
								</li><li class="listitem">
									Slack Notification
								</li><li class="listitem"><p class="simpara">
									Webhook POST
								</p><p class="simpara">
									After selecting an event option and the method of notification, include a <span class="strong strong"><strong>Room ID #</strong></span>, a <span class="strong strong"><strong>Room Notification Token</strong></span>, then, click <span class="strong strong"><strong>Submit</strong></span>.
								</p></li></ul></div></li><li class="listitem">
							Optional. Click <span class="strong strong"><strong>Repository visibility</strong></span>. You can make the repository private, or public, by clicking <span class="strong strong"><strong>Make Public</strong></span>.
						</li><li class="listitem">
							Optional. Click <span class="strong strong"><strong>Delete repository</strong></span>. You can delete the repository by clicking <span class="strong strong"><strong>Delete Repository</strong></span>.
						</li></ol></div></section></section><section class="section" id="enabling-legacy-ui"><div class="titlepage"><div><div><h2 class="title">21.2. Enabling the Red Hat Quay legacy UI</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In the navigation pane of your Red Hat Quay deployment, you are given the option to toggle between <span class="strong strong"><strong>Current UI</strong></span> and <span class="strong strong"><strong>New UI</strong></span>. Click the toggle button to set it to <span class="strong strong"><strong>Current UI</strong></span>.
					</p><p class="simpara">
						<span class="inlinemediaobject"><img src="images/38-ui-toggle.png" alt="Red Hat Quay v2 UI toggle"/></span>
					</p></li></ol></div></section></section><section class="chapter" id="health-check-quay"><div class="titlepage"><div><div><h1 class="title">Chapter 22. Performing health checks on Red Hat Quay deployments</h1></div></div></div><p>
			Health check mechanisms are designed to assess the health and functionality of a system, service, or component. Health checks help ensure that everything is working correctly, and can be used to identify potential issues before they become critical problems. By monitoring the health of a system, Red Hat Quay administrators can address abnormalities or potential failures for things like geo-replication deployments, Operator deployments, standalone Red Hat Quay deployments, object storage issues, and so on. Performing health checks can also help reduce the likelihood of encountering troubleshooting scenarios.
		</p><p>
			Health check mechanisms can play a role in diagnosing issues by providing valuable information about the system’s current state. By comparing health check results with expected benchmarks or predefined thresholds, deviations or anomalies can be identified quicker.
		</p><section class="section" id="health-check-endpoints"><div class="titlepage"><div><div><h2 class="title">22.1. Red Hat Quay health check endpoints</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Links contained herein to any external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or its entities, products, or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.
				</p></div></div><p>
				Red Hat Quay has several health check endpoints. The following table shows you the health check, a description, an endpoint, and an example output.
			</p><div class="table" id="idm45621337414176"><p class="title"><strong>Table 22.1. Health check endpoints</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 13%; " class="col_1"/><col style="width: 38%; " class="col_2"/><col style="width: 25%; " class="col_3"/><col style="width: 25%; " class="col_4"/></colgroup><thead><tr><th align="left" valign="top" id="idm45621337407488" scope="col">Health check</th><th align="left" valign="top" id="idm45621337406400" scope="col">Description</th><th align="left" valign="top" id="idm45621337405312" scope="col">Endpoint</th><th align="left" valign="top" id="idm45621337404224" scope="col">Example output</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45621337407488">
							<p>
								<code class="literal">instance</code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337406400">
							<p>
								The <code class="literal">instance</code> endpoint acquires the entire status of the specific Red Hat Quay instance. Returns a <code class="literal">dict</code> with key-value pairs for the following: <code class="literal">auth</code>, <code class="literal">database</code>, <code class="literal">disk_space</code>, <code class="literal">registry_gunicorn</code>, <code class="literal">service_key</code>, and <code class="literal">web_gunicorn.</code> Returns a number indicating the health check response of either <code class="literal">200</code>, which indicates that the instance is healthy, or <code class="literal">503</code>, which indicates an issue with your deployment.
							</p>
							</td><td align="left" valign="top" headers="idm45621337405312">
							<p>
								<code class="literal"><a class="link" href="https://{quay-ip-endpoint}/health/instance">https://{quay-ip-endpoint}/health/instance</a></code> <span class="emphasis"><em>or</em></span> <code class="literal"><a class="link" href="https://{quay-ip-endpoint}/health">https://{quay-ip-endpoint}/health</a></code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337404224">
							<p>
								<code class="literal">{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</code>
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621337407488">
							<p>
								<code class="literal">endtoend</code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337406400">
							<p>
								The <code class="literal">endtoend</code> endpoint conducts checks on all services of your Red Hat Quay instance. Returns a <code class="literal">dict</code> with key-value pairs for the following: <code class="literal">auth</code>, <code class="literal">database</code>, <code class="literal">redis</code>, <code class="literal">storage</code>. Returns a number indicating the health check response of either <code class="literal">200</code>, which indicates that the instance is healthy, or <code class="literal">503</code>, which indicates an issue with your deployment.
							</p>
							</td><td align="left" valign="top" headers="idm45621337405312">
							<p>
								<code class="literal"><a class="link" href="https://{quay-ip-endpoint}/health/endtoend">https://{quay-ip-endpoint}/health/endtoend</a></code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337404224">
							<p>
								<code class="literal">{"data":{"services":{"auth":true,"database":true,"redis":true,"storage":true}},"status_code":200}</code>
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45621337407488">
							<p>
								<code class="literal">warning</code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337406400">
							<p>
								The <code class="literal">warning</code> endpoint conducts a check on the warnings. Returns a <code class="literal">dict</code> with key-value pairs for the following: <code class="literal">disk_space_warning</code>. Returns a number indicating the health check response of either <code class="literal">200</code>, which indicates that the instance is healthy, or <code class="literal">503</code>, which indicates an issue with your deployment.
							</p>
							</td><td align="left" valign="top" headers="idm45621337405312">
							<p>
								<code class="literal"><a class="link" href="https://{quay-ip-endpoint}/health/warning">https://{quay-ip-endpoint}/health/warning</a></code>
							</p>
							</td><td align="left" valign="top" headers="idm45621337404224">
							<p>
								<code class="literal">{"data":{"services":{"disk_space_warning":true}},"status_code":503}</code>
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="instance-endpoint-quay"><div class="titlepage"><div><div><h2 class="title">22.2. Navigating to a Red Hat Quay health check endpoint</h2></div></div></div><p>
				Use the following procedure to navigate to the <code class="literal">instance</code> endpoint. This procedure can be repeated for <code class="literal">endtoend</code> and <code class="literal">warning</code> endpoints.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						On your web browser, navigate to <code class="literal"><a class="link" href="https://{quay-ip-endpoint}/health/instance">https://{quay-ip-endpoint}/health/instance</a></code>.
					</li><li class="listitem"><p class="simpara">
						You are taken to the health instance page, which returns information like the following:
					</p><pre class="programlisting language-json">{"data":{"services":{"auth":true,"database":true,"disk_space":true,"registry_gunicorn":true,"service_key":true,"web_gunicorn":true}},"status_code":200}</pre><p class="simpara">
						For Red Hat Quay, <code class="literal">"status_code": 200</code> means that the instance is health. Conversely, if you receive <code class="literal">"status_code": 503</code>, there is an issue with your deployment.
					</p></li></ol></div></section></section><section class="chapter" id="branding-quay-deployment"><div class="titlepage"><div><div><h1 class="title">Chapter 23. Branding a Red Hat Quay deployment on the legacy UI</h1></div></div></div><p>
			You can brand the UI of your Red Hat Quay deployment by changing the registry title, logo, footer image, and by directing users to a website embedded in the footer image.
		</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
					Update your Red Hat Quay <code class="literal">config.yaml</code> file to add the following parameters:
				</p><pre class="programlisting language-yaml">BRANDING:
    logo: <span id="CO11-1"/><span class="callout">1</span>
    footer_img: <span id="CO11-2"/><span class="callout">2</span>
    footer_url: <span id="CO11-3"/><span class="callout">3</span>
---
REGISTRY_TITLE: <span id="CO11-4"/><span class="callout">4</span>
REGISTRY_TITLE_SHORT: <span id="CO11-5"/><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The URL of the image that will appear at the top of your Red Hat Quay deployment.
						</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The URL of the image that will appear at the bottom of your Red Hat Quay deployment.
						</div></dd><dt><a href="#CO11-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The URL of the website that users will be directed to when clicking the footer image.
						</div></dd><dt><a href="#CO11-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The the long-form title for the registry. This is displayed in frontend of your Red Hat Quay deployment, for example, at the sign in page of your organization.
						</div></dd><dt><a href="#CO11-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The short-form title for the registry. The title is displayed on various pages of your organization, for example, as the title of the tutorial on your organization’s <span class="strong strong"><strong>Tutorial</strong></span> page.
						</div></dd></dl></div></li><li class="listitem">
					Restart your Red Hat Quay deployment. After restarting, your Red Hat Quay deployment is updated with a new logo, footer image, and footer image URL.
				</li></ol></div></section><section class="chapter" id="quay-schema"><div class="titlepage"><div><div><h1 class="title">Chapter 24. Schema for Red Hat Quay configuration</h1></div></div></div><p>
			Most Red Hat Quay configuration information is stored in the <code class="literal">config.yaml</code> file that is created using the browser-based config tool when Red Hat Quay is first deployed.
		</p><p>
			The configuration options are described in the Red Hat Quay Configuration Guide.
		</p><h2 id="additional_resources">Additional resources</h2></section><div><div xml:lang="en-US" class="legalnotice" id="idm45621347306288"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>